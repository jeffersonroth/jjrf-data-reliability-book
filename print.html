<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Data Reliability Engineering</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/catppuccin.css">
        <link rel="stylesheet" href="theme/catppuccin-highlight.css">

    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="COVER.html">Cover</a></li><li class="chapter-item expanded affix "><a href="TITLE.html">Title</a></li><li class="chapter-item expanded affix "><a href="SUMMARY.html">Summary</a></li><li class="chapter-item expanded affix "><a href="DEDICATION.html">Dedication</a></li><li class="chapter-item expanded affix "><a href="FOREWORD.html">Foreword</a></li><li class="chapter-item expanded affix "><a href="PREFACE.html">Preface</a></li><li class="chapter-item expanded affix "><a href="AUTHOR.html">Author</a></li><li class="chapter-item expanded affix "><a href="OBJECTIVES.html">Objectives</a></li><li class="chapter-item expanded affix "><a href="STRUCTURE.html">Structure</a></li><li class="chapter-item expanded "><a href="CONCEPTS.html"><strong aria-hidden="true">1.</strong> I - Concepts</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/systems_intro.html"><strong aria-hidden="true">1.1.</strong> Introduction to Systems</a></li><li class="chapter-item expanded "><a href="concepts/systems_reliability.html"><strong aria-hidden="true">1.2.</strong> Systems Reliability</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/impediments.html"><strong aria-hidden="true">1.2.1.</strong> Impediments</a></li><li class="chapter-item expanded "><a href="concepts/attributes.html"><strong aria-hidden="true">1.2.2.</strong> Attributes</a></li><li class="chapter-item expanded "><a href="concepts/mechanisms.html"><strong aria-hidden="true">1.2.3.</strong> Mechanisms</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/fault_prevention_avoidance.html"><strong aria-hidden="true">1.2.3.1.</strong> Fault Prevention: Avoidance</a></li><li class="chapter-item expanded "><a href="concepts/fault_tolerance.html"><strong aria-hidden="true">1.2.3.2.</strong> Fault Tolerance</a></li><li class="chapter-item expanded "><a href="concepts/fault_prevention_elimination.html"><strong aria-hidden="true">1.2.3.3.</strong> Fault Prevention: Elimination</a></li><li class="chapter-item expanded "><a href="concepts/fault_prediction.html"><strong aria-hidden="true">1.2.3.4.</strong> Fault Prediction</a></li><li class="chapter-item expanded "><a href="concepts/reliability_tools.html"><strong aria-hidden="true">1.2.3.5.</strong> Reliability Tools</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="concepts/data_quality.html"><strong aria-hidden="true">1.3.</strong> Data Quality</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data_quality_intro.html"><strong aria-hidden="true">1.3.1.</strong> Introduction to Data Quality</a></li><li class="chapter-item expanded "><a href="concepts/master_data.html"><strong aria-hidden="true">1.3.2.</strong> Master Data</a></li><li class="chapter-item expanded "><a href="concepts/data_management_processes.html"><strong aria-hidden="true">1.3.3.</strong> Data Management Processes</a></li><li class="chapter-item expanded "><a href="concepts/data_quality_models.html"><strong aria-hidden="true">1.3.4.</strong> Data Quality Models</a></li></ol></li><li class="chapter-item expanded "><a href="concepts/data_relibility.html"><strong aria-hidden="true">1.4.</strong> Data Reliability</a></li><li class="chapter-item expanded "><a href="concepts/processes.html"><strong aria-hidden="true">1.5.</strong> Processes</a></li><li class="chapter-item expanded "><a href="concepts/operations.html"><strong aria-hidden="true">1.6.</strong> Operations</a></li><li class="chapter-item expanded "><a href="concepts/data_architecture.html"><strong aria-hidden="true">1.7.</strong> Data Architecture</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data_sources.html"><strong aria-hidden="true">1.7.1.</strong> Data Sources</a></li><li class="chapter-item expanded "><a href="concepts/data_tier.html"><strong aria-hidden="true">1.7.2.</strong> Data Tier</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data_lake.html"><strong aria-hidden="true">1.7.2.1.</strong> Data Lake</a></li><li class="chapter-item expanded "><a href="concepts/data_warehouse.html"><strong aria-hidden="true">1.7.2.2.</strong> Data Warehouse</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data_warehouse_tier_architecture.html"><strong aria-hidden="true">1.7.2.2.1.</strong> Two-Tier vs Three-Tier Architecture</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data_warehouse_application_tier.html"><strong aria-hidden="true">1.7.2.2.1.1.</strong> Application Tier</a></li></ol></li><li class="chapter-item expanded "><a href="concepts/data_modelling.html"><strong aria-hidden="true">1.7.2.2.2.</strong> Data Modelling</a></li></ol></li><li class="chapter-item expanded "><a href="concepts/data_lakehouse.html"><strong aria-hidden="true">1.7.2.3.</strong> Data Lakehouse</a></li><li class="chapter-item expanded "><a href="concepts/data_marts.html"><strong aria-hidden="true">1.7.2.4.</strong> Data Marts</a></li></ol></li><li class="chapter-item expanded "><a href="concepts/application_tier.html"><strong aria-hidden="true">1.7.3.</strong> Application Tier</a></li><li class="chapter-item expanded "><a href="concepts/presentation_tier.html"><strong aria-hidden="true">1.7.4.</strong> Presentation Tier</a></li><li class="chapter-item expanded "><a href="concepts/metadata_management_tools.html"><strong aria-hidden="true">1.7.5.</strong> Metadata Management Tools</a></li></ol></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.</strong> II - Use Cases</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.</strong> A - Aranduka Inc.</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.</strong> Data Architecture</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.1.</strong> Operational System and Internal Data Sources</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.2.</strong> Integrating Data Partners</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.3.</strong> Designing the Data Lake</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.3.1.</strong> Anonymized Data</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.3.2.</strong> Distilled Data</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.4.</strong> Designing the Data Warehouse</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.4.1.</strong> Core Data</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.5.</strong> Designing the Data Marts</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.2.</strong> BICC & BI</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.2.1.</strong> Building Reliable Pipelines</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.2.2.</strong> Data Quality Assurance & Monitoring</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.2.3.</strong> Continuous Service</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.3.</strong> Growth, Marketing & Attribution Models</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.4.</strong> Multidimensional Analysis: Geo vs Verticals</div></li></ol></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.</strong> III - Incorporating Data Reliability Engineering</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">3.1.</strong> Solutions Architects</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">3.1.1.</strong> Data Architect</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.2.</strong> Data Engineers</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.3.</strong> Backend Engineers</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.4.</strong> BI Engineers</div></li></ol></li><li class="chapter-item expanded "><a href="EPILOGUE.html">Epilogue</a></li><li class="chapter-item expanded affix "><a href="DICTIONARY.html">Dictionary</a></li><li class="chapter-item expanded affix "><a href="REFERENCES.html">References</a></li><li class="chapter-item expanded affix "><a href="NEXT.html">Next</a></li><li class="chapter-item expanded affix "><a href="BACK_COVER.html">Back Cover</a></li><li class="chapter-item expanded affix "><a href="backlog.html">Backlog</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="latte">Latte</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="frappe">Frappé</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="macchiato">Macchiato</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mocha">Mocha</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Data Reliability Engineering</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div style="break-before: page; page-break-before: always;"></div><p>Data Reliability Engineering</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="summary"><a class="header" href="#summary">Summary</a></h1>
<p><a href="./COVER.html">Cover</a>
<a href="./TITLE.html">Title</a>
<a href="./SUMMARY.html">Summary</a>
<a href="./DEDICATION.html">Dedication</a>
<a href="./FOREWORD.html">Foreword</a>
<a href="./PREFACE.html">Preface</a>
<a href="./AUTHOR.html">Author</a>
<a href="./OBJECTIVES.html">Objectives</a>
<a href="./STRUCTURE.html">Structure</a></p>
<ul>
<li><a href="./CONCEPTS.html">I - Concepts</a>
<ul>
<li><a href="./concepts/systems_intro.html">Introduction to Systems</a></li>
<li><a href="./concepts/systems_reliability.html">Systems Reliability</a>
<ul>
<li><a href="./concepts/impediments.html">Impediments</a></li>
<li><a href="./concepts/attributes.html">Attributes</a></li>
<li><a href="./concepts/mechanisms.html">Mechanisms</a>
<ul>
<li><a href="./concepts/fault_prevention_avoidance.html">Fault Prevention: Avoidance</a></li>
<li><a href="./concepts/fault_tolerance.html">Fault Tolerance</a></li>
<li><a href="./concepts/fault_prevention_elimination.html">Fault Prevention: Elimination</a></li>
<li><a href="./concepts/fault_prediction.html">Fault Prediction</a></li>
<li><a href="./concepts/reliability_tools.html">Reliability Tools</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="./concepts/data_quality.html">Data Quality</a>
<ul>
<li><a href="./concepts/data_quality_intro.html">Introduction to Data Quality</a></li>
<li><a href="./concepts/master_data.html">Master Data</a></li>
<li><a href="./concepts/data_management_processes.html">Data Management Processes</a></li>
<li><a href="./concepts/data_quality_models.html">Data Quality Models</a></li>
</ul>
</li>
<li><a href="./concepts/data_relibility.html">Data Reliability</a></li>
<li><a href="./concepts/processes.html">Processes</a></li>
<li><a href="./concepts/operations.html">Operations</a></li>
<li><a href="./concepts/data_architecture.html">Data Architecture</a>
<ul>
<li><a href="./concepts/data_sources.html">Data Sources</a></li>
<li><a href="./concepts/data_tier.html">Data Tier</a>
<ul>
<li><a href="./concepts/data_lake.html">Data Lake</a></li>
<li><a href="./concepts/data_warehouse.html">Data Warehouse</a>
<ul>
<li><a href="./concepts/data_warehouse_tier_architecture.html">Two-Tier vs Three-Tier Architecture</a>
<ul>
<li><a href="./concepts/data_warehouse_application_tier.html">Application Tier</a></li>
</ul>
</li>
<li><a href="./concepts/data_modelling.html">Data Modelling</a></li>
</ul>
</li>
<li><a href="./concepts/data_lakehouse.html">Data Lakehouse</a></li>
<li><a href="./concepts/data_marts.html">Data Marts</a></li>
</ul>
</li>
<li><a href="./concepts/application_tier.html">Application Tier</a></li>
<li><a href="./concepts/presentation_tier.html">Presentation Tier</a></li>
<li><a href="./concepts/metadata_management_tools.html">Metadata Management Tools</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="">II - Use Cases</a>
<ul>
<li><a href="">A - Aranduka Inc.</a>
<ul>
<li><a href="">Data Architecture</a>
<ul>
<li><a href="">Operational System and Internal Data Sources</a></li>
<li><a href="">Integrating Data Partners</a></li>
<li><a href="">Designing the Data Lake</a>
<ul>
<li><a href="">Anonymized Data</a></li>
<li><a href="">Distilled Data</a></li>
</ul>
</li>
<li><a href="">Designing the Data Warehouse</a>
<ul>
<li><a href="">Core Data</a></li>
</ul>
</li>
<li><a href="">Designing the Data Marts</a></li>
</ul>
</li>
<li><a href="">BICC &amp; BI</a>
<ul>
<li><a href="">Building Reliable Pipelines</a></li>
<li><a href="">Data Quality Assurance &amp; Monitoring</a></li>
<li><a href="">Continuous Service</a></li>
</ul>
</li>
<li><a href="">Growth, Marketing &amp; Attribution Models</a></li>
<li><a href="">Multidimensional Analysis: Geo vs Verticals</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="">III - Incorporating Data Reliability Engineering</a>
<ul>
<li><a href="">Solutions Architects</a>
<ul>
<li><a href="">Data Architect</a></li>
</ul>
</li>
<li><a href="">Data Engineers</a></li>
<li><a href="">Backend Engineers</a></li>
<li><a href="">BI Engineers</a></li>
</ul>
</li>
</ul>
<p><a href="./EPILOGUE.html">Epilogue</a>
<a href="./DICTIONARY.html">Dictionary</a>
<a href="./REFERENCES.html">References</a>
<a href="./NEXT.html">Next</a>
<a href="./BACK_COVER.html">Back Cover</a></p>
<p><a href="./backlog.html">Backlog</a></p>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="concepts"><a class="header" href="#concepts">Concepts</a></h1>
<blockquote>
<p>The first part of this book exposes the different concepts around the data reliability engineering subject. It's intended to be heavilly technical, in contrast with the subsequent parts, intended to explore practical use cases.</p>
</blockquote>
<h2 id="introduction-to-systems-and-systems-reliability"><a class="header" href="#introduction-to-systems-and-systems-reliability"><a href="./concepts/systems_intro.html">Introduction to Systems</a> and <a href="./concepts/systems_reliability.html">Systems Reliability</a></a></h2>
<blockquote>
<p>These chapters explore what are systems, what is reliability, and how to understand systems reliability, specially its impediments, its attributes, and mechanisms to design and maintain reliable systems. All this for general systems, data systems, and data products.</p>
</blockquote>
<h2 id="data-quality"><a class="header" href="#data-quality"><a href="./concepts/data_quality.html">Data Quality</a></a></h2>
<blockquote>
<p>This chapter explores what is data, what is quality, and what is data quality, to finally explore what is data reliability.
The goal is to understand these concepts in all aspects of the data: life cycle, design, modelling, governance, management, access, security, uses, legal frameworks, best practices, maturity, standards, etc.</p>
</blockquote>
<h2 id="processes"><a class="header" href="#processes"><a href="./concepts/processes.html">Processes</a></a></h2>
<blockquote>
<p>This chapter explores, for a given system, the concept of data processes, data and information flow, workflows, orchestration, pipelines, ETL, and ELT.</p>
</blockquote>
<h2 id="operations"><a class="header" href="#operations"><a href="./concepts/operations.html">Operations</a></a></h2>
<blockquote>
<p>This chapter explores the concept of SRE, DataOps, DevOps, Agile methodologies, CI/CD, and other methodologies to assure reliable data operations.</p>
</blockquote>
<h2 id="data-architecture"><a class="header" href="#data-architecture"><a href="./concepts/data_architecture.html">Data Architecture</a></a></h2>
<blockquote>
<p>This chapter explores what is data architecture, including its sources, its storage (Data Lake, Data Warehouses, Data Marts), its application (OLAP servers, processing engines), and its presentation (dashboards, reports).</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction-to-systems"><a class="header" href="#introduction-to-systems">Introduction to Systems</a></h1>
<blockquote>
<p>In the context of data reliability, a system can be defined as a collection of interrelated components working together towards a common goal, often to process, store, and manage data. These components can include hardware, software, databases, procedures, and people, all interacting in a structured way to achieve efficient and reliable data handling.</p>
</blockquote>
<p>For people interested in studying data reliability, it's important to understand a system from both a <strong>technical</strong> and <strong>operational perspective</strong>. Technically, a system would include the architecture, technology, and protocols that ensure data integrity, availability, and consistency. Operationally, it involves the procedures and practices that maintain the system's performance and reliability over time.</p>
<p>In essence, when talking about data reliability, a system can be thought of as the entire ecosystem that supports the lifecycle of data, from its creation and storage to its retrieval and usage. This includes considerations of redundancy, fault tolerance, backup procedures, security measures, and regular maintenance practices, all of which contribute to the system's overall reliability and the trustworthiness of the service it provides.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="systems-reliability"><a class="header" href="#systems-reliability">Systems Reliability</a></h1>
<blockquote>
<p>The reliability of a system is the property that allows the system's service to be justifiably qualified as reliable.</p>
</blockquote>
<p>The goal of this chapter is to introduce the concepts of reliability and safety explored by Alan Burns and Andy Wellings in their book "Real-Time Systems and Programming Languages", concepts developed by different industries mainly between the 60s and 90s, and the concepts of Site Reliability Engineering (SRE) developed from the 2000s onwards, in addition to complementing it with reliability concepts worked on in other engineering fields (mechanical, industrial, etc.), as well as contextualizing it with concepts currently worked on in the software and computer systems industry.</p>
<h2 id="impediments"><a class="header" href="#impediments"><a href="concepts/./impediments.html">Impediments</a></a></h2>
<blockquote>
<p>Impediments prevent a system from functioning perfectly, or are a consequence of it. This subsection will address the detection of different types of impediments, which include <strong>Failures</strong>, <strong>Errors</strong>, and <strong>Defects</strong>.</p>
</blockquote>
<h2 id="attributes"><a class="header" href="#attributes"><a href="concepts/./attributes.html">Attributes</a></a></h2>
<blockquote>
<p>The way and measures by which the <strong>quality of a reliable service can be estimated</strong>.</p>
</blockquote>
<h2 id="mechanisms"><a class="header" href="#mechanisms"><a href="concepts/./mechanisms.html">Mechanisms</a></a></h2>
<blockquote>
<p>The mechanisms through which system reliability is addressed, whether by internalization and adoption of best practices, or by the application of specific methodologies, architectures, or tools. This subsection aims to create a <strong>framework</strong> that engineers can adopt for system reliability from the design phase itself.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="impediments-1"><a class="header" href="#impediments-1">Impediments</a></h1>
<h2 id="failures-errors-and-faults"><a class="header" href="#failures-errors-and-faults">Failures, Errors, and Faults</a></h2>
<blockquote>
<p><strong>Failures</strong> are the result of unexpected internal problems that a system eventually exhibits in its external behavior. These problems are called <strong>errors</strong>, and their mechanical or algorithmic causes are referred to as <strong>faults</strong>.
When a system's behavior deviates from what is specified for it, it is said to have a <strong>failure</strong>, or the system have <strong>failed</strong>.</p>
</blockquote>
<p>Systems are composed of <strong>components</strong>, each of which can be considered a system in itself. Thus, a failure in one system can induce a fault in another, which may result in an error and a potential failure of this system. This can continue and produce an effect on any related system, and so on.</p>
<p>A faulty component of a system is one that will produce an error under a specific set of circumstances during the system's lifetime. Seen in terms of state transitions, <em>a system can be considered as a number of external and internal states</em>.</p>
<p>An external state not specified in the system's behavior will be considered a system failure. The system itself consists of a number of components (each with its own states), all contributing to the system's external behavior. The combination of these components' states is called the system's internal state. <em>An unspecified internal state is considered an error, and the component that produced the illegal state transition is said to be faulty</em>.</p>
<p>The three types of failures:</p>
<ul>
<li><strong>Transient failures</strong>: begin at a specific point in time, remain in the system for some period, and then disappear.</li>
<li><strong>Permanent failures</strong>: begin at a certain point and remain in the system until they are repaired.</li>
<li><strong>Intermittent failures</strong>: are transient failures that occur sporadically.</li>
</ul>
<h2 id="failure-modes"><a class="header" href="#failure-modes">Failure Modes</a></h2>
<blockquote>
<p>A system can fail in many ways. A designer may design the system assuming a finite number of failure modes, however, the system may fail in ways that were not anticipated.</p>
</blockquote>
<p>We can classify the failure modes of the services that a system provides, which are:</p>
<ul>
<li><strong>Value failures</strong>: the value associated with the service is incorrect.</li>
<li><strong>Timing failure</strong>: the service is completed at the wrong time.</li>
<li><strong>Arbitrary failure</strong>: a combination of value and timing failures.</li>
</ul>
<p>Value failure modes are called <strong>value domain</strong>, and are classified into <strong>boundary error</strong>, and <strong>wrong value</strong>, where the value is outside the stipulated range.</p>
<p>Failures in the time domain can cause the service to be delivered:</p>
<ul>
<li><strong>Too early</strong> (premature): the service is delivered before it is required.</li>
<li><strong>Too late</strong> (delayed or performance error): the service is delivered after it is required.</li>
<li><strong>Infinitely late</strong> (omission failure): the service is never delivered.</li>
<li><strong>Unexpected</strong> (commission failure or improvisation): the service is delivered without being expected.</li>
</ul>
<p>In general, we can assume the modes in which a system can fail:</p>
<ul>
<li><strong>Uncontrolled failure</strong>: a system that produces arbitrary errors, both in the value domain and in the time domain (including improvisation errors).</li>
<li><strong>Delay failure</strong>: a system that produces correct services in the value domain but suffers from timing delays.</li>
<li><strong>Silent failure</strong>: a system that produces correct services in both the value and time domains, until it fails. The only possible failure is omission, and when it occurs, all subsequent services will also suffer from omission failures.</li>
<li><strong>Crash failure</strong>: a system that has all the properties of a silent failure but allows other systems to detect that it has entered the state of silent failure.</li>
<li><strong>Controlled failure</strong>: a system that fails in a specified and controlled manner.</li>
<li><strong>Failure-free</strong>: a system that always produces the correct services.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="attributes-1"><a class="header" href="#attributes-1">Attributes</a></h1>
<h2 id="reliability"><a class="header" href="#reliability">Reliability</a></h2>
<blockquote>
<p>It is the probability <em>R(t)</em> that the system will <strong>continue functioning at the end of the process</strong>.</p>
</blockquote>
<p>The time <em>t</em> is measured in continuous working hours between diagnostics. The constant failure rate λ is measured in <em>failures/hour</em>. The useful life of a system component is the constant region (on a logarithmic scale) of the curve between the component's age and its failure rate. The region of the graph before equilibrium is the Burn-In Phase, and the region where the failure rate starts to increase is the End of Life Phase. Thus, we have <em>R(t) = exp(-λt)</em>.</p>
<h2 id="availability"><a class="header" href="#availability">Availability</a></h2>
<blockquote>
<p>It is the measure of the <strong>frequency of incorrect service periods</strong>.</p>
</blockquote>
<h2 id="dependability"><a class="header" href="#dependability">Dependability</a></h2>
<blockquote>
<p>Continuity of service delivery.</p>
</blockquote>
<p>It is a measure (probability) of the <strong>success with which the system conforms to the definitive specification of its behavior</strong>.</p>
<h2 id="safety"><a class="header" href="#safety">Safety</a></h2>
<blockquote>
<p>It is the absence of conditions that can cause damage and the propagation of <strong>catastrophic damage</strong> in production.</p>
</blockquote>
<p>However, as this definition can classify virtually any process as unsafe, we often consider the term <strong>mishap</strong>.</p>
<blockquote>
<p>A mishap is an <strong>unplanned event</strong> or sequence of events that can produce catastrophic damage.</p>
</blockquote>
<p>Despite its similarity to the definition of <strong>dependability</strong>, the difference in emphasis should be noted. Dependability is the measure of success with which the system conforms to the specification of its behavior, typically in terms of <strong>probability</strong>. Safety, however, is the <strong>improbability of conditions leading to a mishap occurring, regardless of whether the intended function is performed</strong>.</p>
<h2 id="integrity"><a class="header" href="#integrity">Integrity</a></h2>
<blockquote>
<p>It is the absence of conditions that can lead to inappropriate alterations of data in production. It is the <strong>improbability of conditions occurring that alter inappropriate data in production, regardless of whether the intended function is performed</strong>.</p>
</blockquote>
<h2 id="maintainability"><a class="header" href="#maintainability">Maintainability</a></h2>
<blockquote>
<p>Ability to undergo repairs and evolve.</p>
</blockquote>
<h2 id="scalability"><a class="header" href="#scalability">Scalability</a></h2>
<blockquote>
<p>Ability to adapt to business needs.</p>
</blockquote>
<h2 id="deficiencies"><a class="header" href="#deficiencies">Deficiencies</a></h2>
<blockquote>
<p>Circumstances that cause or are a product of <strong>unreliability</strong>.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mechanisms-1"><a class="header" href="#mechanisms-1">Mechanisms</a></h1>
<h2 id="fault-prevention-avoidance"><a class="header" href="#fault-prevention-avoidance"><a href="concepts/./fault_prevention_avoidance.html">Fault Prevention: Avoidance</a></a></h2>
<h2 id="fault-tolerance"><a class="header" href="#fault-tolerance"><a href="concepts/./fault_tolerance.html">Fault Tolerance</a></a></h2>
<h2 id="fault-prevention-elimination"><a class="header" href="#fault-prevention-elimination"><a href="concepts/./fault_prevention_elimination.html">Fault Prevention: Elimination</a></a></h2>
<h2 id="fault-predictions"><a class="header" href="#fault-predictions"><a href="concepts/./fault_prediction.html">Fault Predictions</a></a></h2>
<h2 id="reliability-tools"><a class="header" href="#reliability-tools"><a href="concepts/./reliability_tools.html">Reliability Tools</a></a></h2>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fault-prevention-avoidance-1"><a class="header" href="#fault-prevention-avoidance-1">Fault Prevention: Avoidance</a></h1>
<p>There are two phases in fault prevention: <strong>avoidance</strong> and <strong>elimination</strong>.</p>
<blockquote>
<p>Avoidance aims to limit the introduction of potentially defective data and objects during the execution of the process.</p>
</blockquote>
<p>Such as:</p>
<ul>
<li>The use of validated and clean information sources, when possible.</li>
<li>The implementation of data cleaning and validation processes for raw data.</li>
<li>The validation of table and column availability within databases.</li>
<li>The introduction of branch operators for effective data management.</li>
<li>The implementation of rigorous code review processes to maintain a clean and secure codebase.</li>
<li>The adoption of standardized coding practices and secure coding guidelines to minimize errors and security vulnerabilities.</li>
<li>The utilization of automated testing frameworks for continuous testing (unit, integration, system) throughout the development cycle.</li>
<li>The application of configuration management tools and practices to oversee changes in software and hardware, ensuring all modifications are authorized and tested.</li>
<li>The engagement in detailed requirement analysis and system design reviews to affirm the system's resilience against potential faults, including the use of modeling and simulation tools.</li>
<li>The incorporation of fail-safe and fail-soft designs to maintain system safety in case of failure, including redundancy strategies for critical components.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fault-tolerance-1"><a class="header" href="#fault-tolerance-1">Fault Tolerance</a></h1>
<blockquote>
<p>Given the limitations in fault prevention, especially as data and processes frequently change, it becomes necessary to resort to fault tolerance.</p>
</blockquote>
<p>There are different levels of fault tolerance:</p>
<ul>
<li><strong>Full tolerance</strong>: there is no management of adverse or unwanted conditions; the process does not adapt to validations and environmental variables or other external information for the execution of tasks.</li>
<li><strong>Controlled degradation</strong> (or graceful degradation): notifications are triggered in the presence of faults, and if they are significant enough to interrupt the task flow (thresholds, non-existence, or unavailability of data), branch operators will select the subsequent tasks.</li>
<li><strong>Fail-safe</strong>: detected faults are significant enough to determine that the process should not occur; a short-circuit or circuit breaker operator cancels the execution of subsequent tasks, stakeholders are notified, and if there is no automatic process to deal with the problem, the data team can take actions such as rerunning the processes that generate the necessary inputs or escalating the case.</li>
</ul>
<p>The design of fault-tolerant processes assumes:</p>
<ul>
<li>The task algorithms have been correctly designed.</li>
<li>All possible failure modes of the components are known.</li>
<li>All possible interactions between the process and its environment have been considered.</li>
</ul>
<h2 id="redundancy"><a class="header" href="#redundancy">Redundancy</a></h2>
<blockquote>
<p>All techniques used to achieve fault tolerance are based on adding external elements to the system to detect and recover from faults. These elements are redundant in the sense that they are not necessary for the system's normal operation; this is called <strong>protective redundancy</strong>. The goal of tolerance is to minimize redundancy while maximizing reliability, always under the constraints of system complexity and size. <em>Care must be taken when designing fault-tolerant systems, as components increase the complexity and maintenance of the entire system, which can in itself lead to less reliable systems</em>.</p>
</blockquote>
<p>Redundancy in systems is classified into static and dynamic. <strong>Static redundancy</strong>, or masking, involves using redundant components to hide the effects of faults. <strong>Dynamic redundancy</strong> is redundancy within a component that makes it indicate, implicitly or explicitly, that the output is erroneous; recovery must be provided by another component. This fault tolerance technique has four phases:</p>
<ol>
<li><strong>Error detection</strong>: no fault tolerance action will be taken until an error has been detected.</li>
<li><strong>Damage confinement and assessment</strong>: when an error is detected, the extent of the system that has been corrupted and its scope must be estimated (error diagnosis).</li>
<li><strong>Error recovery</strong>: this is one of the most important aspects of fault tolerance. Error recovery techniques should direct the corrupted system to a state from which it can continue its normal operation (perhaps with functional degradation).</li>
<li><strong>Failure treatment and service continuation</strong>: an error is a symptom of a failure; although the damage might have been repaired, the failure still exists, and therefore the error may recur unless some form of maintenance is performed.</li>
</ol>
<h3 id="1-error-detection"><a class="header" href="#1-error-detection">1. Error Detection</a></h3>
<blockquote>
<p>The effectiveness of a fault-tolerant system depends on the <strong>effectiveness of error detection</strong>.</p>
</blockquote>
<p>Error detection is classified into:</p>
<ul>
<li><strong>Environmental detections</strong>: Errors are detected in the environment in which the program runs. They are handled by exceptions.</li>
<li><strong>Application detection</strong>: Errors are detected within the application itself.
<ul>
<li><strong>Reverse checks</strong>: Applied in components with an isomorphic relationship (one-to-one) between input and output. In this method, the output is taken and the input is calculated, which is compared with the original input value. For real numbers, inexact comparison techniques must be adopted.</li>
<li><strong>Rationality checks</strong>: Based on the design and construction knowledge of the system. They verify that the state of the data or the value of an object is reasonable based on its intended use.</li>
</ul>
</li>
</ul>
<h3 id="2-damage-confinement-and-assessment"><a class="header" href="#2-damage-confinement-and-assessment">2. Damage Confinement and Assessment</a></h3>
<blockquote>
<p>There will always be a time magnitude between the occurrence of a defect and the detection of the error, making it important to assess any damage that may have occurred in this time interval.</p>
</blockquote>
<p>Although the type of error detected can provide ideas about the damage to the error handling routine, erroneous information could have been disseminated through the system and its environment. Thus, damage assessment is directly related to the precautions taken by the system designer for damage confinement. Damage confinement refers to structuring the system in such a way as to minimize the damage caused by a faulty component.</p>
<p>There are two main techniques for structuring systems to facilitate damage confinement: <strong>modular decomposition</strong> and <strong>atomic actions</strong>. Modular decomposition means that systems should be broken down into components, each represented by one or more modules. The interaction of the components occurs through well-defined interfaces, and the internal details of the modules are hidden and not directly accessible from the outside. This makes it more difficult for an error in one component to indiscriminately pass to another.</p>
<p>Modular decomposition provides the system with a static structure, while atomic actions provide it with a dynamic structure. An action is said to be atomic if there are no interactions between the activity and the system during the course of the action. These actions are used to move the system from one consistent state to another and to restrict the flow of information between components.</p>
<h3 id="3-error-recovery"><a class="header" href="#3-error-recovery">3. Error Recovery</a></h3>
<blockquote>
<p>Once the error situation has been detected and its possible damages have been assessed, error recovery procedures begin. This phase is probably the most important within fault tolerance techniques, which must transform an erroneous state of the system into another from which the system can continue its normal operation, perhaps with some service degradation.</p>
</blockquote>
<p>Here it's worth mentioning two strategies for error recovery: <strong>forward recovery</strong> and <strong>backward recovery</strong>. Forward error recovery attempts to continue from the erroneous state by making selective corrections to the system's state, including protecting any aspect of the controlled environment that could be put at risk or damaged by the failure.</p>
<p>Backward recovery is based on restoring the system to a safe state prior to the one in which the error occurred, and then executing an alternative section of the task. This will have the same functionality as the section that produced the defect, but using a different algorithm. It is expected that this alternative will not produce the same defect as the previous version, so it will rely on the designer's knowledge of the possible failure modes of this component.</p>
<p>The designer must be clear about the levels of service degradation, taking into account the services and processes that depend on it. Error recovery is part of the Corrective Action and Preventive Action processes (CAPA), which will be worked on in two moments: in this same chapter on fault tolerance when corrective actions are worked on, and in the next chapter, fault prevention, when preventive actions are addressed.</p>
<h3 id="4-failure-treatment-and-continued-service"><a class="header" href="#4-failure-treatment-and-continued-service">4. Failure Treatment and Continued Service</a></h3>
<blockquote>
<p>An error is a manifestation of a defect, and although the error recovery phase may have brought the system to an error-free state, the error can recur. Therefore, the final phase of fault tolerance is to eradicate the failure from the system so that normal service can continue.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fault-prevention-elimination-1"><a class="header" href="#fault-prevention-elimination-1">Fault Prevention: Elimination</a></h1>
<p>The second phase of fault prevention is fault elimination. This phase typically involves procedures to find and eliminate the causes of errors. Although techniques such as code reviews (e.g. linters) and local debugging are used, peer reviews and exhaustive testing with various combinations of input states and environments are not always carried out.</p>
<p>QA testing cannot verify that output values are compatible with the business and its applications, so it usually focuses on time-related failure modes (such as timeouts) and <strong>defects</strong>. Unfortunately, system testing cannot be exhaustive and eliminate all potential faults, mainly due to:</p>
<ul>
<li>Tests are used to demonstrate the presence of faults, not their absence.</li>
<li>The difficulty of performing tests in production. Testing failures in production are akin to <strong>live combat</strong>, meaning the consequences of errors can directly impact the business, leading to potentially poor decisions (for example, an incorrect calculation of a KPI can not only lead to erroneous actions but can also decrease the business's confidence in the data processes). There are process design alternatives for fault detection in production, which I will discuss later.</li>
<li>Errors that were introduced during the system requirements stage may not manifest until the system is operational. For example, a DAG (Directed Acyclic Graph) scheduled to run at a time the data source is not yet available or complete. For this specific example, sensors might be used to only continue the execution when the data source is available, or fail if not available within a specific timeframe (timeout).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="failure-prediction"><a class="header" href="#failure-prediction">Failure Prediction</a></h1>
<p>Accurate and rapid prediction of failures allows those of us maintaining processes to ensure higher service availability. Unfortunately, failure prediction is much more complex than detection.</p>
<p>To predict a failure, it must be identified and classified. Failures must also be predictable, meaning there are system (and component) state changes that lead to failure, or the failure occurs regularly following some pattern. Both cases can be translated into time series prediction problems, and sensor and log data can be used to train prediction models.</p>
<p>The collected data will hardly be ready for use by prediction models, so one or more preprocessing tasks must be carried out:</p>
<ul>
<li><strong>Data synchronization</strong>: metrics collected by various agents must be aligned in time.</li>
<li><strong>Data cleaning</strong>: removal of unnecessary data and generation of missing data (e.g., interpolation).</li>
<li><strong>Data normalization</strong>: metric values are normalized to make magnitudes comparable.</li>
<li><strong>Feature selection</strong>: relevant metrics are identified for use in the models.</li>
</ul>
<p>Once the data is preprocessed, it will be used in two pipelines: a training pipeline and an inference pipeline. The training pipeline uses bulk data to train the model to be made available to the inference pipeline. The inference results will indicate the presence or absence of specific types of failures in the monitored metric sample.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reliability-tools-1"><a class="header" href="#reliability-tools-1">Reliability Tools</a></h1>
<h2 id="data-observability-platforms"><a class="header" href="#data-observability-platforms">Data Observability Platforms</a></h2>
<p>Data observability platforms provide comprehensive monitoring and observability into data pipelines, data quality, and system performance. These platforms can automatically detect and alert on data anomalies, pipeline failures, and performance bottlenecks, enabling quick resolution and ensuring data reliability.</p>
<h2 id="version-control-systems-for-data"><a class="header" href="#version-control-systems-for-data">Version Control Systems for Data</a></h2>
<p>Version control systems designed specifically for data, such as DVC (Data Version Control), enable data engineers to track and manage changes to datasets and machine learning models. This helps in maintaining consistency, reproducibility, and rollback capabilities, enhancing data reliability.</p>
<h2 id="data-lineage-tools"><a class="header" href="#data-lineage-tools">Data Lineage Tools</a></h2>
<p>Data lineage tools track the flow of data through various processes and transformations, providing visibility into the data's origin, what changes were made, and where it's used. This transparency helps in diagnosing and correcting errors, ensuring data reliability and trustworthiness.</p>
<h2 id="automated-data-quality-testing"><a class="header" href="#automated-data-quality-testing">Automated Data Quality Testing</a></h2>
<p>Tools like Great Expectations or Deequ allow data engineers to define and automate data quality checks within data pipelines. By continuously testing data for anomalies, inconsistencies, or deviations from defined quality rules, these tools help maintain high data quality standards.</p>
<h2 id="container-orchestration-systems"><a class="header" href="#container-orchestration-systems">Container Orchestration Systems</a></h2>
<p>Container orchestration systems like Kubernetes can enhance the reliability of data applications and services by managing container deployment, scaling, and failover. This ensures that data services are always available and can dynamically scale based on demand.</p>
<h2 id="infrastructure-as-code-iac-tools"><a class="header" href="#infrastructure-as-code-iac-tools">Infrastructure as Code (IaC) Tools</a></h2>
<p>IaC tools like Terraform allow data engineers to define and manage infrastructure using code, ensuring that data environments are reproducible, consistent, and maintainable. This reduces the risk of environment-related inconsistencies and errors.</p>
<h2 id="feature-flags-and-toggle-management"><a class="header" href="#feature-flags-and-toggle-management">Feature Flags and Toggle Management</a></h2>
<p>Feature flags and toggle management tools enable data engineers to roll out new features and changes in a controlled manner. By gradually introducing changes and monitoring their impact, engineers can ensure system stability and quickly revert changes if issues arise.</p>
<h2 id="chaos-engineering-tools"><a class="header" href="#chaos-engineering-tools">Chaos Engineering Tools</a></h2>
<p>Chaos engineering tools, such as Gremlin or Chaos Mesh, introduce controlled disruptions into data systems (like network latency, server failures, or resource exhaustion) to test and improve their resilience. By proactively identifying and addressing potential points of failure, data systems become more robust and reliable.</p>
<h2 id="workflow-orchestration-tools"><a class="header" href="#workflow-orchestration-tools">Workflow Orchestration Tools</a></h2>
<p>Workflow orchestration tools like Apache Airflow or Prefect help manage complex data pipelines by ensuring tasks are executed in the correct order, managing dependencies, and handling retries and failures gracefully. This improves the reliability of data processing workflows.</p>
<h2 id="apache-airflow"><a class="header" href="#apache-airflow">Apache Airflow</a></h2>
<p>In the context of this chapter on ensuring data reliability, Apache Airflow can be classified as a <strong>Workflow Orchestration Tool</strong>. Airflow is designed to author, schedule, and monitor workflows programmatically. It enables data engineers to define, execute, and manage complex data pipelines, ensuring that data tasks are executed in the correct order, adhering to dependencies and handling retries and failures gracefully. By providing robust scheduling and monitoring capabilities for data workflows, Airflow plays a pivotal role in maintaining the reliability and consistency of data processing operations.</p>
<p>Apache Airflow contributes significantly to data reliability through its robust workflow orchestration capabilities. Here's how Airflow enhances the reliability of data processes:</p>
<h3 id="scheduled-and-automated-workflows"><a class="header" href="#scheduled-and-automated-workflows">Scheduled and Automated Workflows</a></h3>
<p>Airflow allows for the scheduling of complex data workflows, ensuring that data processing tasks are executed at the right time and in the right order. This automation reduces the risk of human error and ensures that critical data processes, such as ETL jobs, data validation, and reporting, are run consistently and reliably.</p>
<h3 id="dependency-management"><a class="header" href="#dependency-management">Dependency Management</a></h3>
<p>Airflow's ability to define dependencies between tasks means that data workflows are executed in a manner that respects the logical sequence of data processing steps. This ensures that upstream failures are appropriately handled before proceeding with downstream tasks, maintaining the integrity and reliability of the data pipeline.</p>
<h3 id="retries-and-failure-handling"><a class="header" href="#retries-and-failure-handling">Retries and Failure Handling</a></h3>
<p>Airflow provides built-in mechanisms for retrying failed tasks and alerting when issues occur. This resilience in the face of failures helps to ensure that temporary issues, such as network outages or transient system failures, do not lead to incomplete or incorrect data processing, thereby enhancing data reliability.</p>
<h3 id="extensive-monitoring-and-logging"><a class="header" href="#extensive-monitoring-and-logging">Extensive Monitoring and Logging</a></h3>
<p>With Airflow's comprehensive monitoring and logging capabilities, data engineers can quickly identify and diagnose issues within their data pipelines. This visibility is crucial for maintaining high data quality and reliability, as it allows for prompt intervention and resolution of issues that could compromise data integrity.</p>
<h3 id="dynamic-pipeline-generation"><a class="header" href="#dynamic-pipeline-generation">Dynamic Pipeline Generation</a></h3>
<p>Airflow supports dynamic pipeline generation, allowing for workflows that adapt to changing data or business requirements. This flexibility ensures that data processes remain relevant and reliable, even as the underlying data or the processing needs evolve.</p>
<h3 id="scalability-1"><a class="header" href="#scalability-1">Scalability</a></h3>
<p>Airflow's architecture supports scaling up to handle large volumes of data and complex workflows. This scalability ensures that as data volumes grow, the data processing pipelines can continue to operate efficiently and reliably without degradation in performance.</p>
<p>By orchestrating data workflows with these capabilities, Airflow plays a critical role in ensuring that data processes are reliable, efficient, and aligned with business needs, making it an essential tool in the data engineer's toolkit for maintaining data reliability.</p>
<h2 id="dbt"><a class="header" href="#dbt">dbt</a></h2>
<p>In the context of this chapter, which discusses various tools and methodologies for ensuring data reliability, dbt (data build tool) can be classified as a <strong>Data Transformation and Testing Tool</strong>. It specializes in managing, testing, and documenting data transformations within modern data warehouses. dbt enables data engineers and analysts to write scalable, maintainable SQL code for transforming raw data into structured and reliable datasets suitable for analysis, thereby playing a crucial role in maintaining and enhancing data reliability.</p>
<p>It plays a significant role in enhancing data reliability within modern data engineering practices. It is a command-line tool that enables data analysts and engineers to transform data in their warehouses more effectively by writing, testing, and deploying SQL queries. Here’s how dbt contributes to data reliability:</p>
<h3 id="version-control-and-collaboration"><a class="header" href="#version-control-and-collaboration">Version Control and Collaboration</a></h3>
<p>dbt encourages the use of version control systems like Git for managing transformation scripts, which enhances collaboration among team members and maintains a historical record of changes. This practice ensures consistency and reliability in data transformations, as changes are tracked, reviewed, and documented.</p>
<h3 id="testing-and-validation"><a class="header" href="#testing-and-validation">Testing and Validation</a></h3>
<p>dbt allows for the implementation of data tests that automatically validate the quality and integrity of the transformed data. These tests can include not-null checks, uniqueness tests, referential integrity checks among tables, and custom business logic validations. By catching issues early in the data transformation stage, dbt helps prevent the propagation of errors downstream, thereby improving the reliability of data used for reporting and analytics.</p>
<h3 id="data-documentation"><a class="header" href="#data-documentation">Data Documentation</a></h3>
<p>With dbt, data documentation is treated as a first-class citizen. dbt generates documentation for the data models, including descriptions of tables and columns and the relationships between different models. This documentation is crucial for understanding the data transformations and ensuring that all stakeholders have a clear and accurate view of the data, its sources, and transformations, which is essential for data reliability.</p>
<h3 id="data-lineage"><a class="header" href="#data-lineage">Data Lineage</a></h3>
<p>dbt provides a visual representation of data lineage, showing how different data models are connected and how data flows through the transformations. This visibility into data lineage helps in understanding the impact of changes, troubleshooting issues, and ensuring that data transformations are reliable and maintain the integrity of the data throughout the pipeline.</p>
<h3 id="incremental-processing"><a class="header" href="#incremental-processing">Incremental Processing</a></h3>
<p>dbt supports incremental data processing, which allows for more efficient data transformations by only processing new or changed data since the last run. This approach reduces the likelihood of processing errors due to handling smaller volumes of data at a time and ensures that the data remains up-to-date and reliable.</p>
<h3 id="modular-and-reusable-code"><a class="header" href="#modular-and-reusable-code">Modular and Reusable Code</a></h3>
<p>dbt promotes writing modular and reusable SQL code, which reduces redundancy and potential for errors in data transformation scripts. By using macros and packages, common logic can be standardized and reused across projects, increasing the reliability of data transformations.</p>
<p>By integrating these features and best practices into the data transformation process, dbt helps ensure that the data is accurate, consistent, and reliable, which is crucial for making informed business decisions and maintaining trust in data systems.</p>
<h2 id="reliability-block-diagrams"><a class="header" href="#reliability-block-diagrams">Reliability Block Diagrams</a></h2>
<blockquote>
<p>Reliability Block Diagrams (RBD) are a method for diagramming and identifying how the reliability of components (or subsystems) <em>R(t)</em> contributes to the success or failure of a redundancy. It is a method that can be used to design and optimize components and select redundancies, aiming to lower failure rates.</p>
</blockquote>
<p>An RBD is represented as a series of connected blocks (in series, parallel, or a combination thereof), indicating redundant components, the type of redundancy, and their respective failure rates.</p>
<p>When analyzing the diagram, components that failed and those that did not fail are indicated. If a path can be found between the start and end of the process with components that did not fail, it can be assumed that the process can be completed.</p>
<p>Each RBD should include statements listing all relationships between components, i.e., what conditions led to the use of one component over another in the process execution.</p>
<p>RBDs can be particularly useful in data engineering to ensure the reliability and availability of data pipelines and storage systems. Here's how RBDs could be applied in the context of data engineering:</p>
<h3 id="designing-data-pipelines"><a class="header" href="#designing-data-pipelines">Designing Data Pipelines</a></h3>
<p>Data pipelines consist of various stages like data collection, processing, transformation, and loading (ETL processes). An RBD can represent each stage as a block, with connections illustrating the flow of data. This helps in identifying critical components whose failure could disrupt the entire pipeline, allowing engineers to implement redundancy or failovers specifically for those components.</p>
<h3 id="infrastructure-reliability"><a class="header" href="#infrastructure-reliability">Infrastructure Reliability</a></h3>
<p>In data engineering, the infrastructure includes databases, servers, network components, and storage systems. An RBD can help visualize the relationship between these components and their impact on overall system reliability. For example, a database cluster might be set up with redundancy to ensure that the failure of a single node doesn't result in data loss or downtime, represented in an RBD by parallel blocks for each redundant component.</p>
<h3 id="dependency-analysis"><a class="header" href="#dependency-analysis">Dependency Analysis</a></h3>
<p>RBDs can help data engineers understand how different data sources and processes depend on each other. For instance, if a data pipeline relies on multiple external APIs or data sources, the RBD can illustrate these dependencies, highlighting potential points of failure if one of the external sources becomes unreliable.</p>
<h3 id="optimizing-redundancies"><a class="header" href="#optimizing-redundancies">Optimizing Redundancies</a></h3>
<p>By using RBDs, data engineers can identify areas where redundancies are necessary to maintain data availability and system performance. This is crucial for critical systems where data must be available at all times. For example, in a data replication strategy, the RBD can help determine the number of replicas needed to achieve the desired level of reliability.</p>
<h3 id="failure-mode-analysis"><a class="header" href="#failure-mode-analysis">Failure Mode Analysis</a></h3>
<p>RBDs allow for the identification of single points of failure within the system. Understanding how individual components contribute to the overall system reliability enables data engineers to prioritize efforts in mitigating risks, such as adding backups, introducing data validation steps, or improving error handling mechanisms.</p>
<h3 id="scalability-and-maintenance-planning"><a class="header" href="#scalability-and-maintenance-planning">Scalability and Maintenance Planning</a></h3>
<p>As data systems scale, RBDs can be updated to reflect new components and dependencies, helping engineers plan for maintenance and scalability while minimizing the impact on reliability. This foresight ensures that the system can grow without compromising on performance or data integrity.</p>
<p>In summary, Reliability Block Diagrams offer a systematic approach for data engineers to design, analyze, and optimize data systems for reliability. By visualizing component dependencies and identifying critical points of failure, RBDs facilitate informed decision-making to enhance system robustness and ensure continuous data availability.</p>
<h2 id="failure-reporting-analysis-and-corrective-action-system-fracas"><a class="header" href="#failure-reporting-analysis-and-corrective-action-system-fracas">Failure Reporting, Analysis, and Corrective Action System (FRACAS)</a></h2>
<blockquote>
<p>FRACAS is a defined system or process for reporting, classifying, and analyzing failures, as well as planning corrective actions for such failures. It is part of the process to keep a history of analyses and actions taken.</p>
</blockquote>
<p>Implementing this process involves automating the analysis of data process logs, commits, pull requests, and tickets.</p>
<p>The FRACAS process is cyclical and follows the adapted FRACAS Kaizen Loop:</p>
<ul>
<li><strong>Failure Mode Analysis</strong>: Analysis of failure modes.</li>
<li><strong>Failure Codes Creation</strong>: Creation of failure codes or the methodology for classifying them.</li>
<li><strong>Work Order History Analysis</strong>: Analysis of the history of tickets sent to the data team.</li>
<li><strong>Root Cause Analysis</strong>: Analysis of root causes.</li>
<li><strong>Strategy Adjustment</strong>: Strategy adjustment.</li>
</ul>
<p>Here's how FRACAS could be applied in the context of data engineering:</p>
<h3 id="failure-reporting"><a class="header" href="#failure-reporting">Failure Reporting</a></h3>
<p>Data engineers and stakeholders report failures or anomalies detected in data processing tasks, data quality issues, or any incidents that affect the expected outcomes of data pipelines. This can be done through automated monitoring tools that alert the team to issues such as failed ETL jobs, discrepancies in data validation checks, or performance bottlenecks.</p>
<h3 id="analysis"><a class="header" href="#analysis">Analysis</a></h3>
<p>Once a failure is reported, it is analyzed to understand its nature, scope, and impact. This involves digging into logs, reviewing the data processing steps where the failure occurred, and identifying the specific point of failure. The analysis aims to classify the failure (e.g., data corruption, process failure, infrastructure issue) and understand the underlying reasons for the failure.</p>
<h3 id="corrective-action"><a class="header" href="#corrective-action">Corrective Action</a></h3>
<p>Based on the analysis, corrective actions are determined and implemented to fix the immediate issue. This could involve rerunning a failed job with corrected parameters, fixing a bug in the data transformation logic, or updating data validation rules to catch similar issues in the future.</p>
<h3 id="system-improvement"><a class="header" href="#system-improvement">System Improvement</a></h3>
<p>Beyond immediate corrective actions, FRACAS also focuses on systemic improvements to prevent similar failures from occurring. This could involve redesigning parts of the data pipeline for greater resilience, adding additional checks and balances in data validation, improving data quality monitoring, or enhancing the infrastructure for better performance and reliability.</p>
<h3 id="documentation-and-learning"><a class="header" href="#documentation-and-learning">Documentation and Learning</a></h3>
<p>All steps of the FRACAS process, from initial failure reporting to final corrective actions and system improvements, are documented. This documentation serves as a knowledge base for the data engineering team, helping them understand common failure modes, effective corrective actions, and best practices for designing more reliable data systems.</p>
<h3 id="continuous-improvement"><a class="header" href="#continuous-improvement">Continuous Improvement</a></h3>
<p>FRACAS is an iterative process. The learnings from each incident are fed back into the data engineering processes, leading to continuous improvement in data pipeline reliability and efficiency. Over time, this reduces the incidence of failures and improves the overall quality and trustworthiness of the data.</p>
<p>By applying FRACAS in data engineering, teams can move from reactive problem-solving to a proactive stance on improving data systems' reliability and efficiency, ultimately supporting better decision-making and operational performance across the organization.</p>
<h2 id="spare-parts-stocking-strategy"><a class="header" href="#spare-parts-stocking-strategy">Spare Parts Stocking Strategy</a></h2>
<blockquote>
<p>Ideally, clean data sources with complex transformations and cleanings, which save time and processing and can be used in multiple stages of multiple processes, will always be available. However, they may temporarily be unavailable or fail. Once such sources are identified and found to be critical to a system or process, it is prudent to have minimal cleaning and transformation tasks that work on raw data or sources of the source. These may not result in final data with the same level of detail but will be good enough.</p>
</blockquote>
<p>These tasks are not designed to be part of the normal process flow but are "spare parts" available for use when maintenance times are too long. The use of such tasks should be for the shortest time possible while the team has time to resolve failures in the original task or design its replacement.</p>
<p>In data engineering, a Spare Parts Stocking Strategy can be metaphorically applied to maintain high availability and reliability of data pipelines and systems. While in traditional contexts, this strategy involves keeping physical spare parts for machinery or equipment, in data engineering, it translates to having backup processes, data sources, and systems in place to ensure continuity in data operations. Here’s how it could be used:</p>
<h3 id="backup-data-processes"><a class="header" href="#backup-data-processes">Backup Data Processes</a></h3>
<p>Just as spare parts can replace failed components in machinery, backup data processes can take over when primary data processes fail. For example, if a primary ETL (Extract, Transform, Load) process fails due to an issue with a data source or transformation logic, a backup ETL process can be initiated. This backup process might use a different data source or a simplified transformation logic to ensure that essential data flows continue, albeit possibly at a reduced fidelity or completeness.</p>
<h3 id="redundant-data-sources"><a class="header" href="#redundant-data-sources">Redundant Data Sources</a></h3>
<p>Having alternate data sources is akin to having spare parts for critical components. If a primary data source becomes unavailable (e.g., due to an API outage or data corruption), the data engineering process can switch to a redundant data source to minimize downtime. This ensures that data pipelines are not entirely dependent on a single source and can continue operating even when one source fails.</p>
<h3 id="pre-processed-data-reservoirs"><a class="header" href="#pre-processed-data-reservoirs">Pre-Processed Data Reservoirs</a></h3>
<p>Maintaining pre-processed versions of critical datasets can be seen as having spare parts ready to be used immediately. In case of a processing failure in real-time data pipelines, these pre-processed datasets can be quickly utilized to ensure continuity in data availability for reporting, analytics, or other downstream processes.</p>
<h3 id="simplified-or-degraded-processing-modes"><a class="header" href="#simplified-or-degraded-processing-modes">Simplified or Degraded Processing Modes</a></h3>
<p>In situations where complex data processing cannot be performed due to system failures, having a simplified or degraded mode of operation can serve as a "spare part." This approach involves having predefined, less resource-intensive processes that can provide essential functionality or data outputs until the primary systems are restored.</p>
<h3 id="automated-failover-mechanisms"><a class="header" href="#automated-failover-mechanisms">Automated Failover Mechanisms</a></h3>
<p>Automated systems that can detect failures and switch to backup processes or systems without manual intervention can be seen as having an automated spare parts deployment system. These mechanisms ensure minimal disruption to data services by quickly responding to failures.</p>
<h3 id="documentation-and-testing"><a class="header" href="#documentation-and-testing">Documentation and Testing</a></h3>
<p>Just as spare parts need to be compatible and tested for specific machinery, backup data processes and sources need to be well-documented and regularly tested to ensure they can effectively replace primary processes when needed. Regular drills or simulations of failures can help ensure that the spare processes are ready to be deployed at a moment's notice.</p>
<p>By adopting a Spare Parts Stocking Strategy in data engineering, organizations can enhance the resilience of their data infrastructure, ensuring that data processing and availability are maintained even in the face of system failures or disruptions. This strategy is crucial for businesses where data availability directly impacts decision-making, operations, and customer satisfaction.</p>
<h2 id="availability-controls"><a class="header" href="#availability-controls">Availability Controls</a></h2>
<blockquote>
<p>Availability failures can occur for numerous reasons (from hardware to bugs), and some systems or processes are significant enough that availability controls should be implemented to ensure that certain services or data remain available when such failures occur.</p>
</blockquote>
<p>Availability controls range from using periodic data backups, snapshots, time travel, redundant processes, backup systems in local or cloud servers, etc.</p>
<p>Availability Controls in data engineering are mechanisms and strategies implemented to ensure that data and data processing capabilities are available when needed, particularly in the face of failures, maintenance, or unexpected demand spikes. These controls are crucial for maintaining the reliability and performance of data systems. Here's how they can be used in data engineering:</p>
<h3 id="data-backups"><a class="header" href="#data-backups">Data Backups</a></h3>
<p>Regular data backups are a fundamental availability control. By maintaining copies of critical datasets, data engineers can ensure that data can be restored in the event of corruption, accidental deletion, or data storage failures. Backups can be scheduled at regular intervals and stored in secure, geographically distributed locations to safeguard against site-specific disasters.</p>
<h3 id="redundant-data-storage"><a class="header" href="#redundant-data-storage">Redundant Data Storage</a></h3>
<p>Using redundant data storage solutions, such as RAID configurations in hardware or distributed file systems in cloud environments, can enhance data availability. These systems store copies of data across multiple disks or nodes, ensuring that the failure of a single component does not result in data loss and that data remains accessible even during partial system outages.</p>
<h3 id="high-availability-architectures"><a class="header" href="#high-availability-architectures">High Availability Architectures</a></h3>
<p>Designing data systems with high availability in mind involves deploying critical components in a redundant manner across multiple servers or clusters. This can include setting up active-active or active-passive configurations for databases, ensuring that if one instance fails, another can immediately take over without disrupting data access.</p>
<h3 id="disaster-recovery-plans"><a class="header" href="#disaster-recovery-plans">Disaster Recovery Plans</a></h3>
<p>Disaster recovery planning involves defining strategies and procedures for recovering from major incidents, such as natural disasters, cyber-attacks, or significant hardware failures. This includes not only data restoration from backups but also the rapid provisioning of replacement computing resources and network infrastructure.</p>
<h3 id="load-balancing-and-scaling"><a class="header" href="#load-balancing-and-scaling">Load Balancing and Scaling</a></h3>
<p>Load balancers distribute incoming data requests across multiple servers or services, preventing any single point from becoming overwhelmed, which could lead to failures and data unavailability. Similarly, implementing auto-scaling for data processing and storage resources can ensure that the system can handle varying loads, maintaining availability during peak demand periods.</p>
<h3 id="data-quality-gates"><a class="header" href="#data-quality-gates">Data Quality Gates</a></h3>
<p>Data quality gates are checkpoints in data pipelines where data is validated against predefined quality criteria. By ensuring that only accurate and complete data moves through the system, these gates help prevent errors and inconsistencies that could lead to processing failures and data unavailability.</p>
<h3 id="monitoring-and-alerting"><a class="header" href="#monitoring-and-alerting">Monitoring and Alerting</a></h3>
<p>Continuous monitoring of data systems and pipelines allows for the early detection of issues that could impact availability. Coupled with an alerting system, monitoring ensures that data engineers can quickly respond to and address potential failures, often before they impact end-users.</p>
<h3 id="versioning-and-data-immutability"><a class="header" href="#versioning-and-data-immutability">Versioning and Data Immutability</a></h3>
<p>Implementing data versioning and immutability can prevent data loss and ensure availability in the face of changes or updates. By keeping immutable historical versions of data, systems can revert to previous states if a new data version causes issues.</p>
<p>By employing these Availability Controls, data engineers can create resilient systems that ensure continuous access to data and data processing capabilities, critical for businesses that rely on timely and reliable data for operational decision-making and customer services.</p>
<h2 id="corrective-actions"><a class="header" href="#corrective-actions">Corrective Actions</a></h2>
<blockquote>
<p>Part of the CAPA (Corrective Action and Preventive Action Process), corrective actions (CAP - Corrective Action Process) consist of detecting failures, determining their root causes, corrective actions, and taking preventive measures to prevent the same failure from occurring for the same reasons. The complete definition is found in ISO 9001.</p>
</blockquote>
<p>Various tools and techniques are used in different industries for their application, including PDCA (Plan, Do, Check, Act), DMAIC (Define, Measure, Analyse, Improve, Control), 8D, etc. Generally, any tool, technique, or methodology is summarized in ISO 9001 in seven "steps":</p>
<ol>
<li><strong>Define the problem</strong>.This involves confirming that the problem is real and identifying the Who, What, When, Where, and Why. In the world of data engineering, this step should be as automated as possible, with the failure detected through sensors.</li>
<li><strong>Define the scope</strong>. It involves measuring the problem to be solved, knowing its frequency, which processes or tasks it affects, and which stakeholders are impacted. For data processes, many scope details should already be known information from the design of the processes and tasks, and the frequency can be determined from observability and FRACAS processes.</li>
<li><strong>Containment actions</strong>. These are specific measures adopted for the shortest possible time while working on a definitive solution to the failure. Such measures should already be designed in advance for each task or sub-task. The selection of measures should be automated; if not, they should be implemented immediately.</li>
<li><strong>Root cause identification</strong>. A clear, precise, and comprehensive diagnosis of the failure. Its documentation is part of the FRACAS.</li>
<li><strong>Corrective action planning</strong>. It involves planning corrective actions specifically based on the root cause.</li>
<li><strong>Implementation of corrective actions</strong>. It involves the final implementation of corrective actions in the process, which should automatically be available when similar failures occur.</li>
<li><strong>Follow-up on results</strong>. Documentation, communication, complete FRACAS.</li>
</ol>
<p>Corrective Actions in data engineering involve identifying, addressing, and mitigating the root causes of identified problems within data processes and systems to prevent their recurrence. This approach is systematic and is crucial for maintaining the integrity, reliability, and efficiency of data operations. Here’s how Corrective Actions can be applied in data engineering:</p>
<h3 id="identification-of-issues"><a class="header" href="#identification-of-issues">Identification of Issues</a></h3>
<p>The first step in the Corrective Action process is the accurate identification of issues within data systems. This could range from data quality problems, data pipeline failures, performance bottlenecks, to security vulnerabilities. Automated monitoring tools, data quality frameworks, and alerting systems play a vital role in early detection.</p>
<h3 id="root-cause-analysis-rca"><a class="header" href="#root-cause-analysis-rca">Root Cause Analysis (RCA)</a></h3>
<p>Once an issue is identified, a thorough Root Cause Analysis is conducted to understand the underlying cause of the problem. Techniques such as the Five Whys, fishbone diagrams, or Pareto analysis can be employed. For instance, if a data pipeline fails frequently due to specific data format inconsistencies, RCA would seek to uncover why these inconsistencies are occurring in the first place.</p>
<h3 id="planning-corrective-actions"><a class="header" href="#planning-corrective-actions">Planning Corrective Actions</a></h3>
<p>Based on the findings from the RCA, a plan for corrective actions is developed. This plan outlines the steps needed to address the root cause of the problem. In the data pipeline example, if the root cause is found to be incorrect data formatting at the source, a corrective action could involve implementing stricter data validation checks at the data ingestion stage.</p>
<h3 id="implementation-of-corrective-actions"><a class="header" href="#implementation-of-corrective-actions">Implementation of Corrective Actions</a></h3>
<p>The planned corrective actions are then implemented. This might involve modifying data validation rules, updating ETL scripts, enhancing data quality checks, or even redesigning parts of the data pipeline for better error handling and resilience.</p>
<h3 id="verification-and-monitoring"><a class="header" href="#verification-and-monitoring">Verification and Monitoring</a></h3>
<p>After the corrective actions are implemented, it’s crucial to verify their effectiveness in resolving the issue and to monitor the system for any unintended consequences. This could involve running test cases, monitoring data pipeline runs for a certain period, or employing data quality dashboards to ensure the issue does not recur.</p>
<h3 id="documentation-and-knowledge-sharing"><a class="header" href="#documentation-and-knowledge-sharing">Documentation and Knowledge Sharing</a></h3>
<p>All steps taken, from issue identification to the implementation of corrective actions and their outcomes, should be thoroughly documented. This documentation serves as a knowledge base for future reference and helps in sharing learnings across the data engineering team and wider organization. It contributes to building a culture of continuous improvement.</p>
<h3 id="preventive-measures"><a class="header" href="#preventive-measures">Preventive Measures</a></h3>
<p>Beyond addressing the immediate issue, the insights gained during the corrective action process can inform preventive measures to avoid similar issues in the future. This might include revising data handling policies, enhancing training for data engineers, or adopting new tools and technologies for better data management.</p>
<p>In data engineering, Corrective Actions are not just about fixing problems but also about improving processes and systems for long-term reliability and efficiency. By systematically addressing the root causes of issues, data teams can enhance the quality, security, and performance of their data infrastructure, supporting better decision-making and operational outcomes across the organization.</p>
<h2 id="antifragility"><a class="header" href="#antifragility">Antifragility</a></h2>
<blockquote>
<p>Inspired by Nassim Nicholas Taleb's book <em>Antifragile: Things That Gain from Disorder</em>, antifragility differs from resilience or robustness concepts, where systems seek to maintain their reliability level. Instead, from their design, systems increase their reliability concerning the system's inputs.</p>
</blockquote>
<p>Antifragility proposes a system design change, which are commonly designed to be fragile, meaning they will fail if operated outside their requirements. Antifragility suggests the opposite, designing systems that improve when exposed to loads outside of the requirements. In this sense, systems are not only designed to respond to the expected or anticipated but interact with their environment in real-time and adapt to it.</p>
<p>Examples of antifragile systems:</p>
<ul>
<li>Self-healing</li>
<li>Real time sensoring, monitoring</li>
<li>Live FRACAS</li>
<li>System Health Management</li>
<li>Automatic Repair</li>
</ul>
<p>Methods such as <strong>Real-Time Anomaly Detection and Adaptation</strong> and <strong>Adaptive Load Balancing</strong> might interest data teams, but they are not covered in this book. Adaptive Load Balancing, in particular, might be a interesting topic for Data Platform or Data DevOps teams.</p>
<h2 id="bulkhead-pattern"><a class="header" href="#bulkhead-pattern">Bulkhead Pattern</a></h2>
<blockquote>
<p>In the nautical world, we find bulkheads, wooden plates found in ships, designed to prevent the ship from sinking when a portion of the hull is compromised. The Bulkhead Pattern adapts exactly this idea, that a failure in one portion of the system should not compromise the entire system.</p>
</blockquote>
<p>This design pattern is commonly applied in software development, consisting of not overloading a service with more calls than it can handle at a given time, an example of this is Netflix's Hystrix system.</p>
<p>In the context of data engineering, the Bulkhead Pattern involves segmenting data processing tasks, resources, and services into isolated units so that a failure in one area does not cascade and disrupt the entire system. Here's how it could be used:</p>
<h3 id="segmenting-data-pipelines"><a class="header" href="#segmenting-data-pipelines">Segmenting Data Pipelines</a></h3>
<p>Data pipelines can be divided into independent segments or modules, each handling a specific part of the data processing workflow. If one segment encounters an issue, such as an unexpected data format or a processing error, it can be addressed or bypassed without halting the entire pipeline. This approach ensures that other data processing activities continue unaffected, maintaining overall system availability and reliability.</p>
<h3 id="isolating-services-and-resources"><a class="header" href="#isolating-services-and-resources">Isolating Services and Resources</a></h3>
<p>In a microservices architecture, each data service (e.g., data ingestion, transformation, and storage services) can be isolated, ensuring that issues in one service don't impact others. Similarly, resources like databases and compute instances can be dedicated to specific tasks or services. If one service or resource fails or becomes overloaded, it won't drag down the others, helping maintain the stability of the broader data platform.</p>
<h3 id="rate-limiting-and-throttling"><a class="header" href="#rate-limiting-and-throttling">Rate Limiting and Throttling</a></h3>
<p>Applying rate limiting to APIs and data ingestion endpoints can prevent any single user or service from consuming too many resources, which could lead to system-wide failures. By throttling the number of requests or the amount of data processed within a given timeframe, the system can remain stable even under high load, protecting against cascading failures.</p>
<h3 id="implementing-circuit-breakers"><a class="header" href="#implementing-circuit-breakers">Implementing Circuit Breakers</a></h3>
<p>Circuit breakers can temporarily halt the flow of data or requests to a service or component when a failure is detected, similar to how a bulkhead would seal off a damaged section of a ship. Once the issue is resolved, or after a certain timeout, the circuit breaker can reset, allowing the normal operation to resume. This prevents repeated failures and gives the system time to recover.</p>
<h3 id="use-of-containers-and-virtualization"><a class="header" href="#use-of-containers-and-virtualization">Use of Containers and Virtualization</a></h3>
<p>Deploying data services and applications in containers or virtualized environments can provide natural isolation, acting as bulkheads. If one containerized component fails, it can be restarted or replaced without affecting others, ensuring that the overall system remains operational.</p>
<p>By employing the Bulkhead Pattern in data engineering, organizations can build more resilient data systems that are capable of withstanding localized issues without widespread impact, ensuring continuous data processing and availability.</p>
<h2 id="cold-standby"><a class="header" href="#cold-standby">Cold Standby</a></h2>
<p>Cold Standby is a redundancy technique used in data reliability engineering and system design to ensure high availability and continuity of service in the event of system failure. Unlike hot standby or warm standby, where backup systems or components are kept running or at a near-ready state, in cold standby, the backup systems are kept fully offline and are <em>only activated when the primary system fails or during maintenance periods</em>. Here’s a deeper look into cold standby:</p>
<ul>
<li><strong>Fully Offline</strong>: The standby system is not running during normal operations; it's fully powered down or in a dormant state.</li>
<li><strong>Manual Activation</strong>: Switching to the cold standby system often requires manual intervention to bring the system online, configure it, and start the services.</li>
<li><strong>Data Synchronization</strong>: Data is not continuously synchronized between the primary and cold standby systems. Instead, data is periodically backed up and would need to be restored on the cold standby system upon activation.</li>
<li><strong>Cost-Effective</strong>: Because the standby system is not running, it doesn't incur costs for power or compute resources during normal operations, making it a cost-effective solution for non-critical applications or where downtime can be tolerated for longer periods.</li>
</ul>
<p>Cold standby systems are typically used in scenarios where high availability is not critically required, or the cost of maintaining a hot or warm standby system cannot be justified. Examples include non-critical batch processing systems, archival systems, or in environments where budget constraints do not allow for more sophisticated redundancy setups.</p>
<p>Implementation considerations:</p>
<ul>
<li><strong>Recovery Time</strong>: The time to recover services using a cold standby can be significant since the system needs to be powered up, configured, and data may need to be restored from backups. This recovery time should be considered in the system's SLA (Service Level Agreement).</li>
<li><strong>Regular Testing</strong>: Regular drills or tests should be conducted to ensure that the cold standby system can be brought online effectively and within the expected time frame.</li>
<li><strong>Data Loss Risk</strong>: Given that data synchronization is not continuous, there is a risk of data loss for transactions or data changes that occurred after the last backup. This risk needs to be assessed and mitigated through frequent backups or other means.</li>
<li><strong>Manual Processes</strong>: The need for manual intervention to activate cold standby systems requires well-documented procedures and trained personnel to ensure a smooth transition during a failure event.</li>
</ul>
<p>Cold Standby is a fundamental concept in designing resilient and reliable systems, especially when balancing the need for availability with cost constraints. It provides a basic level of redundancy that can be suitable for certain applications and scenarios in data reliability engineering.</p>
<h2 id="single-point-of-failure-spof"><a class="header" href="#single-point-of-failure-spof">Single Point of Failure (SPOF)</a></h2>
<p>Eliminating Single Point of Failure (SPOF) is a critical strategy in data reliability engineering aimed at enhancing the resilience and availability of data systems. A Single Point of Failure refers to <em>any component, system, or aspect of the infrastructure whose failure would lead to the failure of the entire system</em>. This could be a database, a network component, a server, or even a piece of software that is critical to data processing or storage.</p>
<p>The goal of eliminating SPOFs is to ensure that no single failure can disrupt the entire service or data flow. This is achieved through redundancy, fault tolerance, and careful system design. Here’s how it relates to data reliability:</p>
<h3 id="redundancy-1"><a class="header" href="#redundancy-1">Redundancy</a></h3>
<p>Introducing redundancy involves duplicating critical components or services so that if one fails, the other can take over without interruption. For example, having multiple data servers, redundant network paths, or replicated databases can prevent downtime caused by the failure of any single component.</p>
<h3 id="fault-tolerance-2"><a class="header" href="#fault-tolerance-2">Fault Tolerance</a></h3>
<p>Building systems to be fault-tolerant means they can continue operating correctly even if some components fail. This might involve implementing software that can reroute data flows away from failed components or hardware that can automatically switch to backup systems.</p>
<h3 id="distributed-architectures"><a class="header" href="#distributed-architectures">Distributed Architectures</a></h3>
<p>Designing systems with distributed architectures can spread out the risk, so no single component's failure can affect the entire system. For example, using cloud services that distribute data and processing across multiple geographical locations can safeguard against regional outages.</p>
<h3 id="regular-testing"><a class="header" href="#regular-testing">Regular Testing</a></h3>
<p>Regularly testing the failover and recovery processes is essential to ensure that redundancy measures work as expected when a real failure occurs. This can include disaster recovery drills and using chaos engineering principles to intentionally introduce failures.</p>
<h3 id="continuous-monitoring-and-alerting"><a class="header" href="#continuous-monitoring-and-alerting">Continuous Monitoring and Alerting</a></h3>
<p>Implementing continuous monitoring and alerting systems helps in the early detection of potential SPOFs before they cause system-wide failures. Monitoring can identify over-utilized resources, impending hardware failures, or software errors that could become SPOFs if not addressed.</p>
<p>By eliminating Single Points of Failure, data engineering teams can create more robust and reliable systems that can withstand individual component failures without significant impact on the overall system performance or data availability. This approach is fundamental to maintaining high levels of service and ensuring that data-driven operations can proceed without interruption.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="calidad-de-datos"><a class="header" href="#calidad-de-datos">Calidad de Datos</a></h1>
<h2 id="fundamentos-de-la-calidad-de-datos"><a class="header" href="#fundamentos-de-la-calidad-de-datos">Fundamentos de la Calidad de Datos</a></h2>
<h3 id="ciclo-de-vida-de-los-datos"><a class="header" href="#ciclo-de-vida-de-los-datos">Ciclo de vida de los datos</a></h3>
<h3 id="dama"><a class="header" href="#dama">DAMA</a></h3>
<h3 id="posmad"><a class="header" href="#posmad">POSMAD</a></h3>
<h3 id="cobit"><a class="header" href="#cobit">COBIT</a></h3>
<h3 id="gobierno-versus-gestión-de-los-datos"><a class="header" href="#gobierno-versus-gestión-de-los-datos">Gobierno versus Gestión de los datos</a></h3>
<h2 id="datos-maestros"><a class="header" href="#datos-maestros">Datos Maestros</a></h2>
<h3 id="arquitectura-mdm"><a class="header" href="#arquitectura-mdm">Arquitectura MDM</a></h3>
<h3 id="modelo-de-madurez"><a class="header" href="#modelo-de-madurez">Modelo de madurez</a></h3>
<h3 id="estándares"><a class="header" href="#estándares">Estándares</a></h3>
<h4 id="iso-8000"><a class="header" href="#iso-8000">ISO 8000</a></h4>
<h4 id="isoiec-22745"><a class="header" href="#isoiec-22745">ISO/IEC 22745</a></h4>
<h2 id="calidad-de-los-procesos-de-datos"><a class="header" href="#calidad-de-los-procesos-de-datos">Calidad de los procesos de datos</a></h2>
<h3 id="dama-dmbok"><a class="header" href="#dama-dmbok">DAMA DMBOK</a></h3>
<h3 id="modelo-de-aiken"><a class="header" href="#modelo-de-aiken">Modelo de Aiken</a></h3>
<h3 id="data-management-maturity-model-dmm"><a class="header" href="#data-management-maturity-model-dmm">Data Management Maturity Model (DMM)</a></h3>
<h3 id="modelo-ibm"><a class="header" href="#modelo-ibm">Modelo IBM</a></h3>
<h3 id="modelo-de-gartner"><a class="header" href="#modelo-de-gartner">Modelo de Gartner</a></h3>
<h3 id="tqdm"><a class="header" href="#tqdm">TQDM</a></h3>
<h3 id="dcam"><a class="header" href="#dcam">DCAM</a></h3>
<h3 id="modelo-mamd"><a class="header" href="#modelo-mamd">Modelo MAMD</a></h3>
<h2 id="modelos-de-calidad-de-datos"><a class="header" href="#modelos-de-calidad-de-datos">Modelos de calidad de datos</a></h2>
<h3 id="modelo-de-calidad-de-datos"><a class="header" href="#modelo-de-calidad-de-datos">Modelo de calidad de datos</a></h3>
<h3 id="medidas-de-calidad-de-datos"><a class="header" href="#medidas-de-calidad-de-datos">Medidas de calidad de datos</a></h3>
<h3 id="proceso-de-evaluación"><a class="header" href="#proceso-de-evaluación">Proceso de evaluación</a></h3>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="confiabilidad-de-datos"><a class="header" href="#confiabilidad-de-datos">Confiabilidad de Datos</a></h1>
<blockquote>
<p>Tomándose por base los fundamentos y metodologías desarrolladas por los <strong>Site Reliability Engineers</strong>, conocidos como los “firefighters” del mundo de la ingeniería de sistemas, los cuales construyen sistemas automatizados para optimizar la disponibilidad de las aplicaciones (reducir downtime), definiremos <strong>Data Reliability</strong> cómo la capacidad del equipo de data en entregar alta disponibilidad de la data durante todo el ciclo de vida de la misma. En resumen, garantizar los periodos de tiempo que la data no presenta inacurácia, no es faltante ni errónea.</p>
</blockquote>
<p>...</p>
<h2 id="consecuencias-de-la-confiabilidad"><a class="header" href="#consecuencias-de-la-confiabilidad">Consecuencias de la confiabilidad</a></h2>
<p>Consecuencias (en administrar el downtime de la data):</p>
<ul>
<li>Los equipos de data reducen de manera muy importante el tiempo perdido en “apagar incendios”, escalaciones y troubleshooting de la data. Utilizan ese tiempo para enfocarse en la construcción de una buena infraestructura, y en agregar valor a la data.</li>
<li>Los equipos de data son más rápidos en actualizar y modificar la infraestructura de la data, ya que tienen claro la confiabilidad del sistema.</li>
<li>Los equipos de data ganan el respeto y la confianza de los stakeholders, ya que entregan datos confiables de manera consistente.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="data-architecture-1"><a class="header" href="#data-architecture-1">Data Architecture</a></h1>
<blockquote>
<p>Data Architecture is about how the data is managed, from <strong>collection</strong>, <strong>transformations</strong>, <strong>distribution</strong>, and <strong>consumption</strong>.</p>
</blockquote>
<p>It includes the models, policies, rules, and standards that govern which data is collected and how it is stored, arranged, integrated, and put to use in data systems and in organizations.</p>
<p>The data architecture aims to set standards to all data systems, and the interaction between them. It also dictates and materialises the organization understanding of its business in a Conceptual, Logical, and Physical level.</p>
<p>The Zachman Framework for enterprise architecture, understands the data architecture in five layers:</p>
<ol>
<li>Scope/Contextual: subjects and architectural standards important for the business.</li>
<li>Business/Conceptual: business entities, its attributes and associations.</li>
<li>System/Logical: how entities are related.</li>
<li>Technology/Physical: representation of a data design as implemented in a database management system.</li>
<li>Detailed Representations: databases.</li>
</ol>
<h2 id="conceptual-layer"><a class="header" href="#conceptual-layer">Conceptual Layer</a></h2>
<blockquote>
<p>Represents all <strong>Business Entities</strong>.</p>
</blockquote>
<p>The <strong>Conceptual Data Model</strong> (CDM) consists of the <strong>Business Entities</strong>, like <strong>User</strong>, <strong>Branch</strong>, <strong>Product</strong>.
The business entities (or business objects) carry <em>attributes</em> (name, identifiers, timestamps, etc.), and <em>associations</em> (relationships) with other business entities.
The complete set of business entities represents the business relationships.</p>
<p>From a data architecture perspective, these business entities are represented in a <strong>Conceptual Schema</strong>, which consists of a map of <strong>concepts</strong> (business entities) and their <strong>relationships</strong> in a database, normally in a <strong>Data Structure Diagram</strong> (DSD). It may also include <strong>Enterprise Data Modelling</strong> (EDM) outputs like  entity–relationship diagrams (ERDs), XML schemas (XSD), and an enterprise wide data dictionary.</p>
<p>The conceptual schema describes the semantics of an organization, and represents a series of assertions about its nature. It describes the objects of significance (business entities), of which the organization is ineterested in collecting information of, its characteristics (attributes), and the associations between each pair of objects of significance (relationships).
Please note that it's not the actual database design, and it is represented in different abrastraction layers.</p>
<p>Examples:</p>
<ul>
<li>Each ORDER must be from one and only one USER.</li>
<li>Each ORDER contains one or more PRODUCTS.</li>
<li>Each ORDER contains products from one or more BRANCHES.</li>
</ul>
<h2 id="logical-layer"><a class="header" href="#logical-layer">Logical Layer</a></h2>
<blockquote>
<p>Represents the logic and how the entities are related.</p>
</blockquote>
<p>The <strong>Logical Data Model</strong> (LDM), also known as Domain Model, represents the abstract structure of a domain of information, expressed independently of a particular database management product or storage technology (physical data model), but in terms of data structures such as relational tables and columns, object-oriented classes, or XML tags.</p>
<p>Once validated and approved, the logical data model can become the basis of a physical data model and form the design of a database.</p>
<h2 id="physical-layer"><a class="header" href="#physical-layer">Physical Layer</a></h2>
<blockquote>
<p>The representation of a data design as implemented, or intended to be implemented, in a database management system.</p>
</blockquote>
<p>The <strong>Physical Data Model</strong> (PDM) typically derives from a logical data model (LDM), though it may be reverse-engineered from a given database implementation. A complete physical data model will include all the database artifacts required to create relationships between tables or to achieve performance goals, such as indexes, constraint definitions, linking tables, partitioned tables or clusters.</p>
<h3 id="cdm-vs-ldm-vs-pdm"><a class="header" href="#cdm-vs-ldm-vs-pdm">CDM vs LDM vs PDM</a></h3>
<h4 id="data-constructs"><a class="header" href="#data-constructs">Data Constructs</a></h4>
<ul>
<li>CDM: uses general high-level data constructs from which Architectural Descriptions are created in non-technical terms.</li>
<li>LDM: includes entities (tables), attributes (columns/fields) and relationships (keys). Is independent of technology (platform, DBMS).</li>
<li>PDM:  includes tables, columns, keys, data types, validation rules, database triggers, stored procedures, domains, and access constraints, as primary keys and indices for fast data access.</li>
</ul>
<h4 id="naming-conventions"><a class="header" href="#naming-conventions">Naming Conventions</a></h4>
<ul>
<li>CDM: non-technical names, so that executives and managers at all levels can understand the data basis of Architectural Description.</li>
<li>LDM: uses business names for entities &amp; attributes.</li>
<li>PDM: uses more defined and less generic specific names for tables and columns, such as abbreviated column names, limited by the database management system (DBMS) and any company defined standards.</li>
</ul>
<h2 id="modern-data-architecture"><a class="header" href="#modern-data-architecture">Modern Data Architecture</a></h2>
<p>A modern approach to data architecture, extensivelly adapted by StartUps, reduces, restricts or understands the data architecture as the implementation of a Data Warehouse, a Data Lake + Data Warehouse, or a Data Lakehouse architecture, consisting of the data sources plus two or three tiers:</p>
<ul>
<li>Bottom/Data Tier: data warehouse server with functional gateway (ODBC, JDBC, etc.). May also include the data lake.</li>
<li>Middle/Application Tier: houses the business logic used to process user inputs (OLAP Servers, Snowflake, Apache Redshift, Databricks Data Lakehouse Platform, Apache Spark, etc.).</li>
<li>Top/Presentation Tier: front-end tools (Power BI, Tableau, etc.).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="data-lake"><a class="header" href="#data-lake">Data Lake</a></h1>
<blockquote>
<p>A Data Lake is a data repository for storing large amounts of Structured, Semi-Structured, and Unstructured data.</p>
</blockquote>
<p>It is a repository for storing all types of data in its native format without fixed limits on account size or file. Data Lake stores a high data quantity to increase native integration and analytic performance.
The Data Lake democratizes data and provides a cost-effective way of storing all organization data for later processing.</p>
<ul>
<li><a href="concepts/data_lake.html#data-lake-vs-data-warehouse">Data Lake vs Data Warehouse</a></li>
<li><a href="concepts/data_lake.html#goals">Goals</a></li>
<li><a href="concepts/data_lake.html#data-lake-architecture">Data Lake Architecture</a>
<ul>
<li><a href="concepts/data_lake.html#layers">Layers</a>
<ul>
<li><a href="concepts/data_lake.html#ingestion-layer-bronze">Ingestion Layer (Bronze)</a></li>
<li><a href="concepts/data_lake.html#distillation-layer-silver">Distillation Layer (Silver)</a></li>
<li><a href="concepts/data_lake.html#processing-layer-gold">Processing Layer (Gold)</a></li>
<li><a href="concepts/data_lake.html#insights-layer">Insights Layer</a></li>
<li><a href="concepts/data_lake.html#unified-operations-layer">Unified Operations Layer</a></li>
</ul>
</li>
<li><a href="concepts/data_lake.html#zones">Zones</a>
<ul>
<li><a href="concepts/data_lake.html#landing-zone">Landing Zone</a></li>
<li><a href="concepts/data_lake.html#raw-zone">Raw Zone</a></li>
<li><a href="concepts/data_lake.html#harmonized-zone">Harmonized Zone</a></li>
<li><a href="concepts/data_lake.html#distilled-zone">Distilled Zone</a></li>
<li><a href="concepts/data_lake.html#explorative-zone">Explorative Zone</a></li>
<li><a href="concepts/data_lake.html#delivery-zone">Delivery Zone</a></li>
<li><a href="concepts/data_lake.html#zones-comparisom">Zones Comparisom</a></li>
</ul>
</li>
<li><a href="concepts/data_lake.html#sandbox">Sandbox</a></li>
<li><a href="concepts/data_lake.html#maturity-stages">Maturity Stages</a>
<ul>
<li><a href="concepts/data_lake.html#handle-and-ingest-data-at-scale">Handle and Ingest data at scale</a></li>
<li><a href="concepts/data_lake.html#building-the-analytical-muscle">Building the analytical muscle</a></li>
<li><a href="concepts/data_lake.html#data-warehouse-and-data-lake-work-in-unison">Data Warehouse and Data Lake work in unison</a></li>
<li><a href="concepts/data_lake.html#enterprise-capability-in-the-lake">Enterprise capability in the lake</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="concepts/data_lake.html#key-components-of-data-lake-architecture">Key Components of Data Lake Architecture</a>
<ul>
<li><a href="concepts/data_lake.html#data-ingestion">Data Ingestion</a></li>
<li><a href="concepts/data_lake.html#data-storage">Data Storage</a></li>
<li><a href="concepts/data_lake.html#data-governance">Data Governance</a></li>
<li><a href="concepts/data_lake.html#security">Security</a></li>
<li><a href="concepts/data_lake.html#data-quality">Data Quality</a></li>
<li><a href="concepts/data_lake.html#data-discovery">Data Discovery</a></li>
<li><a href="concepts/data_lake.html#data-auditing">Data Auditing</a></li>
<li><a href="concepts/data_lake.html#data-lineage">Data Lineage</a></li>
<li><a href="concepts/data_lake.html#data-exploration">Data Exploration</a></li>
</ul>
</li>
<li><a href="concepts/data_lake.html#other-architecture-approaches">Other Architecture Approaches</a></li>
</ul>
<h2 id="data-lake-vs-data-warehouse"><a class="header" href="#data-lake-vs-data-warehouse">Data Lake vs Data Warehouse</a></h2>
<blockquote>
<p>A Data Warehouse is a repository that exclusively keeps pre-processed data from a Data Lake and many databases.</p>
</blockquote>
<p>Data Warehouses store data in a hierarchical format using files and folders. This is not the case with a Data Lake as it has flat architecture. In a Data Lake, every data element is identified by a unique identifier and a set of metadata information.</p>
<h2 id="goals"><a class="header" href="#goals">Goals</a></h2>
<blockquote>
<p>Building and maintaining a Data Lake have five main goals: unifying the data, full query access, performance and scalability, progression, and costs.</p>
</blockquote>
<p><strong>Unification</strong>: Data Lake is a perfect solution to accumulate all the data from distinct data sources (ERP, CRM, logs, data partners data, internal generated data) in one place. The Data Lake architecture makes it easier for companies to get a holistic view of data and generate insights from it.</p>
<p><strong>Full Query Access</strong>: storing data in Data Lakes allows full access to data that can be directly used by BI tools to pull data whenever needed. ELT process is a flexible, reliable, and fast way to load data into Data Lake and then use it with other tools.</p>
<p><strong>Performance and Scalability</strong>: Data Lake Architecture supports fast query processing. It enables users to perform ad hoc analytical queries independent of the production environment. Data Lake provides faster querying and makes it easier to scale up and down. Data Lake offer business agility.</p>
<p><strong>Progression</strong>: getting data in one place is a necessary step before progressing to other stages because loading data from one source makes it easier to work with BI tools. Data Lake helps you make data cleaner and error-free data that has less repetition.</p>
<p><strong>Costs</strong>: S3 repositories are a cost-efficient storage of large volumes of data.</p>
<h2 id="data-lake-architecture"><a class="header" href="#data-lake-architecture">Data Lake Architecture</a></h2>
<div style="position: relative; width: 100%; height: 0; padding-top: 50.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbUxswrb4&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbUxswrb4&#x2F;view?utm_content=DAFbUxswrb4&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Data Lake Architecture Framework.</p></a>
<blockquote>
<p>Data Lakes are often structured in zones or layers models. These models define in which processing degrees (raw, cleansed, aggregated) data are available in the data lake, and how they are governed (regarding access rights, data quality, and responsibilities).</p>
</blockquote>
<p>Zones are similar to the layers in data warehousing, but data may not move through all zones or even move back.</p>
<h3 id="layers"><a class="header" href="#layers">Layers</a></h3>
<div style="position: relative; width: 100%; height: 0; padding-top: 35.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbVHIphys&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbVHIphys&#x2F;view?utm_content=DAFbVHIphys&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Data Lake Layers.</p></a>
<p>The following data lake model approach is structured in layers:</p>
<ul>
<li>Ingestion Layer</li>
<li>Distilation Layer</li>
<li>Processing Layer</li>
<li>Insights Layer</li>
<li>Unified Operations Layer</li>
</ul>
<p>The <strong>Raw Data</strong> entering the Data Lake consists of the organizations internal data (Operational Systems), specially relational data from databases, also streaming and batch data from data partners.</p>
<p>In the other extreme, representing the data leaving the Data Lake, the <strong>Business Systems</strong>, consists of databases, the Data Warehouse, dashboards, reports, and external data connections.</p>
<p>The first three layers constitute the medallion architecture, which is a is a data design pattern used to logically organize data in a Data Lake (similar as in a Data Lakehouse), with the goal of incrementally and progressively improving the structure and quality of data as it flows through each layer of the architecture (from Bronze ⇒ Silver ⇒ Gold layer tables).
Medallion architectures are sometimes also referred to as "multi-hop" architectures.</p>
<p>You can see more details regarding this architecture approach in the <a href="https://www.researchgate.net/profile/Sidharth-S-Prakash/publication/343219651_Evolution_of_Data_Warehouses_to_Data_Lakes_for_Enterprise_Business_Intelligence/links/5f1d52ad92851cd5fa48958a/Evolution-of-Data-Warehouses-to-Data-Lakes-for-Enterprise-Business-Intelligence.pdf">article</a>{{Prakash, S. S. (2020). Evolution of Data Warehouses to Data Lakes for Enterprise Business Intelligence. Evolution, 8(4).}} "Evolution of Data Warehouses to Data Lakes for Enterprise Business Intelligence".</p>
<h4 id="ingestion-layer-bronze"><a class="header" href="#ingestion-layer-bronze">Ingestion Layer (Bronze)</a></h4>
<blockquote>
<p>The purpose of the Ingestion Layer of the Data Lake Architecture is to ingest raw data into the Data Lake. There is no data modification in this layer. This is where we land all the data from external source systems.</p>
</blockquote>
<p>The table structures in this layer correspond to the source system table structures "as-is," along with any additional metadata columns that capture the load date/time, process ID, etc.
The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.</p>
<p>The layer can ingest raw data in real-time or in batches, which is in turn organized into a logical folder structure.
The Ingestion Layer can pull data from different external sources, like social media platforms.</p>
<h4 id="distillation-layer-silver"><a class="header" href="#distillation-layer-silver">Distillation Layer (Silver)</a></h4>
<blockquote>
<p>The purpose of the Distillation Layer of the Data Lake Architecture is to convert the data stored in the Ingestion (Bronze) Layer in a Structured format for analytics.</p>
</blockquote>
<p>The data is matched, denormalized, merged, conformed, cleansed, and derive "just-enough" so that the Silver layer can provide an "Enterprise view" of all its key business entities, concepts and transactions (for example, master customers, stores, non-duplicated transactions and cross-reference tables).
The data in this layer becomes uniform in terms of format, encoding, and data type (parquet).</p>
<p>The Silver layer brings the data from different sources into an Enterprise view and enables self-service analytics for ad-hoc reporting, advanced analytics and ML.
It serves as a source for Departmental Analysts, Data Engineers and Data Scientists to further create projects and analysis to answer business problems via enterprise and departmental data projects in the Gold Layer.</p>
<p>In the lakehouse data engineering paradigm (of which we’re extending to the Data Lake), typically the ELT methodology is followed vs. ETL - which means only minimal or "just-enough" transformations and data cleansing rules are applied while loading the Silver layer.
Speed and agility to ingest and deliver the data in the data lake is prioritized, and a lot of project-specific complex transformations and business rules are applied while loading the data from the Silver to Gold layer.
From a data modeling perspective, the Silver Layer has more 3rd-Normal Form like data models. Data Vault-like, write-performant data models can be used in this layer.</p>
<h4 id="processing-layer-gold"><a class="header" href="#processing-layer-gold">Processing Layer (Gold)</a></h4>
<blockquote>
<p>This layer of the Data Lake Architecture executes user queries and advanced analytical tools on the Structured Data.</p>
</blockquote>
<p>The processes can be run in batch, in real-time, or interactively.
It is the layer that implements the business logic and analytical applications consume the data.
It is also known as the Trusted, Gold, or Production-Ready Layer.</p>
<p>It is typically organized in consumption-ready "project-specific" databases.
The Gold layer is for reporting and uses more de-normalized and read-optimized data models with fewer joins.
The final layer of data transformations and data quality rules are applied here.
Final presentation layer of projects such as Customer Analytics, Product Quality Analytics, Inventory Analytics, Customer Segmentation, Product Recommendations, Marking/Sales Analytics etc. fit in this layer.
We see a lot of Kimball style star schema-based data models or Inmon style Data marts fit.</p>
<p>Often, the Data Marts (and Data Warehouse data) from the traditional RDBMS technology stack are ingested into the Gold layer.</p>
<h4 id="insights-layer"><a class="header" href="#insights-layer">Insights Layer</a></h4>
<blockquote>
<p>This layer of the Data Lake Architecture acts as the query interface, or the output interface, of the Data Lake.</p>
</blockquote>
<p>It uses SQL and NoSQL queries to request or fetch data from the Data Lake.
The queries are normally executed by company users who need access to the data.
Once the data is fetched from the Data Lake, it is the same layer that displays it to the user for viewing.</p>
<p>Some examples include Amazon QuickSight, an AWS native BI tool and allows users to connect with software-as-a-service (SaaS) applications such as Salesforce or ServiceNow, third-party databases such as MySQL, Postgres, and SQL Server, as well as native AWS services including Amazon Athena, an interactive query service that allows them to analyze unstructured data in Amazon S3 data lakes using standard SQL queries.
While QuickSight doesn’t connect directly to the data lake, integration with Amazon Athena allows BI users to query data inside the lake without having to move data or build an ETL pipeline.</p>
<h4 id="unified-operations-layer"><a class="header" href="#unified-operations-layer">Unified Operations Layer</a></h4>
<blockquote>
<p>This layer governs system management and monitoring.</p>
</blockquote>
<p>It includes auditing and proficiency management, data management, workflow management. AWS data lake environments and monitoring tools and best practices are described in this <a href="https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/monitoring-optimizing-data-lake-environment.html">article</a><sup><a name="to-footnote-1"><a href="concepts/data_lake.html#footnote-1">1</a></a></sup> from AWS.</p>
<h3 id="zones"><a class="header" href="#zones">Zones</a></h3>
<div style="position: relative; width: 100%; height: 0; padding-top: 35.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbVKs6Or4&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbVKs6Or4&#x2F;view?utm_content=DAFbVKs6Or4&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Zone Reference Model for Enterprise-Grade Data Lake Management - Data Flow.</p></a>
<p>The following data lake approach is explored by the <a href="https://www.ipvs.uni-stuttgart.de/departments/as/publications/giebleca/20_zoneReferenceModel_EDOC_Preprint.pdf">University of Stuttgart and Bosch GmbH</a><sup><a name="to-footnote-2"><a href="concepts/data_lake.html#footnote-2">2</a></a></sup>, and known as Zone Reference Model for Enterprise-Grade Data Lake Management. It consists of:</p>
<ul>
<li>Landing Zone</li>
<li>Raw Zone</li>
<li>Harmonized Zone</li>
<li>Distilled Zone</li>
<li>Delivery Zone</li>
<li>Explorative Zone</li>
</ul>
<p><img src="concepts/../mdbook-plantuml-img/b510a51550cb2a65e78ebfbb460618d52bd15e92.svg" alt="" /></p>
<p style="text-align: center;">Zone Reference Model for Enterprise-Grade Data Lake Management - Meta-model for zones - Attributes.</p>
<p>Regarding how a zone interacts with other zones and the outside world, we have:</p>
<ul>
<li><strong>Zone</strong> <em>receives</em> data from another <strong>Zone</strong></li>
<li><strong>Zone</strong> <em>forwards</em> data to another <strong>Zone</strong></li>
<li><strong>Zone</strong> <em>imports</em> data from <strong>Data Source</strong></li>
<li><strong>Zone</strong> <em>exports</em> data to <strong>Data Sink</strong></li>
</ul>
<p>All zones contain a protected part.
This part is encrypted and secured, and stores data that need extensive protection (for example, PII, personal data). Data wander from the protected part of one zone to the protected part of the next zone.
They may only leave the protected part after being desensitized (for example, by anonymization).
Data in this part are subject to strict access controls and governance.
The protected part shares all other characteristics with the rest of the zone it is in.</p>
<h4 id="landing-zone"><a class="header" href="#landing-zone">Landing Zone</a></h4>
<blockquote>
<p>The Landing Zone is the first zone of the data lake. Data are ingested as batch or as data stream from the sources.</p>
</blockquote>
<p>The Landing Zone is beneficial when the requirements of the ingested data and those of the Raw Zone diverge.
For example, data might need to be ingested at a vast rate due to its volume and velocity.
If the technical implementation of the Raw Zone cannot provide this high ingestion rate, a Landing Zone can function as a mediator in between: data are ingested at a high rate into the Landing Zone, and then are forwarded to the Raw Zone as batches.
Examples of data coming to the Landing zone include data streamed by Kafka, Amazon Kinesis, Amazon SQS, RabbitMQ, Apache Spark, etc.</p>
<p>For the data characteristics, data ingested into the Landing Zone remains mostly raw.
Their granularity remains raw, just like in the source systems.
The schema of the data is not changed; they can simply be copied in their source system format.
However, their syntax might be changed.
Basic transformations are allowed upon ingestion into the Landing Zone, such as adjusting the character set of strings or transforming timestamps into a common format.
In addition, data may be masked or anonymized to comply with legal regulations.
Aside from these changes, the semantic of the data remains the same as in the source systems.</p>
<h4 id="raw-zone"><a class="header" href="#raw-zone">Raw Zone</a></h4>
<blockquote>
<p>All data in the data lake is available in mostly raw format in the Raw Zone. Only basic transformations (see Landing Zone) are applied on the data. If the Landing Zone is omitted, these transformations are performed in the Raw Zone.</p>
</blockquote>
<p>Differently from the Landing Zone, the Raw Zone stores data persistently.
In general, data should neither be manipulated nor deleted from the Raw Zone.
This zone persists (when possible) the original data type (json, csv, xml).</p>
<h4 id="harmonized-zone"><a class="header" href="#harmonized-zone">Harmonized Zone</a></h4>
<blockquote>
<p>The Harmonized Zone is the place where master data are accessible for analyses.</p>
</blockquote>
<p>A subset of the data stored in the Raw Zone is passed to the Harmonized Zone in a demand-based manner.
It is important to note that these data are not deleted from the Raw Zone.
Instead, the Harmonized Zone contains a copy of or a view on the data in the Raw Zone.
The Harmonized Zone is also the place where master data are accessible for analyses.
As these data are crucial for enterprises, master data management is of high importance in the data lake.
Thus, they should exclusively be accessed after being cleansed.</p>
<p>The data characteristics in this zone differ greatly from those in the Raw Zone.
Data schema and syntax change when compared to the source data.
Data from different source systems are integrated into a consolidated schema, regardless of their structure.
The data syntax is also consolidated in the Harmonized Zone: when data from multiple source systems are merged, data types have to be adapted.</p>
<p>The aim of the Harmonized Zone is to provide a harmonized and consolidated view on data.
To this end, the Harmonized Zone uses a standardized modeling approach (dimensional modeling or Data Vault) that all of the enterprise’s data are modeled in.
The files in this zone facilitates the ingestion (parquet).</p>
<h4 id="distilled-zone"><a class="header" href="#distilled-zone">Distilled Zone</a></h4>
<blockquote>
<p>Prepares data for processing and facilitates ingestion.</p>
</blockquote>
<p>In contrast to the Raw and Harmonized Zone, where the focus is to quickly make data available for use, the Distilled Zone focuses on increasing the efficiency of following analyses by preparing the data accordingly.
The granularity of the data may be changed (for example, data may be aggregated for the calculation of KPIs).
Complex processing is applied that change the data’s semantics but are too extensive for the Landing Zone, Raw Zone, and Harmonized Zone.
However, the schema might also change slightly, depending on the supported use case (for example, fields to enrich the data could be added).
The files in this zone facilitates the ingestion (parquet).</p>
<h4 id="explorative-zone"><a class="header" href="#explorative-zone">Explorative Zone</a></h4>
<blockquote>
<p>The Explorative Zone is the place where data scientists can play with and flexibly use the data.</p>
</blockquote>
<p>Data scientists can use and explore data in the data lake in any way they desire, except for sensitive data. These data are only usable according to strict rules. Granularity, schema, syntax, and semantic may be changed in any way necessary for analyses.</p>
<h4 id="delivery-zone"><a class="header" href="#delivery-zone">Delivery Zone</a></h4>
<blockquote>
<p>In the Delivery Zone, small subsets of data are tailored to specific usage and applications.</p>
</blockquote>
<p>This does not only include analytical use cases, such as reporting and OLAP, but also operational use cases.
This zone thus provides functionality similar to data marts and operational data stores in data warehousing.
Data from this zone may be forwarded to external data sinks.</p>
<p>The Delivery Zone especially supports users with little knowledge on data analytics.
Data have to be easily findable and importable into various analytics tools.
As for the modeling approach, data are available in whatever format supports the intended use case best, for example, dimensional modeling for OLAP, or flat tables for operational use.</p>
<h4 id="zones-comparisom"><a class="header" href="#zones-comparisom">Zones Comparisom</a></h4>
<table>
    <tr>
        <td></td>
        <td><strong>Landing</strong></td>
        <td><strong>Raw</strong></td>
        <td><strong>Harmonized</strong></td>
        <td><strong>Distilled</strong></td>
        <td><strong>Explorative</strong></td>
        <td><strong>Delivery</strong></td>
    </tr>
    <tr>
        <td><strong>Granularity</strong></td>
        <td>Raw</td>
        <td>Raw</td>
        <td>Raw</td>
        <td>Aggregated</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
    <tr>
        <td><strong>Schema</strong></td>
        <td>Any</td>
        <td>Any</td>
        <td>Consolidated</td>
        <td>Consolidated, Enriched</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
    <tr>
        <td><strong>Syntax</strong></td>
        <td>Basic transformations</td>
        <td>Basic transformations</td>
        <td>Consolidated</td>
        <td>Consolidated</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
    <tr>
        <td><strong>Semantics</strong></td>
        <td>Mostly unchanged, unless needed for compliance</td>
        <td>Mostly unchanged, unless needed for compliance</td>
        <td>Mostly unchanged, unless needed for compliance</td>
        <td>Complex processing</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
    <tr>
        <td><strong>Governed</strong></td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>False</td>
        <td>True</td>
    </tr>
    <tr>
        <td><strong>Historized</strong></td>
        <td>False</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>N/A</td>
        <td>N/A</td>
    </tr>
    <tr>
        <td><strong>Persistent</strong></td>
        <td>False</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>False</td>
        <td>True</td>
    </tr>
    <tr>
        <td><strong>Has Protected Part</strong></td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
    </tr>
    <tr>
        <td><strong>Use Case Dependent</strong></td>
        <td>False</td>
        <td>False</td>
        <td>False</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
    </tr>
    <tr>
        <td><strong>User Groups</strong></td>
        <td>Systems, Processes</td>
        <td>Data Scientists, Systems, Processes</td>
        <td>Data Scientists, Systems, Processes</td>
        <td>Data Scientists, Domain Experts, Systems, Processes</td>
        <td>Data Scientists</td>
        <td>Anyone</td>
    </tr>
    <tr>
        <td><strong>Modelling Approach</strong></td>
        <td>Any</td>
        <td>Any</td>
        <td>Standardized</td>
        <td>Standardized</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
</table>
<p style="text-align: center;">Zone Reference Model for Enterprise-Grade Data Lake Management - Zones comparison.</p>
<h3 id="sandbox"><a class="header" href="#sandbox">Sandbox</a></h3>
<blockquote>
<p>Also known as the Analytics Sandbox, it provides data scientists and advanced analysts with a place for data exploration.</p>
</blockquote>
<p>An Analytics Sandbox is a separate environment that is part of the overall data lake architecture, meaning that it is a centralized environment meant to be used by multiple users and is maintained with the support of IT. Some key characteristics of this layer:</p>
<ul>
<li>The environment is controlled by the analyst</li>
<li>Allows them to install and use the data tools of their choice</li>
<li>Allows them to manage the scheduling and processing of the data assets</li>
<li>Enables analysts to explore and experiment with internal and external data</li>
<li>Can hold and process large amounts of data efficiently from many different data sources; big data (unstructured), transactional data (structured), web data, social media data, documents, etc.</li>
</ul>
<p>There are many advantages to having an Analytics Sandbox as part of your data architecture.
The most important is that it decreases the amount of time that it takes a business to gain knowledge and insight from their data.
It does this by providing an on-demand/always ready environment that allows analysts to quickly dive into and process large amounts of data and prototype their solutions without kicking off a big BI project.
In other words, it enables agile BI by empowering your advanced users.</p>
<p>Another major benefit to the business and IT team is that by giving the business a place to prototype their data solutions it allows the business to figure what they want on their own without involving IT.
When they decide that a solution is adding business value, it becomes a good candidate for something that should be productionized and built into the Data Warehouse process at some point.
This saves both teams a lot of time and effort.</p>
<h3 id="maturity-stages"><a class="header" href="#maturity-stages">Maturity Stages</a></h3>
<p>The implementation of a Data Lake solution consists of some main maturity stages.</p>
<ol>
<li>Handle and ingest data at scale</li>
<li>Building the analytical muscle</li>
<li>Data Warehouse and Data Lake working in unison</li>
<li>Enterprise capability</li>
</ol>
<h4 id="handle-and-ingest-data-at-scale"><a class="header" href="#handle-and-ingest-data-at-scale">Handle and Ingest data at scale</a></h4>
<blockquote>
<p>This stage consists in improving the ability to transform and analyze data.</p>
</blockquote>
<h4 id="building-the-analytical-muscle"><a class="header" href="#building-the-analytical-muscle">Building the analytical muscle</a></h4>
<blockquote>
<p>This stage involves improving the ability to transform and analyze data.</p>
</blockquote>
<p>In this stage, the company start acquiring more data and building applications.
In this stage, capabilities of the Data Warehouse and the Data Lake are used together.</p>
<h4 id="data-warehouse-and-data-lake-work-in-unison"><a class="header" href="#data-warehouse-and-data-lake-work-in-unison">Data Warehouse and Data Lake work in unison</a></h4>
<blockquote>
<p>This step involves getting data and analytics into the hands of as many people as possible.</p>
</blockquote>
<p>In this stage, the Data Lake and the Data Warehouse start to work in a union.
Both playing their part in analytics.</p>
<h4 id="enterprise-capability-in-the-lake"><a class="header" href="#enterprise-capability-in-the-lake">Enterprise capability in the lake</a></h4>
<blockquote>
<p>In this maturity stage of the data lake, enterprise capabilities are added to the Data Lake.</p>
</blockquote>
<p>It includes the adoption of information governance, information lifecycle management capabilities, and Metadata management.</p>
<h2 id="key-components-of-data-lake-architecture"><a class="header" href="#key-components-of-data-lake-architecture">Key Components of Data Lake Architecture</a></h2>
<p><img src="concepts/../mdbook-plantuml-img/2b8772bdc69bce03cd960127332a78ef0a2193ea.svg" alt="" /></p>
<p style="text-align: center;">Key Components of Data Lake Architecture.</p>
<h4 id="data-ingestion"><a class="header" href="#data-ingestion">Data Ingestion</a></h4>
<blockquote>
<p>Data Ingestion allows connectors to get data from a different data sources and load into the Data Lake.</p>
</blockquote>
<p>Data Ingestion supports:</p>
<ul>
<li>All types of Structured, Semi-Structured, and Unstructured data.</li>
<li>Multiple ingestions like Batch, Real-Time, One-time load.</li>
<li>Many types of data sources like Databases, Webservers, Emails, and FTP.</li>
</ul>
<h4 id="data-storage"><a class="header" href="#data-storage">Data Storage</a></h4>
<blockquote>
<p>Data storage should be scalable, offers cost-effective storage and allow fast access to data exploration. It should support various data formats.</p>
</blockquote>
<h4 id="data-governance"><a class="header" href="#data-governance">Data Governance</a></h4>
<blockquote>
<p>Data governance is a process of managing availability, usability, security, and integrity of data used in an organization.</p>
</blockquote>
<h4 id="security"><a class="header" href="#security">Security</a></h4>
<blockquote>
<p>Security needs to be implemented in every layer of the Data Lake. It starts with Storage, Unearthing, and Consumption. The basic need is to stop access for unauthorized users. It should support different tools to access data with easy to navigate GUI and Dashboards.</p>
</blockquote>
<p>Authentication, Accounting, Authorization and Data Protection are some important features of Data Lake security.</p>
<h4 id="data-quality-1"><a class="header" href="#data-quality-1">Data Quality</a></h4>
<blockquote>
<p>Data quality is an essential component of Data Lake architecture. Data is used to exact business value. Extracting insights from poor quality data will lead to poor quality insights.</p>
</blockquote>
<h4 id="data-discovery"><a class="header" href="#data-discovery">Data Discovery</a></h4>
<blockquote>
<p>Data Discovery is another important stage before you can begin preparing data or analysis. In this stage, tagging technique is used to express the data understanding, by organizing and interpreting the data ingested in the Data Lake.</p>
</blockquote>
<h4 id="data-auditing"><a class="header" href="#data-auditing">Data Auditing</a></h4>
<blockquote>
<p>Data auditing helps to evaluate risk and compliance.</p>
</blockquote>
<p>The main Data auditing tasks are:</p>
<ul>
<li>Tracking changes to important dataset elements</li>
<li>Captures how/when/who changes to these elements.</li>
</ul>
<h4 id="data-lineage-1"><a class="header" href="#data-lineage-1">Data Lineage</a></h4>
<blockquote>
<p>This component deals with data’s origins. It mainly deals with where it movers over time and what happens to it. It eases errors corrections in a data analytics process from origin to destination.</p>
</blockquote>
<h4 id="data-exploration"><a class="header" href="#data-exploration">Data Exploration</a></h4>
<blockquote>
<p>It is the beginning stage of data analysis. It helps to identify right dataset is vital before starting Data Exploration.</p>
</blockquote>
<p>All given components need to work together to play an important part in Data Lake building easily evolve and explore the environment.</p>
<h2 id="other-architecture-approaches"><a class="header" href="#other-architecture-approaches">Other Architecture Approaches</a></h2>
<p><strong>Data Lake Lambda Architecture for Smart Grids Big Data Analytics</strong>: relies on Lambda architecture that is capable of performing parallel batch and real-time operations on distributed data. See the <a href="https://ieeexplore.ieee.org/abstract/document/8417407">article</a>{{A. A. Munshi and Y. A. -R. I. Mohamed, "Data Lake Lambda Architecture for Smart Grids Big Data Analytics," in IEEE Access, vol. 6, pp. 40463-40471, 2018, doi: 10.1109/ACCESS.2018.2858256.}}. Also see a brief explantion of the Lambda Architecture in this <a href="https://www.researchgate.net/profile/Ajit-Singh-46/publication/331890045_Architecture_of_Data_Lake/links/6061ef85458515e8347d6ecc/Architecture-of-Data-Lake.pdf">article</a>{{Ajit Singh, "Architecture of Data Lake", International Journal of Scientific Research in Computer Science, Engineering and Information Technology (IJSRCSEIT), ISSN : 2456-3307, Volume 5 Issue 2, pp. 411-414, March-April 2019. Available at doi: https://doi.org/10.32628/CSEIT1952121}}.<p><hr/></p>
<p><a name="footnote-1"><a href="concepts/data_lake.html#to-footnote-1">1</a></a>: The article discuss data lake optimizations using AWS products, like CloudWatch, Macie, CloudTrail, and S3 Intelligent-Tiering.</p>
<p><a name="footnote-2"><a href="concepts/data_lake.html#to-footnote-2">2</a></a>: C. Giebler, C. Gröger, E. Hoos, H. Schwarz and B. Mitschang, "A Zone Reference Model for Enterprise-Grade Data Lake Management," 2020 IEEE 24th International Enterprise Distributed Object Computing Conference (EDOC), Eindhoven, Netherlands, 2020, pp. 57-66, doi: 10.1109/EDOC49727.2020.00017.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-warehouse"><a class="header" href="#data-warehouse">Data Warehouse</a></h1>
<blockquote>
<p>A <strong>Data Warehouse</strong> (DWH), also known as Enterprise Data Warehouse (EDW) is a central repository of information that can be analyzed to make more informed decisions.</p>
</blockquote>
<p>Data flows into a data warehouse from Data Lake, transactional systems, relational databases, and other sources, typically on a regular cadence. Business analysts, data engineers, data scientists, and decision makers access the data through Business Intelligence (BI) tools, SQL clients, and other analytics applications.</p>
<ul>
<li><a href="concepts/data_warehouse.html#goals">Goals</a></li>
<li><a href="concepts/data_warehouse.html#data-warehouse-architecture">Data Warehouse Architecture</a>
<ul>
<li><a href="concepts/data_warehouse.html#data-sources">Data Sources</a></li>
<li><a href="concepts/data_warehouse.html#warehouse">Warehouse</a>
<ul>
<li><a href="concepts/data_warehouse.html#metadata">Metadata</a></li>
<li><a href="concepts/data_warehouse.html#summarized-data">Summarized data</a></li>
</ul>
</li>
<li><a href="concepts/data_warehouse.html#end-user-access-tools">End-User access Tools</a></li>
<li><a href="concepts/data_warehouse.html#two-tier-vs-three-tier-architecture">Two-Tier vs Three-Tier Architecture</a></li>
</ul>
</li>
<li><a href="concepts/data_warehouse.html#data-modeling-methodologies">Data Modeling Methodologies</a></li>
<li><a href="concepts/data_warehouse.html#maturity-stages">Maturity Stages</a></li>
<li><a href="concepts/data_warehouse.html#key-components-of-a-data-warehouse">Key Components of a Data Warehouse</a></li>
</ul>
<h2 id="goals-1"><a class="header" href="#goals-1">Goals</a></h2>
<blockquote>
<p>When implementing a data warehouse, the main goals are to achieve: consistency, enable data-driven decision-making and improvement, and to maintain data Single Source of Truth.</p>
</blockquote>
<p><strong>Consistency</strong>: to maintain a uniform format to all collected data, making it easier for corporate decision-makers to analyze and share data insights with their colleagues. Standardizing data from different sources also reduces the risk of error in interpretation and improves overall accuracy.</p>
<p><strong>Decision-making</strong>: successful business leaders develop data-driven strategies and rarely make decisions without consulting the facts. Data warehousing improves the speed and efficiency of accessing different data sets and makes it easier for corporate decision-makers to derive insights that will guide the business and marketing strategies that set them apart from their competitors.</p>
<p><strong>Improving</strong>: allow business leaders to quickly access the organization historical activities and evaluate initiatives that have been successful — or unsuccessful — in the past. This allows executives to see where they can adjust their strategy to decrease costs, maximize efficiency and increase business results.</p>
<p><strong>Single Source of Truth</strong>: the whole organization would benefit on having a single source of truth, specially when there are multiple data sources to a common business dimension.</p>
<h2 id="data-warehouse-architecture"><a class="header" href="#data-warehouse-architecture">Data Warehouse Architecture</a></h2>
<p><img src="concepts/../mdbook-plantuml-img/85627189967d2e5f9857b7b50dfac44b17456eb7.svg" alt="" /></p>
<p style="text-align: center;">Data Warehouse Solution.</p>
<p>There are several data warehouses architecture approaches available. Data warehouses would have in common some key components:</p>
<h3 id="data-sources"><a class="header" href="#data-sources">Data Sources</a></h3>
<blockquote>
<p>In most architectures approaches, it’s the Source Layer, or Data Source Layer, and consists on all the data sources the Warehouse Layer will consume.</p>
</blockquote>
<p><strong>Operational System</strong>: is a method used in data warehousing to refer to a system that is used to process the day-to-day transactions of an organization. Physically, it will normally refer to the databases the organization applications and micro-services create.</p>
<p><strong>Flat Files System</strong>: is a system of files in which transactional data is stored, and every file in the system must have a different name.</p>
<h3 id="warehouse"><a class="header" href="#warehouse">Warehouse</a></h3>
<blockquote>
<p>In most architectures approaches, it’s the Warehouse Layer, or Data Warehouse Layer, and consists on all the data stored in RDBMS database with available gateway access (ODBC, JDBC, etc.). It also contains the metadata, and some degree of data summarization, and business logic applied, which differentiate an <strong>DWH database</strong> from a <strong>Production database</strong>.</p>
</blockquote>
<h4 id="metadata"><a class="header" href="#metadata">Metadata</a></h4>
<blockquote>
<p>Metadata is the road-map to a data warehouse, it defines the warehouse objects, and acts as a directory. This directory helps the decision support system to locate the contents of a data warehouse.</p>
</blockquote>
<p>It normally contains:</p>
<ol>
<li>A description of the Data Warehouse structure, including the warehouse schema, dimensions, hierarchies, data mart locations, contents, etc.</li>
<li>Operational metadata, which usually describes the currency level of the stored data (for example, active, archived or purged), and warehouse monitoring information (for example, usage statistics, error reports, audit, etc).</li>
<li>System performance data, which includes indices, used to improve data access and retrieval performance.</li>
<li>Information about the mapping from operational databases, which provides source RDBMSs and their contents, cleaning and transformation rules, etc.</li>
<li>Summarization algorithms, predefined queries, and reports business data, which include business terms and definitions, ownership information, etc.</li>
</ol>
<p>Metadata management tool examples are Datahub, Open Metadata, and Amundsen.</p>
<h4 id="summarized-data"><a class="header" href="#summarized-data">Summarized data</a></h4>
<blockquote>
<p>The area of the data warehouse that maintains all the predefined lightly and highly summarized (aggregated) data. The main goal is to speed up query performance, and the summarized records are updated continuously as new information is loaded into the warehouse.</p>
</blockquote>
<h3 id="end-user-access-tools"><a class="header" href="#end-user-access-tools">End-User access Tools</a></h3>
<blockquote>
<p>The main purpose of a data warehouse is to provide information to the business for strategic decision-making. These end-users interact with the warehouse using end-client access tools.</p>
</blockquote>
<p>The examples of some of the end-user access tools can be:</p>
<ul>
<li>Reporting and Query Tools</li>
<li>Application Development Tools</li>
<li>Executive Information Systems Tools</li>
<li>Online Analytical Processing Tools</li>
<li>Data Mining Tools</li>
</ul>
<h3 id="two-tier-vs-three-tier-architecture"><a class="header" href="#two-tier-vs-three-tier-architecture">Two-Tier vs Three-Tier Architecture</a></h3>
<p>The data warehouse will normally be designed in a Two-Tier or in a Three-Tier architecture approach. The details of which one will be explored in the chapter <a href="concepts/./data_warehouse_tier_architecture.html">Data Warehouse Tier Architecture</a>.</p>
<p>In short, the tiers are:</p>
<ol>
<li>Bottom/Data Tier: data warehouse server with functional gateway (ODBC, JDBC, etc.).</li>
<li>Middle/Application Tier: houses the business logic used to process user inputs. Example: OLAP Servers, Snowflake, Apache Redshift, Databricks Data Lakehouse Platform, Apache Spark.</li>
<li>Top/Presentation Tier: front-end tools.</li>
</ol>
<h2 id="data-modeling-methodologies"><a class="header" href="#data-modeling-methodologies">Data Modeling Methodologies</a></h2>
<blockquote>
<p>Being one of the most important topics of data warehouse design and architecture, the data modeling methodology choosing process is arduous and polemic, and will impact the whole design and implementation of the data warehouse solution.</p>
</blockquote>
<p>Specially for startups, the first versions or iterations of a data solution (implemented before the organization even start discussing the implementation of a data warehouse solution) will be very similar to a <strong>Kimball</strong> (or Bottom-up) methodology approach, though not planned explicitly as such. Which means the data marts (or the data to be accessed by the first BI tools adopted in the organization) are first formed based on the business requirements.</p>
<p>There are lots of advantages and disadvantages of priming this approach over a top-down approach (or any of the hybrid or alternative methodologies).</p>
<p>See all the details of the different data modelling methodologies in the chapter <a href="concepts/./data_modelling.html">Data Modelling</a>. See also the implementation of a three-tier data warehouse architecture in the <a href="https://iopscience.iop.org/article/10.1088/1757-899X/306/1/012061/pdf">paper</a>{{Tangkawarow, I. R. H. T., Runtuwene, J. P. A., Sangkop, F. I., &amp; Ngantung, L. V. F. (2018, February). Three Tier-Level Architecture Data Warehouse Design of Civil Servant Data in Minahasa Regency. In IOP Conference Series: Materials Science and Engineering (Vol. 306, No. 1, p. 012061). IOP Publishing.}} "Three Tier-Level Architecture Data Warehouse Design of Civil Servant Data in Minahasa Regency".</p>
<h2 id="maturity-stages-1"><a class="header" href="#maturity-stages-1">Maturity Stages</a></h2>
<p>#TODO</p>
<h2 id="key-components-of-a-data-warehouse"><a class="header" href="#key-components-of-a-data-warehouse">Key Components of a Data Warehouse</a></h2>
<p><strong>Data Ingestion</strong>: allows connectors to get data from a different data sources and load into the Data Warehouse. The data will normally come from the Data Lake and External Sources connection (Fivetran), through multiple ETLs (Airflow, services, apps, ETL tools and platforms, etc.).</p>
<p><strong>Data Storage</strong>: the data is stored in the data warehouse database, a relational database (RDBMS), like Postgres.</p>
<p><strong>Data Governance</strong>: is a process of managing availability, usability, security, and integrity of data used in an organization.</p>
<p><strong>Security</strong>: it needs to be implemented in every layer of the Data Warehouse. It includes setting up the data warehouse read-only by default, and setting up custom User Groups. It also includes the access to the databases (VPCs, VPNs, Whitelisting, etc.), strong and active DevOps monitoring and the enforcing of best practices in all levels of the data warehouse environment (data ingestion, data marts consumption, ETLs design, etc.).</p>
<p><strong>Data Quality</strong>: it is an essential component of Data Warehouse architecture. Data is used to exact business value. Extracting insights from poor quality data will lead to poor quality insights.</p>
<p><strong>Data Discovery</strong>: it is another important stage before you can begin preparing data or analysis. All this rely on good metadata, and data modeling.</p>
<p><strong>Data Auditing</strong>: it helps to evaluate risk and compliance. Two major Data auditing tasks are tracking changes to the key dataset.</p>
<ul>
<li>Tracking changes to important dataset elements.</li>
<li>Captures how/when/who changes to these elements.</li>
</ul>
<p><strong>Data Lineage</strong>: it deals with data’s origins. It mainly deals with where it movers over time and what happens to it. It eases errors corrections in a data analytics process from origin to destination. Some data modeling techniques may facilitate lineage in comparison to others (Vault vs Kimball vs Inmon).</p>
<p><strong>Data Exploration</strong>: it is the beginning stage of data analysis. It helps to identify right dataset is vital before starting Data Exploration. All given components need to work together to play an important part in Data Warehouse building easily evolve and explore the environment.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tier-architecture"><a class="header" href="#tier-architecture">Tier Architecture</a></h1>
<h2 id="two-tier-architecture"><a class="header" href="#two-tier-architecture">Two-Tier Architecture</a></h2>
<p><img src="concepts/../mdbook-plantuml-img/34e8845308d5034a348498bba9809daa207f0ca8.svg" alt="" /></p>
<p style="text-align: center;">Two-Tier Data Warehouse Architecture.</p>
<p><strong>Data Source Layer</strong>: A data warehouse system uses a heterogeneous source of data. That data is stored initially to the organization's relational databases or legacy databases, or it may come from an information system outside the organization walls.</p>
<p><strong>Data Staging Layer</strong>: The data stored to the source should be extracted, cleansed to remove inconsistencies and fill gaps, and integrated to merge heterogeneous sources into one standard schema. The ETLs can combine heterogeneous schemata, extract, transform, cleanse, validate, filter, and load source data into a data warehouse. Note that this can be achieved in two ways:</p>
<ol>
<li>Having the Distillation Layer (Silver) in the Data Lake as the Data Staging Layer.</li>
<li>Creating a separated database within the Data Warehouse, or a separated database schema. Following the principle that all the data in the data warehouse should be cleaned and have high quality standards, a separated database should be preferred.</li>
</ol>
<p><strong>Data Warehouse Layer</strong>: Information is saved to one logically centralized individual repository: a data warehouse. The data warehouses can be directly accessed, but it can also be used as a source for creating data marts, which partially replicate data warehouse contents and are designed for specific enterprise departments. Metadata repositories store information on sources, access procedures, data staging, users, data mart schema, and so on.</p>
<p><strong>Analysis Layer</strong>: In this layer, integrated data is efficiently, and flexibly accessed to issue reports, dynamically analyze information, and simulate hypothetical business scenarios. It should feature aggregated information navigators, complex query optimizers, and customer-friendly GUIs.</p>
<h2 id="three-tier-architecture"><a class="header" href="#three-tier-architecture">Three-Tier Architecture</a></h2>
<p><img src="concepts/../mdbook-plantuml-img/c23cf858fede086a583ebceb59b95433ed0e7a9a.svg" alt="" /></p>
<p style="text-align: center;">Three-Tier Data Warehouse Architecture.</p>
<p>As the name of the architecture suggests, it consists of three tiers (levels):</p>
<ol>
<li>Bottom/Data Tier: data warehouse server with functional gateway (ODBC, JDBC, etc.).</li>
<li>Middle/Application Tier: houses the business logic used to process user inputs. Example: OLAP Servers, Snowflake, Apache Redshift, Databricks Data Lakehouse Platform, Apache Spark.</li>
<li>Top/Presentation Tier: front-end tools.</li>
</ol>
<p>The Bottom and Top tiers were already discussed in details in the previous section, so we only have left the implementation of the Middle Tier, very important to enable fast querying of the data warehouse. It is important to note the solutions to the middle tier are often referred as the Data Warehouse itself, but they’re only the Application tier/level in a complete data warehouse solution (It’s like saying Snowflake is the DWH, when it’s just one part of the complete DWH solution).</p>
<h3 id="reconciled-layer"><a class="header" href="#reconciled-layer">Reconciled Layer</a></h3>
<p>The Reconciled Layer sits between the source data and data warehouse. The main advantage of the reconciled layer is that it creates a standard reference data model for the whole company. At the same time, it separates the problems of source data extraction and integration from those of data warehouse population. In some cases, the reconciled layer is also directly used to accomplish better operational tasks, such as producing daily reports that cannot be satisfactorily prepared using the corporate applications or generating data flows to feed external processes periodically to benefit from cleaning and integration.</p>
<p>This architecture is especially useful for the extensive, enterprise-wide systems. A disadvantage of this structure is the extra file storage space used through the extra redundant reconciled layer. It also makes the analytical tools a little further away from being real-time.</p>
<p>Please note that a reconciled layer could be part of the Data Warehouse, or the Data Lake (see Harmonized Zone, in Data Lake concepts chapter).</p>
<h3 id="middleapplication-tier"><a class="header" href="#middleapplication-tier">Middle/Application Tier</a></h3>
<blockquote>
<p>Houses the business logic used to process user inputs. Example: OLAP Servers, Snowflake, Apache Redshift, Databricks Data Lakehouse Platform.</p>
</blockquote>
<p>See details in the chapter <a href="concepts/./data_warehouse_application_tier.html">Data Warehouse Middle/Application Tier</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-warehouse-middleapplication-tier"><a class="header" href="#data-warehouse-middleapplication-tier">Data Warehouse Middle/Application Tier</a></h1>
<blockquote>
<p>Houses the business logic used to process user inputs.</p>
</blockquote>
<p>One could argue that OLAP is <a href="https://www.kdnuggets.com/2022/10/olap-dead.html">dead</a>, at least in the traditional format (solutions like Mondrian), and cloud/modern solutions should be applied to accompany business fast demand for data.</p>
<p>Given this book aims to explore business scenarios mostly common to startups, in this sense, given costs restrictions, solutions like Apache Kylin seem to be a better approach (details below). When costs are less restricted, and/or data demands increase, other solutions like AWS Redshift, Snowflake, and Databricks Lakehouse are preferred/recommended.</p>
<h2 id="snowflake"><a class="header" href="#snowflake">Snowflake</a></h2>
<h2 id="aws-redshift"><a class="header" href="#aws-redshift">AWS Redshift</a></h2>
<p>#TODO</p>
<h2 id="snowflake-vs-aws-redshift"><a class="header" href="#snowflake-vs-aws-redshift">Snowflake vs AWS Redshift</a></h2>
<h2 id="databricks-lakehouse-platform"><a class="header" href="#databricks-lakehouse-platform">Databricks Lakehouse Platform</a></h2>
<blockquote>
<p>It combines the ACID transactions and data governance of data warehouses with the flexibility and cost-efficiency of data lakes to enable business intelligence (BI) and machine learning (ML) on all data.</p>
</blockquote>
<h3 id="data-lakehouse-vs-data-warehouse-vs-data-lake"><a class="header" href="#data-lakehouse-vs-data-warehouse-vs-data-lake">Data Lakehouse vs Data Warehouse vs Data Lake</a></h3>
<p>Data in the data warehouse is easy to use, but harder to store. The opposite is true for the data lake: it’s easy to ingest and store data, but a pain to consume and query.</p>
<p>The data lakehouse has a layer design, with a warehouse layer on top of a data lake. This architecture, which enables combining structured and unstructured data, makes it efficient for business intelligence and business analysis. Data lakehouses provide structured storage for some types of data and unstructured storage for others while keeping all data in one place.</p>
<h2 id="olap-servers"><a class="header" href="#olap-servers">OLAP Servers</a></h2>
<h3 id="relational-olap-servers-rolap"><a class="header" href="#relational-olap-servers-rolap">Relational OLAP Servers (ROLAP)</a></h3>
<blockquote>
<p>They use a relational or extended-relational DBMS to save and handle warehouse data, and OLAP middleware to provide missing pieces. They work primarily from the data that resides in a relational database, where the base data and dimension tables are stored as relational tables. This model permits the multidimensional analysis of data.</p>
</blockquote>
<p>This technique relies on manipulating the data stored in the relational database to give the presence of traditional OLAP's slicing and dicing functionality.</p>
<h4 id="advantages"><a class="header" href="#advantages">Advantages</a></h4>
<p><strong>Can handle large amounts of information</strong>: the data size limitation of ROLAP technology is depends on the data size of the underlying RDBMS. So, ROLAP itself does not restrict the data amount.</p>
<p>RDBMS already comes with a lot of features. So ROLAP technologies, (works on top of the RDBMS) can control these functionalities.</p>
<h4 id="disadvantages"><a class="header" href="#disadvantages">Disadvantages</a></h4>
<p><strong>Performance can be slow</strong>: each ROLAP report is a SQL query (or multiple SQL queries) in the relational database, the query time can be prolonged if the underlying data size is large.</p>
<p><strong>Limited by SQL functionalities</strong>: ROLAP technology relies on upon developing SQL statements to query the relational database, and SQL statements do not suit all needs.</p>
<h3 id="multidimensional-olap-servers-molap"><a class="header" href="#multidimensional-olap-servers-molap">Multidimensional OLAP Servers (MOLAP)</a></h3>
<blockquote>
<p>It is based on a native logical model that directly supports multidimensional data and operations. Data are stored physically into multidimensional arrays, and positional techniques are used to access them.</p>
</blockquote>
<p>One of the significant distinctions of MOLAP against a ROLAP is that data are summarized and are stored in an optimized format in a multidimensional cube, instead of in a relational database. In MOLAP model, data are structured into proprietary formats by client's reporting requirements with the calculations pre-generated on the cubes.</p>
<h4 id="advantages-1"><a class="header" href="#advantages-1">Advantages</a></h4>
<p><strong>Excellent Performance</strong>: a MOLAP cube is built for fast information retrieval, and is optimal for slicing and dicing operations.</p>
<p><strong>Can perform complex calculations</strong>: all evaluation have been pre-generated when the cube is created. Hence, complex calculations are not only possible, but they return quickly.</p>
<h4 id="disadvantages-1"><a class="header" href="#disadvantages-1">Disadvantages</a></h4>
<p><strong>Limited in the amount of information it can handle</strong>: Because all calculations are performed when the cube is built, it is not possible to contain a large amount of data in the cube itself.</p>
<h3 id="hybrid-olap-servers-holap"><a class="header" href="#hybrid-olap-servers-holap">Hybrid OLAP Servers (HOLAP)</a></h3>
<blockquote>
<p>It incorporates the best features of MOLAP and ROLAP into a single architecture. It saves more substantial quantities of detailed data in the relational tables while the aggregations are stored in the pre-calculated cubes. HOLAP also can drill through from the cube down to the relational tables for delineated data.</p>
</blockquote>
<h4 id="advantages-2"><a class="header" href="#advantages-2">Advantages</a></h4>
<ul>
<li>It provides benefits of both MOLAP and ROLAP.</li>
<li>It provides fast access at all levels of aggregation.</li>
<li>It balances the disk space requirement, as it only stores the aggregate information on the OLAP server and the detail record remains in the relational database. So no duplicate copy of the detail record is maintained.</li>
</ul>
<h4 id="disadvantages-2"><a class="header" href="#disadvantages-2">Disadvantages</a></h4>
<p>HOLAP architecture is very complicated because it supports both MOLAP and ROLAP servers.</p>
<h3 id="olap-servers-options"><a class="header" href="#olap-servers-options">OLAP Servers options</a></h3>
<h4 id="apache-kylin"><a class="header" href="#apache-kylin">Apache Kylin</a></h4>
<p>Only supports MOLAP and Offline data storage modes. It supports both SQL and MDX queries, have RESTful API capabilities (also ODBC, and JDBC), and can be integrated/connect with Tableau (also Redash, Superset, Zeppelin, Qlik, and Excel).</p>
<p>It supports Real Time processing, partitioning, usage based optimizations, load balancing and clustering. It supports LDAP, SAML, Kerboros authentication.</p>
<h4 id="mondrian-olap-server"><a class="header" href="#mondrian-olap-server">Mondrian OLAP Server</a></h4>
<p>Only supports ROLAP data storage modes. It supports MDX queries but not SQL, and have REST API capabilities. Does not natively connect with Tableau, but queries can be performed via Java APIs. It supports Real Time processing, and partitioning.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-modelling"><a class="header" href="#data-modelling">Data Modelling</a></h1>
<h2 id="kimballbottom-up"><a class="header" href="#kimballbottom-up">Kimball/Bottom-Up</a></h2>
<blockquote>
<p>The design of the Data Marts comes from the business requirements.</p>
</blockquote>
<p>The primary data sources are then evaluated, ETL tools are used to fetch data from several sources and load it into a staging area of the relational database server.
Once data is uploaded in the  data warehouse staging area, the next phase includes loading data into a dimensional data warehouse model that is denormalized by nature.
This model partitions data into the fact and dimension tables.
Kimball dimensional modelling allows users to construct several star schemas to fulfill various reporting needs.</p>
<div style="position: relative; width: 100%; height: 0; padding-top: 50.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbfV1mwLM&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbfV1mwLM&#x2F;view?utm_content=DAFbfV1mwLM&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Kimball Approach to Data Warehouse Lifecycle.</p></a>
<h3 id="advantages-3"><a class="header" href="#advantages-3">Advantages</a></h3>
<p><strong>Fast to construct (quick initial phase)</strong>: it is fast to construct as no normalization is involved, which means swift execution of the initial phase of the data warehousing design process.</p>
<p><strong>Simplified queries</strong>: in a star schema, most data operators can easily comprehend it because of its denormalized structure, which simplifies querying and analysis.</p>
<p><strong>Simplified business management</strong>: the data warehouse system footprint is trivial because it focuses on individual business areas and processes rather than the whole company. So, it takes less space in the database, simplifying system management.</p>
<p><strong>Fast data retrieval</strong>: as data is segregated into fact tables and dimensions.</p>
<p><strong>Smaller teams</strong>: a smaller team of designers and planners is sufficient for data warehouse management because data source systems are stable, and the data warehouse is process-oriented. Also, query optimization is straightforward, predictable, and controllable.</p>
<p><strong>Deeper insights</strong>: it allows business intelligence tools to deeper across several star schemas and generates reliable insights.</p>
<h3 id="disadvantages-3"><a class="header" href="#disadvantages-3">Disadvantages</a></h3>
<p><strong>No Single Source of Truth</strong>: Data isn’t entirely integrated before reporting, so the idea of a single source of truth is lost.</p>
<p><strong>Too prone to data irregularities</strong>: this is because in denormalization technique, redundant data is added to database tables.</p>
<p><strong>Too difficult and expensive to add new columns</strong>: performance issues may occur due to the addition of columns in the fact table, as these tables are quite in-depth. The addition of new columns can expand the fact table dimensions, affecting its performance.</p>
<p><strong>Can't respond (well) to business changes</strong>: it is too difficult to alter the models.</p>
<p><strong>Not BI-friendly</strong>: as it is business process-oriented, instead of focusing on the company as a whole, it cannot handle all the BI reporting requirements.</p>
<p><strong>Inconsistent dimensional view</strong>: this model is not strong as top-down approach as dimensional view of data marts is not consistent as it is in Inmon approach.</p>
<p>In brief, the Kimball approach have a <strong>low start-up cost</strong>, is <strong>faster to deliver</strong> the first phase of the data warehouse design, is faster to release to production (first version), but is suitable for <strong>Tactical</strong> business decision support requirements (versus Strategic), and <strong>addresses individual business requirements</strong> (vs Enterprise-wide). Another important topic that derives from this methodology approach is the <strong>Data Warehouse Bus Architecture</strong>.</p>
<h2 id="inmontop-down"><a class="header" href="#inmontop-down">Inmon/Top-Down</a></h2>
<blockquote>
<p>Subject-oriented, nonvolatile, integrated, time-variant collection of data in support of management’s decisions.</p>
</blockquote>
<p>On the other hand, Bill Inmon, the father of data warehousing, came up with the concept to develop a data warehouse which identifies the main subject areas and entities the enterprise works with, such as customers, product, vendor, etc. Inmon’s definition of a data warehouse is that it is a “subject-oriented, nonvolatile, integrated, time-variant collection of data in support of management’s decisions”.</p>
<p>The model then creates a <strong>thorough, logical model for every primary entity</strong> (Business Entities). For instance, a logical model is constructed for products with all the attributes associated with that entity. This logical model could include many entities, including all the details, aspects, relationships, dependencies, and affiliations.</p>
<p>The Inmon design approach uses the <strong>normalized form</strong> for building <strong>entity structure</strong>, avoiding data redundancy as much as possible. This results in clearly identifying business requirements and <strong>preventing any data update irregularities</strong>. Moreover, the advantage of this top-down approach in database design is that it is <strong>robust to business changes</strong> and contains a dimensional perspective of data across data mart.</p>
<p>Next, the physical model is constructed, which follows the normalized structure. This Inmon model creates a <strong>Single Source of Truth</strong> for the whole business to consume. Data loading becomes less complex due to the normalized structure of the model. However, using this arrangement for querying is challenging as it includes numerous tables and links.</p>
<p>This Inmon data warehouse methodology proposes constructing data marts separately for each division, such as finance, marketing sales, etc. All the data entering the data warehouse is integrated. The data warehouse acts as a single data source for various data marts to ensure integrity and consistency across the enterprise.</p>
<h3 id="advantages-4"><a class="header" href="#advantages-4">Advantages</a></h3>
<p><strong>Single Source of Truth</strong>: the data warehouse acts as a unified source of truth for the entire business, where all data is integrated.</p>
<p><strong>Very low data redundancy</strong>: there’s less possibility of data update irregularities, making the data warehouse ETL processes more straightforward and less susceptible to failure.</p>
<p><strong>Great flexibility</strong>: it’s easier to update the data warehouse in case there’s any change in the business requirements or source data.</p>
<p><strong>BI-friendly</strong>: It can handle diverse company-wide reporting requirements.</p>
<h3 id="disadvantages-4"><a class="header" href="#disadvantages-4">Disadvantages</a></h3>
<p><strong>Increasing complexity</strong>: it increases as multiple tables are added to the data model with time.</p>
<p><strong>Skilled Human Resources</strong>: resources skilled in data warehouse data modelling are required, which can be expensive and challenging to find.</p>
<p><strong>Slow setup</strong>: the preliminary setup and delivery are time-consuming.</p>
<p><strong>Expert management</strong>: this approach requires experts to manage a data warehouse effectively.</p>
<p>In brief, the Inmon have a <strong>high start-up cost</strong>, requires <strong>more time to be in production</strong> and meet business needs (very large projects with a very broad scope), and <strong>requires a bigger team os specialists</strong>, but is more <strong>suitable for for systems and business changes</strong>, better <strong>integrates with the whole organization</strong>, favors <strong>Strategic business decision support requirements</strong> (vs Tactical), and <strong>facilitates Business Intelligence development</strong>.</p>
<h2 id="hybrid"><a class="header" href="#hybrid">Hybrid</a></h2>
<blockquote>
<p>In a hybrid model, the data warehouse is built using the Inmon model, and on top of the integrated data warehouse, the business process oriented data marts are built using the star schema for reporting.</p>
</blockquote>
<p>The hybrid approach provides a <strong>Single Source of Truth</strong> for the data marts, creating a highly flexible solutions from a BI point of view.</p>
<p>Based on the <a href="https://www.researchgate.net/publication/261302233_The_Customer-Centric_Data_Warehouse_An_Architectural_Approach_to_Meet_the_Challenges_of_Customer_Orientation">Hub and Spoke Architecture</a>, the hybrid design methodology can also make use of <a href="https://en.wikipedia.org/wiki/Operational_data_store">Operational Data Stores</a> (ODS), integrating and cleaning data from multiple data sources. The information is then parsed into the actual Data Warehouse.</p>
<p>Hybrid methods will normally keep the data in the 3rd normal form, reducing redundancy. Although normal relational database is not efficient for BI reports. Data marts for specific reports can then be built on top of the data warehouse solution.</p>
<p>When the data is denormalized, all the data available is pulled (as advocated by Inmon) while using a denormalized design (as advocated by Kimball). One example is the <a href="https://resilientbiz.com/the-resilient-hybrid-methodology-data-warehouse/">Carry Forward</a> method.</p>
<p>Another hybrid methodology is the Data Vault, discussed below.</p>
<div style="position: relative; width: 100%; height: 0; padding-top: 50.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbfihtfys&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbfihtfys&#x2F;view?utm_content=DAFbfihtfys&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Example of a Hybrid Methodology approach.</p></a>
<h2 id="vault"><a class="header" href="#vault">Vault</a></h2>
<p>The <strong>Vault Data Modelling</strong> is a hybrid design, consisting of the best of breed practices from both <strong>3rd normal form</strong> and <strong>star-schema</strong>.</p>
<p>It is not a true 3rd normal form, and breaks some of the rules that 3NF dictates. It is a top-down architecture with bottom-up design, geared to be strictly a data warehouse. It is not geared to be end-user accessible, which when built, still requires the user of a data mart or star-schema based release are for business purposes.</p>
<p>Data Vault data modelling breaks data into a small number of standard components – the most common of which are <strong>Hubs</strong>, <strong>Links</strong> and <strong>Satellites</strong>.</p>
<p><strong>Hubs</strong> are entities of interest to the business. They contain just a distinct list of business keys and metadata about when each key was first loaded and from where.</p>
<p><strong>Links</strong> connect Hubs and may record a transaction, composition, or other type of relationship between hubs. They contain details of the hubs involved (as foreign keys) and metadata about when the link was first loaded and from where.</p>
<p><strong>Satellites</strong> connect to Hubs or Links. They are Point in Time: so we can ask and answer the question, “what did we know when?”. Satellites contain data about their parent Hub or Link and metadata about when the data was loaded, from where, and a business effectivity date.</p>
<p>The data model of the data warehouse is constructed using these components. These are:</p>
<p><strong>Standard</strong>: each component is always constructed the same way.</p>
<p><strong>Simple</strong>: easy to understand, and with a little practice, easy to apply them to model your system.</p>
<p><strong>Connected</strong>: hubs only connect to links and satellites, links only connect to hubs and satellites, and satellites only connect to hubs or links.</p>
<p>Data Vault has staging, vault and mart layers. Star schemas live in the mart layer, each star schema exposes a subset of the vault for a particular group of users.  Typically, hubs and their satellites form dimensions, links and their satellites form facts.</p>
<p>A Data Vault complements the Data Lake and is a solution for organizations that need to integrate and add structure to the data held in the Data Lake.</p>
<div style="position: relative; width: 100%; height: 0; padding-top: 50.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbf7fKp9s&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbf7fKp9s&#x2F;view?utm_content=DAFbf7fKp9s&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Example of Data Vault 2.0 Modelling Methodology approach.</p></a>
<h2 id="bus-architecture"><a class="header" href="#bus-architecture">Bus Architecture</a></h2>
<blockquote>
<p>A bus architecture is composed of a set of tightly integrated data marts that get their power from conformed dimensions and fact tables. A conformed dimension is defined and implemented one time, so that it means the same thing everywhere it's used.</p>
</blockquote>
<p>A dimension table is the "lookup" table of a dimensional model.
It contains textual data that decodes an identifier in associated fact tables.
A conformed dimension is defined and implemented one time and used throughout the multiple star schemas that make up the enterprise data mart.
Dimensions define the who, what, where, when, why, and how of a situation, and are laid out for the benefit of business users.</p>
<blockquote>
<p>To conform a dimension, every stakeholder must agree on a common definition for the dimension, so that the dimension means the same thing no matter where it’s used.</p>
</blockquote>
<p>#TODO: continue from https://www.itprotoday.com/sql-server/data-warehouse-bus-architecture</p>
<p>#TODO. Inflow, Upflow, Downflow, Outflow and Meta flow.</p>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="backlog"><a class="header" href="#backlog">Backlog</a></h1>
<h2 id="topics"><a class="header" href="#topics">Topics</a></h2>
<p><strong>Fault Tolerant Systems</strong>:</p>
<ul>
<li>General Reliability Development Hazard logs (FRACAS) [Redundancia]</li>
<li>High Availability [https://www.controlglobal.com/assets/14WPpdf/140324-ISA-ControlSystemsHighAvailability.pdf]</li>
<li>Technical documentation</li>
<li>Safety cases</li>
<li>Bulkhead</li>
<li>Change Control</li>
<li>Cold Standby</li>
<li>Defensive Design</li>
<li>Derating</li>
<li>Design Debt</li>
<li>Design Life</li>
<li>Design Thinking</li>
<li>Durability</li>
<li>Edge Case</li>
<li>Entropy</li>
<li>Error Tolerance</li>
<li>Fault Tolerance</li>
<li>Fail Well</li>
<li>Fail-Safe</li>
<li>Graceful Degradation</li>
<li>Mistake Proofing &amp; Poka Yoke Technique</li>
<li>No Fault Found</li>
<li>Resilience</li>
<li>Safety by Design</li>
<li>Self-Healing</li>
<li>Service Life</li>
<li>Systems Thinking</li>
<li>Testbed</li>
<li>Waer and Tear</li>
<li>Deconstructability</li>
<li>Refinement</li>
<li>Defense in Depth</li>
<li>FMEA Design and Process</li>
<li>Physics of Failure (PoF)</li>
<li>Built-in Self-test</li>
<li>Eliminating single point of failure (SPOF)</li>
</ul>
<p><strong>Analysis</strong>:</p>
<ul>
<li>Root Cause analysis</li>
<li>Fault tree analysis (FTA)</li>
<li>Failure mode and effects analysis (FMEA)</li>
<li>Failure mode, effects and criticality analysis (FMECA)</li>
<li>Reliability, Availability and Maintainability Study (RAMS)</li>
<li>Mission Readiness analysis</li>
<li>Functional System Failure analysis</li>
<li>Inherent Design Reliability analysis</li>
<li>Use/Load analysis and wear calculations</li>
<li>Fatigue and creep analysis</li>
<li>Component Stress analysis</li>
<li>Field failure monitoring</li>
<li>Field data analysis</li>
<li>Caution and warning analysis</li>
<li>Chaos Engineering</li>
<li>Reliability Risk Assessments</li>
<li>Hazard analysis</li>
<li>Manufactoring defect analysis</li>
<li>Residual Risk analysis (RCA)</li>
<li>Weibull</li>
<li>Accelerated Life Testing (ALT Analysis)</li>
<li>Material Strength analysis</li>
<li>Quality of Service</li>
<li>Quality Control</li>
<li>Defect Rate</li>
<li>Failure Rate</li>
<li>Mean Time Between Failures</li>
<li>Mean Time to Repair (MTTR)</li>
<li>Mean Corrective Maintenance Time (MCMT)</li>
<li>Mean Preventive Maintenance Time (MPMT)</li>
<li>Mean Maintenance Hours per Repair (MMH/Repair)</li>
<li>Maximum Corrective Maintenance Time (MaxCMT)</li>
</ul>
<p><strong>Data Quality</strong>:</p>
<ul>
<li>Data Quality Completeness</li>
<li>Data Quality Correctness</li>
<li>Data Quality Credibility</li>
<li>Data Quality Precision</li>
<li>Data Quality Relevance</li>
<li>Data Quality Timeliness</li>
<li>Data Quality Traceability</li>
<li>Data Integrity</li>
<li>Data Cleansing</li>
<li>Data Corruption</li>
<li>Data Degradation</li>
<li>Data Artifact</li>
<li>Data Rot</li>
<li>Information Quality Accurate</li>
<li>Information Quality Completeness</li>
<li>Information Quality Comprehensible</li>
<li>Information Quality Credibility</li>
<li>Information Quality Precision</li>
<li>Information Quality Relevance</li>
<li>Information Quality Timeliness</li>
<li>Information Quality Uniqueness</li>
<li>Conformance Quality</li>
<li>Credence Quality</li>
<li>Quality Assurance</li>
<li>Quality Control</li>
<li>Service Quality</li>
<li>Experience Quality</li>
<li>Code Smell</li>
<li>Referential Integrity</li>
<li>Reusability</li>
</ul>
<p><strong>Maintenance</strong>:</p>
<ul>
<li>Maintenance Requirement Allocation</li>
<li>Predictive and Preventive maintenance</li>
<li>Reliability Centered Maintenance (RCM)</li>
</ul>
<p><strong>Failures</strong>:</p>
<ul>
<li>Manufactoring-induced failures</li>
<li>Assembly-induced failures</li>
<li>Transport-induced failures</li>
<li>Storage-induced failures</li>
<li>Systmatic failures</li>
</ul>
<p><strong>Tests</strong>:</p>
<ul>
<li>System Diagnostics Design</li>
<li>Failure/Reliability testing</li>
</ul>
<p><strong>Human Factors</strong>:</p>
<ul>
<li>Human Factors</li>
<li>Human Interaction</li>
<li>Human Errors</li>
<li>Latent Human Error</li>
</ul>
<p><strong>DataOps</strong>:</p>
<p><strong>Business Process Management</strong>:</p>
<ul>
<li>BPM</li>
<li>BPI</li>
<li>BPE</li>
<li>BPA</li>
<li>BPR</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="assets/mermaid.min.js"></script>
        <script type="text/javascript" src="assets/mermaid-init.js"></script>

        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </body>
</html>
