<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Data Reliability Engineering</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/mdbook-admonish.css">
        <link rel="stylesheet" href="theme/catppuccin.css">
        <link rel="stylesheet" href="theme/catppuccin-admonish.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="COVER.html">Cover</a></li><li class="chapter-item expanded affix "><a href="TITLE.html">Title</a></li><li class="chapter-item expanded affix "><a href="SUMMARY.html">Summary</a></li><li class="chapter-item expanded affix "><a href="DEDICATION.html">Dedication</a></li><li class="chapter-item expanded affix "><a href="FOREWORD.html">Foreword</a></li><li class="chapter-item expanded affix "><a href="PREFACE.html">Preface</a></li><li class="chapter-item expanded affix "><a href="AUTHOR.html">Author</a></li><li class="chapter-item expanded affix "><a href="OBJECTIVES.html">Objectives</a></li><li class="chapter-item expanded affix "><a href="STRUCTURE.html">Structure</a></li><li class="chapter-item expanded "><a href="CONCEPTS.html"><strong aria-hidden="true">1.</strong> I - Concepts</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/systems_intro.html"><strong aria-hidden="true">1.1.</strong> Introduction to Systems</a></li><li class="chapter-item expanded "><a href="concepts/data_architecture.html"><strong aria-hidden="true">1.2.</strong> Data Architecture</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data_sources.html"><strong aria-hidden="true">1.2.1.</strong> Data Sources</a></li><li class="chapter-item expanded "><a href="concepts/data_tier.html"><strong aria-hidden="true">1.2.2.</strong> Data Tier</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data_lake.html"><strong aria-hidden="true">1.2.2.1.</strong> Data Lake</a></li><li class="chapter-item expanded "><a href="concepts/data_warehouse.html"><strong aria-hidden="true">1.2.2.2.</strong> Data Warehouse</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data_warehouse_tier_architecture.html"><strong aria-hidden="true">1.2.2.2.1.</strong> Two-Tier vs Three-Tier Architecture</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data_warehouse_application_tier.html"><strong aria-hidden="true">1.2.2.2.1.1.</strong> Application Tier</a></li></ol></li><li class="chapter-item expanded "><a href="concepts/data_modelling.html"><strong aria-hidden="true">1.2.2.2.2.</strong> Data Modelling</a></li><li class="chapter-item expanded "><a href="concepts/data-architecture/slowly_changing_dimensions.html"><strong aria-hidden="true">1.2.2.2.3.</strong> Slowly Changing Dimensions (SCD)</a></li></ol></li><li class="chapter-item expanded "><a href="concepts/data_lakehouse.html"><strong aria-hidden="true">1.2.2.3.</strong> Data Lakehouse</a></li><li class="chapter-item expanded "><a href="concepts/data_marts.html"><strong aria-hidden="true">1.2.2.4.</strong> Data Marts</a></li></ol></li><li class="chapter-item expanded "><a href="concepts/application_tier.html"><strong aria-hidden="true">1.2.3.</strong> Application Tier</a></li><li class="chapter-item expanded "><a href="concepts/presentation_tier.html"><strong aria-hidden="true">1.2.4.</strong> Presentation Tier</a></li><li class="chapter-item expanded "><a href="concepts/metadata_management_tools.html"><strong aria-hidden="true">1.2.5.</strong> Metadata Management Tools</a></li><li class="chapter-item expanded "><a href="concepts/data-architecture/operational_data_stores.html"><strong aria-hidden="true">1.2.6.</strong> Operational Data Stores vs. Data Operational Stores</a></li></ol></li><li class="chapter-item expanded "><a href="concepts/systems_reliability.html"><strong aria-hidden="true">1.3.</strong> Systems Reliability</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/systems-reliability/impediments.html"><strong aria-hidden="true">1.3.1.</strong> Impediments</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/attributes.html"><strong aria-hidden="true">1.3.2.</strong> Attributes</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/mechanisms.html"><strong aria-hidden="true">1.3.3.</strong> Mechanisms</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/systems-reliability/fault_prevention_avoidance.html"><strong aria-hidden="true">1.3.3.1.</strong> Fault Prevention: Avoidance</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/fault_tolerance.html"><strong aria-hidden="true">1.3.3.2.</strong> Fault Tolerance</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/fault_prevention_elimination.html"><strong aria-hidden="true">1.3.3.3.</strong> Fault Prevention: Elimination</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/fault_prediction.html"><strong aria-hidden="true">1.3.3.4.</strong> Fault Prediction</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/reliability_tools.html"><strong aria-hidden="true">1.3.3.5.</strong> Reliability Tools</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/systems-reliability/observability_tools.html"><strong aria-hidden="true">1.3.3.5.1.</strong> Observability Tools</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/data_quality_automation_tools.html"><strong aria-hidden="true">1.3.3.5.2.</strong> Data Quality Automation Tools</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/version_control_systems.html"><strong aria-hidden="true">1.3.3.5.3.</strong> Version Control Systems</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/data_lineage_tools.html"><strong aria-hidden="true">1.3.3.5.4.</strong> Data Lineage Tools</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/workflow_orchestration_tools.html"><strong aria-hidden="true">1.3.3.5.5.</strong> Workflow Orchestration Tools</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/data_transformation_tools.html"><strong aria-hidden="true">1.3.3.5.6.</strong> Data Transformation and Testing Tools</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/infrastructure_as_code_tools.html"><strong aria-hidden="true">1.3.3.5.7.</strong> Infrastructure as Code Tools</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/container_orchestration_tools.html"><strong aria-hidden="true">1.3.3.5.8.</strong> Container Orchestration Tools</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/fracas.html"><strong aria-hidden="true">1.3.3.5.9.</strong> Failure Reporting, Analysis, and Corrective Action System (FRACAS)</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/corrective_actions.html"><strong aria-hidden="true">1.3.3.5.10.</strong> Corrective Actions</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/reliability_block_diagrams.html"><strong aria-hidden="true">1.3.3.5.11.</strong> Reliability Block Diagrams</a></li></ol></li></ol></li></ol></li><li class="chapter-item expanded "><a href="concepts/data_quality.html"><strong aria-hidden="true">1.4.</strong> Data Quality</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data-quality/foundations.html"><strong aria-hidden="true">1.4.1.</strong> Foundations of Data Quality</a></li><li class="chapter-item expanded "><a href="concepts/data-quality/master_data.html"><strong aria-hidden="true">1.4.2.</strong> Master Data</a></li><li class="chapter-item expanded "><a href="concepts/data-quality/management.html"><strong aria-hidden="true">1.4.3.</strong> Data Management</a></li><li class="chapter-item expanded "><a href="concepts/data-quality/models.html"><strong aria-hidden="true">1.4.4.</strong> Data Quality Models</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data-quality/accuracy_dimension.html"><strong aria-hidden="true">1.4.4.1.</strong> Accuracy Dimension</a></li><li class="chapter-item expanded "><a href="concepts/data-quality/completeness_dimension.html"><strong aria-hidden="true">1.4.4.2.</strong> Completeness Dimension</a></li><li class="chapter-item expanded "><a href="concepts/data-quality/consistency_dimension.html"><strong aria-hidden="true">1.4.4.3.</strong> Consistency Dimension</a></li><li class="chapter-item expanded "><a href="concepts/data-quality/timeliness_dimension.html"><strong aria-hidden="true">1.4.4.4.</strong> Timeliness Dimension</a></li><li class="chapter-item expanded "><a href="concepts/data-quality/relevance_dimension.html"><strong aria-hidden="true">1.4.4.5.</strong> Relevance Dimension</a></li><li class="chapter-item expanded "><a href="concepts/data-quality/reliability_dimension.html"><strong aria-hidden="true">1.4.4.6.</strong> Reliability Dimension</a></li><li class="chapter-item expanded "><a href="concepts/data-quality/uniqueness_dimension.html"><strong aria-hidden="true">1.4.4.7.</strong> Uniqueness Dimension</a></li><li class="chapter-item expanded "><a href="concepts/data-quality/validity_dimension.html"><strong aria-hidden="true">1.4.4.8.</strong> Validity Dimension</a></li><li class="chapter-item expanded "><a href="concepts/data-quality/accessibility_dimension.html"><strong aria-hidden="true">1.4.4.9.</strong> Accessibility Dimension</a></li><li class="chapter-item expanded "><a href="concepts/data-quality/integrity_dimension.html"><strong aria-hidden="true">1.4.4.10.</strong> Integrity Dimension</a></li><li class="chapter-item expanded "><a href="concepts/data-quality/metrics_database.html"><strong aria-hidden="true">1.4.4.11.</strong> Metrics/Audit Database & Service</a></li><li class="chapter-item expanded "><a href="concepts/data-quality/dimensions_final_thoughts.html"><strong aria-hidden="true">1.4.4.12.</strong> Final Thoughts on Data Quality Dimensions</a></li></ol></li><li class="chapter-item expanded "><a href="concepts/data-quality/final_thoughts.html"><strong aria-hidden="true">1.4.5.</strong> Final Thoughts on Data Quality</a></li></ol></li><li class="chapter-item expanded "><a href="concepts/data_relibility.html"><strong aria-hidden="true">1.5.</strong> Data Reliability</a></li><li class="chapter-item expanded "><a href="concepts/processes.html"><strong aria-hidden="true">1.6.</strong> Processes</a></li><li class="chapter-item expanded "><a href="concepts/operations.html"><strong aria-hidden="true">1.7.</strong> Operations</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.</strong> II - Use Cases</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.</strong> A - Aranduka Inc.</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.</strong> Data Architecture</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.1.</strong> Operational System and Internal Data Sources</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.2.</strong> Integrating Data Partners</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.3.</strong> Designing the Data Lake</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.3.1.</strong> Anonymized Data</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.3.2.</strong> Distilled Data</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.4.</strong> Designing the Data Warehouse</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.4.1.</strong> Core Data</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.5.</strong> Designing the Data Marts</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.2.</strong> BICC & BI</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.2.1.</strong> Building Reliable Pipelines</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.2.2.</strong> Data Quality Assurance & Monitoring</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.2.3.</strong> Continuous Service</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.3.</strong> Growth, Marketing & Attribution Models</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.4.</strong> Multidimensional Analysis: Geo vs Verticals</div></li></ol></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.</strong> III - Incorporating Data Reliability Engineering</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">3.1.</strong> Solutions Architects</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">3.1.1.</strong> Data Architect</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.2.</strong> Data Engineers</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.3.</strong> Backend Engineers</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.4.</strong> BI Engineers</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.</strong> IV - Appendices</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">4.1.</strong> Reliability Tools</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/systems-reliability/chaos_engineering_tools.html"><strong aria-hidden="true">4.1.1.</strong> Chaos Engineering Tools</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/high_availability.html"><strong aria-hidden="true">4.1.2.</strong> High Availability</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/antifragility.html"><strong aria-hidden="true">4.1.3.</strong> Antifragility</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/bulkhead_pattern.html"><strong aria-hidden="true">4.1.4.</strong> Bulkhead Pattern</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/cold_standby.html"><strong aria-hidden="true">4.1.5.</strong> Cold Standby</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/single_point_of_failure.html"><strong aria-hidden="true">4.1.6.</strong> Single Point of Failure (SPOF)</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/grdhl.html"><strong aria-hidden="true">4.1.7.</strong> General Reliability Development Hazard Logs (GRDHL)</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/spare_parts_stocking_strategy.html"><strong aria-hidden="true">4.1.8.</strong> Spare Parts Stocking Strategy</a></li><li class="chapter-item expanded "><a href="concepts/systems-reliability/availability_controls.html"><strong aria-hidden="true">4.1.9.</strong> Availability Controls</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="EPILOGUE.html">Epilogue</a></li><li class="chapter-item expanded affix "><a href="DICTIONARY.html">Dictionary</a></li><li class="chapter-item expanded affix "><a href="REFERENCES.html">References</a></li><li class="chapter-item expanded affix "><a href="NEXT.html">Next</a></li><li class="chapter-item expanded affix "><a href="BACK_COVER.html">Back Cover</a></li><li class="chapter-item expanded affix "><a href="backlog.html">Backlog</a></li><li class="chapter-item expanded affix "><a href="test/images.html">Test: Images</a></li><li class="chapter-item expanded affix "><a href="test/admonish.html">Test: Admonishes</a></li><li class="chapter-item expanded affix "><a href="test/diagrams.html">Test: Diagrams</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="latte">Latte</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="frappe">Frappé</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="macchiato">Macchiato</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mocha">Mocha</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Data Reliability Engineering</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/jeffersonroth/jjrf-data-reliability-book/" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div style="break-before: page; page-break-before: always;"></div><p>Data Reliability Engineering</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="summary"><a class="header" href="#summary">Summary</a></h1>
<!-- markdownlint-disable no-empty-links -->
<p><a href="./COVER.html">Cover</a>
<a href="./TITLE.html">Title</a>
<a href="./SUMMARY.html">Summary</a>
<a href="./DEDICATION.html">Dedication</a>
<a href="./FOREWORD.html">Foreword</a>
<a href="./PREFACE.html">Preface</a>
<a href="./AUTHOR.html">Author</a>
<a href="./OBJECTIVES.html">Objectives</a>
<a href="./STRUCTURE.html">Structure</a></p>
<ul>
<li><a href="./CONCEPTS.html">I - Concepts</a>
<ul>
<li><a href="./concepts/systems_intro.html">Introduction to Systems</a></li>
<li><a href="./concepts/data_architecture.html">Data Architecture</a>
<ul>
<li><a href="./concepts/data_sources.html">Data Sources</a></li>
<li><a href="./concepts/data_tier.html">Data Tier</a>
<ul>
<li><a href="./concepts/data_lake.html">Data Lake</a></li>
<li><a href="./concepts/data_warehouse.html">Data Warehouse</a>
<ul>
<li><a href="./concepts/data_warehouse_tier_architecture.html">Two-Tier vs Three-Tier Architecture</a>
<ul>
<li><a href="./concepts/data_warehouse_application_tier.html">Application Tier</a></li>
</ul>
</li>
<li><a href="./concepts/data_modelling.html">Data Modelling</a></li>
<li><a href="./concepts/data-architecture/slowly_changing_dimensions.html">Slowly Changing Dimensions (SCD)</a></li>
</ul>
</li>
<li><a href="./concepts/data_lakehouse.html">Data Lakehouse</a></li>
<li><a href="./concepts/data_marts.html">Data Marts</a></li>
</ul>
</li>
<li><a href="./concepts/application_tier.html">Application Tier</a></li>
<li><a href="./concepts/presentation_tier.html">Presentation Tier</a></li>
<li><a href="./concepts/metadata_management_tools.html">Metadata Management Tools</a></li>
<li><a href="./concepts/data-architecture/operational_data_stores.html">Operational Data Stores vs. Data Operational Stores</a></li>
</ul>
</li>
<li><a href="./concepts/systems_reliability.html">Systems Reliability</a>
<ul>
<li><a href="./concepts/systems-reliability/impediments.html">Impediments</a></li>
<li><a href="./concepts/systems-reliability/attributes.html">Attributes</a></li>
<li><a href="./concepts/systems-reliability/mechanisms.html">Mechanisms</a>
<ul>
<li><a href="./concepts/systems-reliability/fault_prevention_avoidance.html">Fault Prevention: Avoidance</a></li>
<li><a href="./concepts/systems-reliability/fault_tolerance.html">Fault Tolerance</a></li>
<li><a href="./concepts/systems-reliability/fault_prevention_elimination.html">Fault Prevention: Elimination</a></li>
<li><a href="./concepts/systems-reliability/fault_prediction.html">Fault Prediction</a></li>
<li><a href="./concepts/systems-reliability/reliability_tools.html">Reliability Tools</a>
<ul>
<li><a href="./concepts/systems-reliability/observability_tools.html">Observability Tools</a></li>
<li><a href="./concepts/systems-reliability/data_quality_automation_tools.html">Data Quality Automation Tools</a></li>
<li><a href="./concepts/systems-reliability/version_control_systems.html">Version Control Systems</a></li>
<li><a href="./concepts/systems-reliability/data_lineage_tools.html">Data Lineage Tools</a></li>
<li><a href="./concepts/systems-reliability/workflow_orchestration_tools.html">Workflow Orchestration Tools</a></li>
<li><a href="./concepts/systems-reliability/data_transformation_tools.html">Data Transformation and Testing Tools</a></li>
<li><a href="./concepts/systems-reliability/infrastructure_as_code_tools.html">Infrastructure as Code Tools</a></li>
<li><a href="./concepts/systems-reliability/container_orchestration_tools.html">Container Orchestration Tools</a></li>
<li><a href="./concepts/systems-reliability/fracas.html">Failure Reporting, Analysis, and Corrective Action System (FRACAS)</a></li>
<li><a href="./concepts/systems-reliability/corrective_actions.html">Corrective Actions</a></li>
<li><a href="./concepts/systems-reliability/reliability_block_diagrams.html">Reliability Block Diagrams</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="./concepts/data_quality.html">Data Quality</a>
<ul>
<li><a href="./concepts/data-quality/foundations.html">Foundations of Data Quality</a></li>
<li><a href="./concepts/data-quality/master_data.html">Master Data</a></li>
<li><a href="./concepts/data-quality/management.html">Data Management</a></li>
<li><a href="./concepts/data-quality/models.html">Data Quality Models</a>
<ul>
<li><a href="./concepts/data-quality/accuracy_dimension.html">Accuracy Dimension</a></li>
<li><a href="./concepts/data-quality/completeness_dimension.html">Completeness Dimension</a></li>
<li><a href="./concepts/data-quality/consistency_dimension.html">Consistency Dimension</a></li>
<li><a href="./concepts/data-quality/timeliness_dimension.html">Timeliness Dimension</a></li>
<li><a href="./concepts/data-quality/relevance_dimension.html">Relevance Dimension</a></li>
<li><a href="./concepts/data-quality/reliability_dimension.html">Reliability Dimension</a></li>
<li><a href="./concepts/data-quality/uniqueness_dimension.html">Uniqueness Dimension</a></li>
<li><a href="./concepts/data-quality/validity_dimension.html">Validity Dimension</a></li>
<li><a href="./concepts/data-quality/accessibility_dimension.html">Accessibility Dimension</a></li>
<li><a href="./concepts/data-quality/integrity_dimension.html">Integrity Dimension</a></li>
<li><a href="./concepts/data-quality/metrics_database.html">Metrics/Audit Database &amp; Service</a></li>
<li><a href="./concepts/data-quality/dimensions_final_thoughts.html">Final Thoughts on Data Quality Dimensions</a></li>
</ul>
</li>
<li><a href="./concepts/data-quality/final_thoughts.html">Final Thoughts on Data Quality</a></li>
</ul>
</li>
<li><a href="./concepts/data_relibility.html">Data Reliability</a></li>
<li><a href="./concepts/processes.html">Processes</a></li>
<li><a href="./concepts/operations.html">Operations</a></li>
</ul>
</li>
<li><a href="">II - Use Cases</a>
<ul>
<li><a href="">A - Aranduka Inc.</a>
<ul>
<li><a href="">Data Architecture</a>
<ul>
<li><a href="">Operational System and Internal Data Sources</a></li>
<li><a href="">Integrating Data Partners</a></li>
<li><a href="">Designing the Data Lake</a>
<ul>
<li><a href="">Anonymized Data</a></li>
<li><a href="">Distilled Data</a></li>
</ul>
</li>
<li><a href="">Designing the Data Warehouse</a>
<ul>
<li><a href="">Core Data</a></li>
</ul>
</li>
<li><a href="">Designing the Data Marts</a></li>
</ul>
</li>
<li><a href="">BICC &amp; BI</a>
<ul>
<li><a href="">Building Reliable Pipelines</a></li>
<li><a href="">Data Quality Assurance &amp; Monitoring</a></li>
<li><a href="">Continuous Service</a></li>
</ul>
</li>
<li><a href="">Growth, Marketing &amp; Attribution Models</a></li>
<li><a href="">Multidimensional Analysis: Geo vs Verticals</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="">III - Incorporating Data Reliability Engineering</a>
<ul>
<li><a href="">Solutions Architects</a>
<ul>
<li><a href="">Data Architect</a></li>
</ul>
</li>
<li><a href="">Data Engineers</a></li>
<li><a href="">Backend Engineers</a></li>
<li><a href="">BI Engineers</a></li>
</ul>
</li>
<li><a href="">IV - Appendices</a>
<ul>
<li><a href="">Reliability Tools</a>
<ul>
<li><a href="./concepts/systems-reliability/chaos_engineering_tools.html">Chaos Engineering Tools</a></li>
<li><a href="./concepts/systems-reliability/high_availability.html">High Availability</a></li>
<li><a href="./concepts/systems-reliability/antifragility.html">Antifragility</a></li>
<li><a href="./concepts/systems-reliability/bulkhead_pattern.html">Bulkhead Pattern</a></li>
<li><a href="./concepts/systems-reliability/cold_standby.html">Cold Standby</a></li>
<li><a href="./concepts/systems-reliability/single_point_of_failure.html">Single Point of Failure (SPOF)</a></li>
<li><a href="./concepts/systems-reliability/grdhl.html">General Reliability Development Hazard Logs (GRDHL)</a></li>
<li><a href="./concepts/systems-reliability/spare_parts_stocking_strategy.html">Spare Parts Stocking Strategy</a></li>
<li><a href="./concepts/systems-reliability/availability_controls.html">Availability Controls</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><a href="./EPILOGUE.html">Epilogue</a>
<a href="./DICTIONARY.html">Dictionary</a>
<a href="./REFERENCES.html">References</a>
<a href="./NEXT.html">Next</a>
<a href="./BACK_COVER.html">Back Cover</a></p>
<p><a href="./backlog.html">Backlog</a></p>
<p><a href="./test/images.html">Test: Images</a>
<a href="./test/admonish.html">Test: Admonishes</a>
<a href="./test/diagrams.html">Test: Diagrams</a></p>
<!-- markdownlint-enable no-empty-links --><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="concepts"><a class="header" href="#concepts">Concepts</a></h1>
<blockquote>
<p>The first part of this book exposes the different concepts around the data reliability engineering subject. It's intended to be heavily technical, in contrast with the subsequent parts, crafted to explore practical use cases.</p>
</blockquote>
<h2 id="introduction-to-systems-and-systems-reliability"><a class="header" href="#introduction-to-systems-and-systems-reliability"><a href="./concepts/systems_intro.html">Introduction to Systems</a> and <a href="./concepts/systems_reliability.html">Systems Reliability</a></a></h2>
<blockquote>
<p>These chapters explore the concepts of systems, reliability, and how to understand systems reliability, especially its impediments, attributes, and mechanisms to design and maintain reliable systems.</p>
</blockquote>
<h2 id="data-architecture"><a class="header" href="#data-architecture"><a href="./concepts/data_architecture.html">Data Architecture</a></a></h2>
<blockquote>
<p>This chapter explores in depth concepts of data architecture, including its sources, its storage (Data Lake, Data Warehouses, Data Marts), its application (OLAP servers, processing engines), and its presentation (dashboards, reports).</p>
</blockquote>
<h2 id="data-quality"><a class="header" href="#data-quality"><a href="./concepts/data_quality.html">Data Quality</a></a></h2>
<blockquote>
<p>This chapter explores the concepts of data, quality, and data quality, to finally explore the concepts of data reliability. The goal is to understand these concepts in all aspects of the data: life cycle, design, modeling, governance, management, access, security, uses, legal frameworks, best practices, maturity, standards, etc.</p>
</blockquote>
<h2 id="processes"><a class="header" href="#processes"><a href="./concepts/processes.html">Processes</a></a></h2>
<blockquote>
<p>This chapter explores, for a given system, the concept of data processes, data and information flow, workflows, orchestration, pipelines, ETL, and ELT.</p>
</blockquote>
<h2 id="operations"><a class="header" href="#operations"><a href="./concepts/operations.html">Operations</a></a></h2>
<blockquote>
<p>This chapter explores the concept of SRE, DataOps, DevOps, Agile methodologies, CI/CD, and other methodologies to ensure reliable data operations.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction-to-systems"><a class="header" href="#introduction-to-systems">Introduction to Systems</a></h1>
<blockquote>
<p>For the context of this book, a system is a collection of interrelated components working together towards a common goal, often to process, store, and manage data. These components can include hardware, software, databases, procedures, and people, all interacting in a structured way to achieve efficient and reliable data handling.</p>
</blockquote>
<p>To better understand data reliability engineering, it's essential to understand a system from a <strong>technical</strong> and <strong>operational perspective</strong>.
Technically, a system would include the architecture, technology, and protocols that ensure data integrity, availability, and consistency.
Operationally, it involves the procedures and practices that maintain the system's performance and reliability over time.</p>
<p>When discussing data reliability, a system is the entire ecosystem supporting the data lifecycle, from creation and storage to retrieval and usage.
A system includes considerations of redundancy, fault tolerance, backup procedures, security measures, and regular maintenance practices, all of which contribute to the system's overall reliability and the trustworthiness of its service.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-architecture-1"><a class="header" href="#data-architecture-1">Data Architecture</a></h1>
<blockquote>
<p>Data Architecture is about how the data is managed, from <strong>collection</strong>, <strong>transformations</strong>, <strong>distribution</strong>, and <strong>consumption</strong>.</p>
</blockquote>
<p>It includes the models, policies, rules, and standards governing which data is collected and how it is stored, arranged, integrated, and put to use in data systems within the organization.</p>
<p>The data architecture aims to set standards for all data systems and the interaction between them. It also dictates and materializes the organization's understanding of its business in a Conceptual, Logical, and Physical level.</p>
<p>The Zachman Framework for enterprise architecture understands the data architecture in five layers:</p>
<ol>
<li>Scope/Contextual: subjects and architectural standards important for the business.</li>
<li>Business/Conceptual: business entities, their attributes, and associations.</li>
<li>System/Logical: how entities are related.</li>
<li>Technology/Physical: representation of a data design as implemented in a database management system.</li>
<li>Detailed Representations: databases.</li>
</ol>
<h2 id="conceptual-layer-cdm"><a class="header" href="#conceptual-layer-cdm">Conceptual Layer (CDM)</a></h2>
<blockquote>
<p>Represents all <strong>Business Entities</strong>.</p>
</blockquote>
<p>The <strong>Conceptual Data Model</strong> (CDM) consists of the <strong>Business Entities</strong>, like <strong>User</strong>, <strong>Branch</strong>, and <strong>Product</strong>. The business entities (or business objects) carry <em>attributes</em> (name, identifiers, timestamps, etc.), and <em>associations</em> (relationships) with other business entities.
The complete set of business entities represents the business relationships.</p>
<p>From a data architecture perspective, these business entities are represented in a <strong>Conceptual Schema</strong>, which consists of a map of <strong>concepts</strong> (business entities) and their <strong>relationships</strong> in a database, normally in a <strong>Data Structure Diagram</strong> (DSD). It may also include <strong>Enterprise Data Modelling</strong> (EDM) outputs like entity–relationship diagrams (ERDs), XML schemas (XSD), and an enterprise-wide data dictionary.</p>
<p>The conceptual schema describes the semantics of an organization and represents a series of assertions about its nature. It describes the objects of significance (business entities) the organization is interested in collecting information, their characteristics (attributes), and the associations between each pair of objects of significance (relationships). Please note that the conceptual schema is not the actual database design, and is represented in different abstraction layers.</p>
<p>Examples:</p>
<ul>
<li>Each ORDER must be from one and only one USER.</li>
<li>Each ORDER contains one or more PRODUCTS.</li>
<li>Each ORDER contains products from one or more BRANCHES.</li>
</ul>
<h2 id="logical-layer-ldm"><a class="header" href="#logical-layer-ldm">Logical Layer (LDM)</a></h2>
<blockquote>
<p>Represents the logic and how the entities are related.</p>
</blockquote>
<p>The <strong>Logical Data Model</strong> (LDM), also known as the Domain Model, represents the abstract structure of a domain of information, expressed independently of a particular database management product or storage technology (physical data model), but in terms of data structures such as relational tables and columns, object-oriented classes, or XML tags.</p>
<p>Once validated and approved, the logical data model can become the basis of a physical data model and form the design of a database.</p>
<h2 id="physical-layer-pdm"><a class="header" href="#physical-layer-pdm">Physical Layer (PDM)</a></h2>
<blockquote>
<p>The representation of a data design as implemented, or intended to be implemented, in a database management system.</p>
</blockquote>
<p>The <strong>Physical Data Model</strong> (PDM) typically derives from a logical data model (LDM), though it may be reverse-engineered from a given database implementation. A complete physical data model will include all the database artifacts required to create relationships between tables or to achieve performance goals, such as indexes, constraint definitions, linking tables, partitioned tables, or clusters.</p>
<h3 id="cdm-vs-ldm-vs-pdm"><a class="header" href="#cdm-vs-ldm-vs-pdm">CDM vs. LDM vs. PDM</a></h3>
<h4 id="data-constructs"><a class="header" href="#data-constructs">Data Constructs</a></h4>
<ul>
<li><strong>Conceptual Data Model</strong> (CDM): uses general high-level data constructs from which Architectural Descriptions are created in non-technical terms.</li>
<li><strong>Logical Data Model</strong> (LDM): includes entities (tables), attributes (columns/fields), and relationships (keys). Is independent of technology (platform, DBMS).</li>
<li><strong>Physical Data Model</strong> (PDM):  includes tables, columns, keys, data types, validation rules, database triggers, stored procedures, domains, and access constraints as primary keys and indices for fast data access.</li>
</ul>
<h4 id="naming-conventions"><a class="header" href="#naming-conventions">Naming Conventions</a></h4>
<ul>
<li><strong>Conceptual Data Model</strong> (CDM): uses non-technical names, so that executives and managers at all levels can understand the data basis of Architectural Description.</li>
<li><strong>Logical Data Model</strong> (LDM): uses business names for entities &amp; attributes.</li>
<li><strong>Physical Data Model</strong> (PDM): uses more defined and less generic specific names for tables and columns, such as abbreviated column names, limited by the database management system (DBMS) and any company-defined standards.</li>
</ul>
<h2 id="modern-data-architecture"><a class="header" href="#modern-data-architecture">Modern Data Architecture</a></h2>
<p>A modern approach to data architecture, extensively adopted by StartUps, reduces, restricts, or understands the data architecture as the implementation of a Data Warehouse, a Data Lake + Data Warehouse, or a Data Lakehouse architecture, consisting of the data sources plus two or three tiers:</p>
<ul>
<li>Bottom/Data Tier: data warehouse server with functional gateway (ODBC, JDBC, etc.). It may also include the data lake.</li>
<li>Middle/Application Tier: houses the business logic used to process user inputs (OLAP Servers, Snowflake, Amazon Redshift, Databricks Data Lakehouse Platform, Apache Spark, etc.).</li>
<li>Top/Presentation Tier: front-end tools (Power BI, Tableau, etc.).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="data-lake"><a class="header" href="#data-lake">Data Lake</a></h1>
<blockquote>
<p>A Data Lake is a data repository for storing large amounts of Structured, Semi-Structured, and Unstructured data.</p>
</blockquote>
<p>It is a repository for storing all types of data in its native format without fixed limits on account size or file.
The Data Lake stores a high data quantity to increase native integration and analytic performance.
The Data Lake democratizes data and provides a cost-effective way of storing all organization data for later processing.</p>
<ul>
<li><a href="concepts/data_lake.html#data-lake-vs-data-warehouse">Data Lake vs Data Warehouse</a></li>
<li><a href="concepts/data_lake.html#goals">Goals</a></li>
<li><a href="concepts/data_lake.html#data-lake-architecture">Data Lake Architecture</a>
<ul>
<li><a href="concepts/data_lake.html#layers">Layers</a>
<ul>
<li><a href="concepts/data_lake.html#ingestion-layer-bronze">Ingestion Layer (Bronze)</a></li>
<li><a href="concepts/data_lake.html#distillation-layer-silver">Distillation Layer (Silver)</a></li>
<li><a href="concepts/data_lake.html#processing-layer-gold">Processing Layer (Gold)</a></li>
<li><a href="concepts/data_lake.html#insights-layer">Insights Layer</a></li>
<li><a href="concepts/data_lake.html#unified-operations-layer">Unified Operations Layer</a></li>
</ul>
</li>
<li><a href="concepts/data_lake.html#zones">Zones</a>
<ul>
<li><a href="concepts/data_lake.html#landing-zone">Landing Zone</a></li>
<li><a href="concepts/data_lake.html#raw-zone">Raw Zone</a></li>
<li><a href="concepts/data_lake.html#harmonized-zone">Harmonized Zone</a></li>
<li><a href="concepts/data_lake.html#distilled-zone">Distilled Zone</a></li>
<li><a href="concepts/data_lake.html#explorative-zone">Explorative Zone</a></li>
<li><a href="concepts/data_lake.html#delivery-zone">Delivery Zone</a></li>
<li><a href="concepts/data_lake.html#zones-comparisom">Zones Comparisom</a></li>
</ul>
</li>
<li><a href="concepts/data_lake.html#sandbox">Sandbox</a></li>
<li><a href="concepts/data_lake.html#maturity-stages">Maturity Stages</a>
<ul>
<li><a href="concepts/data_lake.html#handle-and-ingest-data-at-scale">Handle and Ingest data at scale</a></li>
<li><a href="concepts/data_lake.html#building-the-analytical-muscle">Building the analytical muscle</a></li>
<li><a href="concepts/data_lake.html#data-warehouse-and-data-lake-work-in-unison">Data Warehouse and Data Lake work in unison</a></li>
<li><a href="concepts/data_lake.html#enterprise-capability-in-the-lake">Enterprise capability in the lake</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="concepts/data_lake.html#key-components-of-data-lake-architecture">Key Components of Data Lake Architecture</a>
<ul>
<li><a href="concepts/data_lake.html#data-ingestion">Data Ingestion</a></li>
<li><a href="concepts/data_lake.html#data-storage">Data Storage</a></li>
<li><a href="concepts/data_lake.html#data-governance">Data Governance</a></li>
<li><a href="concepts/data_lake.html#security">Security</a></li>
<li><a href="concepts/data_lake.html#data-quality">Data Quality</a></li>
<li><a href="concepts/data_lake.html#data-discovery">Data Discovery</a></li>
<li><a href="concepts/data_lake.html#data-auditing">Data Auditing</a></li>
<li><a href="concepts/data_lake.html#data-lineage">Data Lineage</a></li>
<li><a href="concepts/data_lake.html#data-exploration">Data Exploration</a></li>
</ul>
</li>
<li><a href="concepts/data_lake.html#other-architecture-approaches">Other Architecture Approaches</a></li>
</ul>
<h2 id="data-lake-vs-data-warehouse"><a class="header" href="#data-lake-vs-data-warehouse">Data Lake vs Data Warehouse</a></h2>
<blockquote>
<p>A Data Warehouse is a repository that exclusively keeps pre-processed data from a Data Lake and many databases.</p>
</blockquote>
<p>Data Warehouses store data in a hierarchical format using files and folders.
This is not the case with a Data Lake, as it has a flat architecture.
In a Data Lake, every data element is identified by a unique identifier and a set of metadata information.</p>
<h2 id="goals"><a class="header" href="#goals">Goals</a></h2>
<blockquote>
<p>Building and maintaining a Data Lake has five main goals: unifying the data, full query access, performance and scalability, progression, and costs.</p>
</blockquote>
<p><strong>Unification</strong>: Data Lake is a perfect solution to accumulate all the data from distinct data sources (ERP, CRM, logs, data partners data, internally generated data) in one place.
The Data Lake architecture makes it easier for companies to get a holistic view of data and generate insights from it.</p>
<p><strong>Full Query Access</strong>: storing data in Data Lakes allows full access to data that can be directly used by BI tools to pull data whenever needed.
The ELT process is a flexible, reliable, and fast way to load data into Data Lake and then use it with other tools.</p>
<p><strong>Performance and Scalability</strong>: Data Lake Architecture supports fast query processing.
It enables users to perform ad hoc analytical queries independent of the production environment.
Data Lake provides faster querying and makes it easier to scale up and down. The Data Lake offers business agility.</p>
<p><strong>Progression</strong>: getting data in one place is a necessary step before progressing to other stages because loading data from one source makes it easier to work with BI tools.
Data Lake helps you make data cleaner and error-free data that has less repetition.</p>
<p><strong>Costs</strong>: S3 repositories are a cost-efficient storage of large volumes of data.</p>
<h2 id="data-lake-architecture"><a class="header" href="#data-lake-architecture">Data Lake Architecture</a></h2>
<div style="position: relative; width: 100%; height: 0; padding-top: 50.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbUxswrb4&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbUxswrb4&#x2F;view?utm_content=DAFbUxswrb4&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Data Lake Architecture Framework.</p></a>
<blockquote>
<p>Data Lakes are often structured in zones or layers models. These models define in which processing degrees (raw, cleansed, aggregated) data are available in the data lake, and how they are governed (regarding access rights, data quality, and responsibilities).</p>
</blockquote>
<p>Zones are similar to the layers in data warehousing, but data may not move through all zones or even move back.</p>
<h3 id="layers"><a class="header" href="#layers">Layers</a></h3>
<div style="position: relative; width: 100%; height: 0; padding-top: 35.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbVHIphys&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbVHIphys&#x2F;view?utm_content=DAFbVHIphys&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Data Lake Layers.</p></a>
<p>The following data lake model approach is structured in layers:</p>
<ul>
<li>Ingestion Layer</li>
<li>Distillation Layer</li>
<li>Processing Layer</li>
<li>Insights Layer</li>
<li>Unified Operations Layer</li>
</ul>
<p>The <strong>Raw Data</strong> entering the Data Lake consists of the organization's internal data (Operational Systems), especially relational data from databases, as well as streaming and batch data from data partners.</p>
<p>In the other extreme, representing the data leaving the Data Lake, the <strong>Business Systems</strong>, consists of databases, the Data Warehouse, dashboards, reports, and external data connections.</p>
<p>The first three layers constitute the medallion architecture, which is a data design pattern used to logically organize data in a Data Lake, with the goal of incrementally and progressively improving the structure and quality of data as it flows through each layer of the architecture (from Bronze ⇒ Silver ⇒ Gold layer tables).
Medallion architectures are sometimes also referred to as "multi-hop" architectures.</p>
<p>You can see more details regarding this architecture approach in the <a href="https://www.researchgate.net/profile/Sidharth-S-Prakash/publication/343219651_Evolution_of_Data_Warehouses_to_Data_Lakes_for_Enterprise_Business_Intelligence/links/5f1d52ad92851cd5fa48958a/Evolution-of-Data-Warehouses-to-Data-Lakes-for-Enterprise-Business-Intelligence.pdf">article</a><sup><a name="to-footnote-1"><a href="concepts/data_lake.html#footnote-1">1</a></a></sup> "Evolution of Data Warehouses to Data Lakes for Enterprise Business Intelligence".</p>
<h4 id="ingestion-layer-bronze"><a class="header" href="#ingestion-layer-bronze">Ingestion Layer (Bronze)</a></h4>
<blockquote>
<p>The purpose of the Ingestion Layer of the Data Lake Architecture is to ingest raw data into the Data Lake. There is no data modification in this layer. This is where we land all the data from external source systems.</p>
</blockquote>
<p>The table structures in this layer correspond to the source system table structures "as-is," along with any additional metadata columns that capture the load date/time, process ID, etc.
The focus in this layer is quick Change Data Capture and the ability to provide a historical archive of source (cold storage), data lineage, auditability, and reprocessing if needed without rereading the data from the source system.</p>
<p>The layer can ingest raw data in real-time or in batches, which is in turn organized into a logical folder structure.
The Ingestion Layer can pull data from different external sources, like social media platforms.</p>
<h4 id="distillation-layer-silver"><a class="header" href="#distillation-layer-silver">Distillation Layer (Silver)</a></h4>
<blockquote>
<p>The purpose of the Distillation Layer of the Data Lake Architecture is to convert the data stored in the Ingestion (Bronze) Layer into a Structured format for analytics.</p>
</blockquote>
<p>The data is matched, denormalized, merged, conformed, cleansed, and derived "just enough" so that the Silver layer can provide an "Enterprise view" of all its key business entities, concepts, and transactions (for example, master customers, stores, non-duplicated transactions, and cross-reference tables).
The data in this layer becomes uniform in terms of format, encoding, and data type (parquet).</p>
<p>The Silver layer brings the data from different sources into an Enterprise view and enables self-service analytics for ad-hoc reporting, advanced analytics, and ML.
It serves as a source for Departmental Analysts, Data Engineers, and Data Scientists to further create projects and analyses to answer business problems via enterprise and departmental data projects in the Gold Layer.</p>
<p>In the lakehouse data engineering paradigm (of which we’re extending to the Data Lake), typically the ELT methodology is followed vs. ETL - which means only minimal or "just enough" transformations and data cleansing rules are applied while loading the Silver layer.
Speed and agility to ingest and deliver the data in the data lake are prioritized, and a lot of project-specific complex transformations and business rules are applied while loading the data from the Silver to Gold layer.
From a data modeling perspective, the Silver Layer has more 3rd-Normal Form-like data models. Data Vault-like, write-performant data models can be used in this layer.</p>
<h4 id="processing-layer-gold"><a class="header" href="#processing-layer-gold">Processing Layer (Gold)</a></h4>
<blockquote>
<p>This layer of the Data Lake Architecture executes user queries and advanced analytical tools on the Structured Data.</p>
</blockquote>
<p>The processes can be run in batch, in real-time, or interactively.
It is the layer that implements the business logic and analytical applications consume the data.
It is also known as the Trusted, Gold, or Production-Ready Layer.</p>
<p>It is typically organized in consumption-ready "project-specific" databases.
The Gold layer is for reporting and uses more de-normalized and read-optimized data models with fewer joins.
The final layer of data transformations and data quality rules are applied here.
The Final presentation layer of projects such as Customer Analytics, Product Quality Analytics, Inventory Analytics, Customer Segmentation, Product Recommendations, Marking/Sales Analytics, etc. fit in this layer.
We see a lot of Kimball's style star schema-based data models or Inmon's style Data marts fit.</p>
<p>Often, the Data Marts (and Data Warehouse data) from the traditional RDBMS technology stack are ingested into the Gold layer.</p>
<h4 id="insights-layer"><a class="header" href="#insights-layer">Insights Layer</a></h4>
<blockquote>
<p>This layer of the Data Lake Architecture acts as the query interface, or the output interface, of the Data Lake.</p>
</blockquote>
<p>It uses SQL and NoSQL queries to request or fetch data from the Data Lake.
The queries are normally executed by company users who need access to the data.
Once the data is fetched from the Data Lake, it is the same layer that displays it to the user for viewing.</p>
<p>Some examples include Amazon QuickSight, an AWS native BI tool and allows users to connect with software-as-a-service (SaaS) applications such as Salesforce or ServiceNow, third-party databases such as MySQL, Postgres, and SQL Server, as well as native AWS services including Amazon Athena, an interactive query service that allows them to analyze unstructured data in Amazon S3 data lakes using standard SQL queries.
While QuickSight doesn’t connect directly to the data lake, integration with Amazon Athena allows BI users to query data inside the lake without having to move data or build an ETL pipeline.</p>
<h4 id="unified-operations-layer"><a class="header" href="#unified-operations-layer">Unified Operations Layer</a></h4>
<blockquote>
<p>This layer governs system management and monitoring.</p>
</blockquote>
<p>It includes auditing and proficiency management, data management, workflow management. AWS data lake environments and monitoring tools and best practices are described in this <a href="https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/monitoring-optimizing-data-lake-environment.html">article</a><sup><a name="to-footnote-2"><a href="concepts/data_lake.html#footnote-2">2</a></a></sup> from AWS.</p>
<h3 id="zones"><a class="header" href="#zones">Zones</a></h3>
<div style="position: relative; width: 100%; height: 0; padding-top: 35.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbVKs6Or4&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbVKs6Or4&#x2F;view?utm_content=DAFbVKs6Or4&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Zone Reference Model for Enterprise-Grade Data Lake Management - Data Flow.</p></a>
<p>The following data lake approach is explored by the <a href="https://www.ipvs.uni-stuttgart.de/departments/as/publications/giebleca/20_zoneReferenceModel_EDOC_Preprint.pdf">University of Stuttgart and Bosch GmbH</a><sup><a name="to-footnote-3"><a href="concepts/data_lake.html#footnote-3">3</a></a></sup>, and is known as Zone Reference Model for Enterprise-Grade Data Lake Management. It consists of:</p>
<ul>
<li>Landing Zone</li>
<li>Raw Zone</li>
<li>Harmonized Zone</li>
<li>Distilled Zone</li>
<li>Delivery Zone</li>
<li>Explorative Zone</li>
</ul>
<p>PlantUML rendering error:
PlantUML did not generate an image, did you forget the @startuml, @enduml block (java -jar /usr/share/plantuml/plantuml.jar -tsvg -nometadata /tmp/.tmpTX3b6j/b510a51550cb2a65e78ebfbb460618d52bd15e92.puml)?</p>
<p style="text-align: center;">Zone Reference Model for Enterprise-Grade Data Lake Management - Meta-model for zones - Attributes.</p>
<p>Regarding how a zone interacts with other zones and the outside world, we have:</p>
<ul>
<li><strong>Zone</strong> <em>receives</em> data from another <strong>Zone</strong></li>
<li><strong>Zone</strong> <em>forwards</em> data to another <strong>Zone</strong></li>
<li><strong>Zone</strong> <em>imports</em> data from a <strong>Data Source</strong></li>
<li><strong>Zone</strong> <em>exports</em> data to the <strong>Data Sink</strong></li>
</ul>
<p>All zones contain a protected part.
This part is encrypted and secured, and stores data that need extensive protection (for example, PII, personal data). Data wander from the protected part of one zone to the protected part of the next zone.
They may only leave the protected part after being desensitized (for example, by anonymization).
Data in this part are subject to strict access controls and governance.
The protected part shares all other characteristics with the rest of the zone it is in.</p>
<h4 id="landing-zone"><a class="header" href="#landing-zone">Landing Zone</a></h4>
<blockquote>
<p>The Landing Zone is the first zone of the data lake. Data are ingested in batches or in data streams from the sources.</p>
</blockquote>
<p>The Landing Zone is beneficial when the requirements of the ingested data and those of the Raw Zone diverge.
For example, data might need to be ingested at a vast rate due to its volume and velocity.
If the technical implementation of the Raw Zone cannot provide this high ingestion rate, a Landing Zone can function as a mediator in between: data are ingested at a high rate into the Landing Zone, and then are forwarded to the Raw Zone as batches.
Examples of data coming to the Landing zone include data streamed by Kafka, Amazon Kinesis, Amazon SQS, RabbitMQ, Apache Spark, etc.</p>
<p>For the data characteristics, data ingested into the Landing Zone remains mostly raw.
Their granularity remains raw, just like in the source systems.
The schema of the data is not changed; they can simply be copied in their source system format.
However, their syntax might be changed.
Basic transformations are allowed upon ingestion into the Landing Zone, such as adjusting the character set of strings or transforming timestamps into a common format.
In addition, data may be masked or anonymized to comply with legal regulations.
Aside from these changes, the semantics of the data remains the same as in the source systems.</p>
<h4 id="raw-zone"><a class="header" href="#raw-zone">Raw Zone</a></h4>
<blockquote>
<p>All data in the data lake is available in mostly raw format in the Raw Zone. Only basic transformations (see Landing Zone) are applied to the data. If the Landing Zone is omitted, these transformations are performed in the Raw Zone.</p>
</blockquote>
<p>Differently from the Landing Zone, the Raw Zone stores data persistently.
In general, data should neither be manipulated nor deleted from the Raw Zone.
This zone persists (when possible) the original data type (JSON, CSV, XML).</p>
<h4 id="harmonized-zone"><a class="header" href="#harmonized-zone">Harmonized Zone</a></h4>
<blockquote>
<p>The Harmonized Zone is the place where master data are accessible for analysis.</p>
</blockquote>
<p>A subset of the data stored in the Raw Zone is passed to the Harmonized Zone in a demand-based manner.
It is important to note that these data are not deleted from the Raw Zone.
Instead, the Harmonized Zone contains a copy of or a view of the data in the Raw Zone.
The Harmonized Zone is also the place where master data are accessible for analysis.
As these data are crucial for enterprises, master data management is of high importance in the data lake.
Thus, they should exclusively be accessed after being cleansed.</p>
<p>The data characteristics in this zone differ greatly from those in the Raw Zone.
Data schema and syntax change when compared to the source data.
Data from different source systems are integrated into a consolidated schema, regardless of their structure.
The data syntax is also consolidated in the Harmonized Zone: when data from multiple source systems are merged, data types have to be adapted.</p>
<p>The Harmonized Zone aims to provide a harmonized and consolidated view of data.
To this end, the Harmonized Zone uses a standardized modeling approach (dimensional modeling or Data Vault) in which all of the enterprise’s data are modeled.
The files in this zone facilitate the ingestion (parquet).</p>
<h4 id="distilled-zone"><a class="header" href="#distilled-zone">Distilled Zone</a></h4>
<blockquote>
<p>Prepares data for processing and facilitates ingestion.</p>
</blockquote>
<p>In contrast to the Raw and Harmonized Zone, where the focus is to quickly make data available for use, the Distilled Zone focuses on increasing the efficiency of following analyses by preparing the data accordingly.
The granularity of the data may be changed (for example, data may be aggregated for the calculation of KPIs).
Complex processing is applied, changing the data’s semantics, but is too extensive for the Landing Zone, Raw Zone, and Harmonized Zone
However, the schema might also change slightly, depending on the supported use case (for example, fields to enrich the data could be added).
The files in this zone facilitate the ingestion (parquet).</p>
<h4 id="explorative-zone"><a class="header" href="#explorative-zone">Explorative Zone</a></h4>
<blockquote>
<p>The Explorative Zone is the place where data scientists can play with and flexibly use the data.</p>
</blockquote>
<p>Data scientists can use and explore data in the data lake in any way they desire, except for sensitive data. These data are only usable according to strict rules. Granularity, schema, syntax, and semantics may be changed in any way necessary for analyses.</p>
<h4 id="delivery-zone"><a class="header" href="#delivery-zone">Delivery Zone</a></h4>
<blockquote>
<p>In the Delivery Zone, small subsets of data are tailored to specific usage and applications.</p>
</blockquote>
<p>This does not only include analytical use cases, such as reporting and OLAP, but also operational use cases.
This zone thus provides functionality similar to data marts and operational data stores in data warehousing.
Data from this zone may be forwarded to external data sinks.</p>
<p>The Delivery Zone especially supports users with little knowledge on data analytics.
Data have to be easily findable and importable into various analytics tools.
As for the modeling approach, data are available in whatever format supports the intended use case best, for example, dimensional modeling for OLAP, or flat tables for operational use.</p>
<h4 id="zones-comparisom"><a class="header" href="#zones-comparisom">Zones Comparisom</a></h4>
<table>
    <tr>
        <td></td>
        <td><strong>Landing</strong></td>
        <td><strong>Raw</strong></td>
        <td><strong>Harmonized</strong></td>
        <td><strong>Distilled</strong></td>
        <td><strong>Explorative</strong></td>
        <td><strong>Delivery</strong></td>
    </tr>
    <tr>
        <td><strong>Granularity</strong></td>
        <td>Raw</td>
        <td>Raw</td>
        <td>Raw</td>
        <td>Aggregated</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
    <tr>
        <td><strong>Schema</strong></td>
        <td>Any</td>
        <td>Any</td>
        <td>Consolidated</td>
        <td>Consolidated, Enriched</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
    <tr>
        <td><strong>Syntax</strong></td>
        <td>Basic transformations</td>
        <td>Basic transformations</td>
        <td>Consolidated</td>
        <td>Consolidated</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
    <tr>
        <td><strong>Semantics</strong></td>
        <td>Mostly unchanged, unless needed for compliance</td>
        <td>Mostly unchanged, unless needed for compliance</td>
        <td>Mostly unchanged, unless needed for compliance</td>
        <td>Complex processing</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
    <tr>
        <td><strong>Governed</strong></td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>False</td>
        <td>True</td>
    </tr>
    <tr>
        <td><strong>Historized</strong></td>
        <td>False</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>N/A</td>
        <td>N/A</td>
    </tr>
    <tr>
        <td><strong>Persistent</strong></td>
        <td>False</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>False</td>
        <td>True</td>
    </tr>
    <tr>
        <td><strong>Has Protected Part</strong></td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
    </tr>
    <tr>
        <td><strong>Use Case Dependent</strong></td>
        <td>False</td>
        <td>False</td>
        <td>False</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
    </tr>
    <tr>
        <td><strong>User Groups</strong></td>
        <td>Systems, Processes</td>
        <td>Data Scientists, Systems, Processes</td>
        <td>Data Scientists, Systems, Processes</td>
        <td>Data Scientists, Domain Experts, Systems, Processes</td>
        <td>Data Scientists</td>
        <td>Anyone</td>
    </tr>
    <tr>
        <td><strong>Modelling Approach</strong></td>
        <td>Any</td>
        <td>Any</td>
        <td>Standardized</td>
        <td>Standardized</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
</table>
<p style="text-align: center;">Zone Reference Model for Enterprise-Grade Data Lake Management - Zones comparison.</p>
<h3 id="sandbox"><a class="header" href="#sandbox">Sandbox</a></h3>
<blockquote>
<p>Also known as the Analytics Sandbox, it provides data scientists and advanced analysts with a place for data exploration.</p>
</blockquote>
<p>An Analytics Sandbox is a separate environment that is part of the overall data lake architecture, meaning that it is a centralized environment meant to be used by multiple users and is maintained with the support of IT. Some key characteristics of this layer:</p>
<ul>
<li>The environment is controlled by the analyst</li>
<li>Allows them to install and use the data tools of their choice</li>
<li>Allows them to manage the scheduling and processing of the data assets</li>
<li>Enables analysts to explore and experiment with internal and external data</li>
<li>Can hold and process large amounts of data efficiently from many different data sources; big data (unstructured), transactional data (structured), web data, social media data, documents, etc.</li>
</ul>
<p>There are many advantages to having an Analytics Sandbox as part of your data architecture.
The most important is that it decreases the amount of time that it takes a business to gain knowledge and insight from their data.
It does this by providing an on-demand/always-ready environment that allows analysts to quickly dive into and process large amounts of data and prototype their solutions without kicking off a big BI project.
In other words, it enables agile BI by empowering your advanced users.</p>
<p>Another major benefit to the business and IT team is that by giving the business a place to prototype their data solutions it allows the business to figure out what they want on their own without involving IT.
When they decide that a solution is adding business value, it becomes a good candidate for something that should be productized and built into the Data Warehouse process at some point.
This saves both teams a lot of time and effort.</p>
<h3 id="maturity-stages"><a class="header" href="#maturity-stages">Maturity Stages</a></h3>
<p>The implementation of a Data Lake solution consists of some main maturity stages.</p>
<ol>
<li>Handle and ingest data at scale</li>
<li>Building the analytical muscle</li>
<li>Data Warehouse and Data Lake working in unison</li>
<li>Enterprise capability</li>
</ol>
<h4 id="handle-and-ingest-data-at-scale"><a class="header" href="#handle-and-ingest-data-at-scale">Handle and Ingest data at scale</a></h4>
<blockquote>
<p>This stage consists of improving the ability to transform and analyze data.</p>
</blockquote>
<h4 id="building-the-analytical-muscle"><a class="header" href="#building-the-analytical-muscle">Building the analytical muscle</a></h4>
<blockquote>
<p>This stage involves improving the ability to transform and analyze data.</p>
</blockquote>
<p>In this stage, the company starts acquiring more data and building applications.
In this stage, the capabilities of the Data Warehouse and the Data Lake are used together.</p>
<h4 id="data-warehouse-and-data-lake-work-in-unison"><a class="header" href="#data-warehouse-and-data-lake-work-in-unison">Data Warehouse and Data Lake work in unison</a></h4>
<blockquote>
<p>This step involves getting data and analytics into the hands of as many people as possible.</p>
</blockquote>
<p>In this stage, the Data Lake and the Data Warehouse start to work in a union.
Both lay their part in analytics.</p>
<h4 id="enterprise-capability-in-the-lake"><a class="header" href="#enterprise-capability-in-the-lake">Enterprise capability in the lake</a></h4>
<blockquote>
<p>In this maturity stage of the data lake, enterprise capabilities are added to the Data Lake.</p>
</blockquote>
<p>It includes the adoption of information governance, information lifecycle management capabilities, and Metadata management.</p>
<h2 id="key-components-of-data-lake-architecture"><a class="header" href="#key-components-of-data-lake-architecture">Key Components of Data Lake Architecture</a></h2>
<p><img src="concepts/../mdbook-plantuml-img/2b8772bdc69bce03cd960127332a78ef0a2193ea.svg" alt="" /></p>
<p style="text-align: center;">Key Components of Data Lake Architecture.</p>
<h4 id="data-ingestion"><a class="header" href="#data-ingestion">Data Ingestion</a></h4>
<blockquote>
<p>Data Ingestion allows connectors to get data from different data sources and load it into the Data Lake.</p>
</blockquote>
<p>Data Ingestion supports:</p>
<ul>
<li>All types of Structured, Semi-Structured, and Unstructured data.</li>
<li>Multiple ingestions like Batch, Real-Time, and One-time load.</li>
<li>Many types of data sources like Databases, Webservers, Emails, and FTP.</li>
</ul>
<h4 id="data-storage"><a class="header" href="#data-storage">Data Storage</a></h4>
<blockquote>
<p>Data storage should be scalable, offer cost-effective storage, and allow fast access to data exploration. It should support various data formats.</p>
</blockquote>
<h4 id="data-governance"><a class="header" href="#data-governance">Data Governance</a></h4>
<blockquote>
<p>Data governance is a process of managing the availability, usability, security, and integrity of data used in an organization.</p>
</blockquote>
<h4 id="security"><a class="header" href="#security">Security</a></h4>
<blockquote>
<p>Security needs to be implemented in every layer of the Data Lake. It starts with Storage, Unearthing, and Consumption. The basic need is to stop access for unauthorized users. It should support different tools to access data with easy-to-navigate GUI and Dashboards.</p>
</blockquote>
<p>Authentication, Accounting, Authorization, and Data Protection are some important features of Data Lake security.</p>
<h4 id="data-quality-1"><a class="header" href="#data-quality-1">Data Quality</a></h4>
<blockquote>
<p>Data quality is an essential component of Data Lake architecture. Data is used to exact business value. Extracting insights from poor-quality data will lead to poor-quality insights.</p>
</blockquote>
<h4 id="data-discovery"><a class="header" href="#data-discovery">Data Discovery</a></h4>
<blockquote>
<p>Data Discovery is another important stage before you can begin preparing data or analysis. In this stage, tagging technique is used to express the data understanding, by organizing and interpreting the data ingested in the Data Lake.</p>
</blockquote>
<h4 id="data-auditing"><a class="header" href="#data-auditing">Data Auditing</a></h4>
<blockquote>
<p>Data auditing helps to evaluate risk and compliance.</p>
</blockquote>
<p>The main Data auditing tasks are:</p>
<ul>
<li>Tracking changes to important dataset elements</li>
<li>Captures how/when/who changes to these elements.</li>
</ul>
<h4 id="data-lineage"><a class="header" href="#data-lineage">Data Lineage</a></h4>
<blockquote>
<p>This component deals with the data’s origins. It mainly deals with where it movers over time and what happens to it. It eases error corrections in a data analytics process from origin to destination.</p>
</blockquote>
<h4 id="data-exploration"><a class="header" href="#data-exploration">Data Exploration</a></h4>
<blockquote>
<p>It is the beginning stage of data analysis. It helps to identify the right dataset is vital before starting Data Exploration.</p>
</blockquote>
<p>All given components need to work together to play an important part in Data Lake building easily evolving and exploring the environment.</p>
<h2 id="other-architecture-approaches"><a class="header" href="#other-architecture-approaches">Other Architecture Approaches</a></h2>
<p><strong>Data Lake Lambda Architecture for Smart Grids Big Data Analytics</strong>: relies on Lambda architecture that is capable of performing parallel batch and real-time operations on distributed data. See the <a href="https://ieeexplore.ieee.org/abstract/document/8417407">article</a>. Also, see a brief explanation of the Lambda Architecture in this <a href="https://www.researchgate.net/profile/Ajit-Singh-46/publication/331890045_Architecture_of_Data_Lake/links/6061ef85458515e8347d6ecc/Architecture-of-Data-Lake.pdf">article</a>.</p>
<p>A. A. Munshi and Y. A. -R. I. Mohamed, "Data Lake Lambda Architecture for Smart Grids Big Data Analytics," in IEEE Access, vol. 6, pp. 40463-40471, 2018, doi: 10.1109/ACCESS.2018.2858256.</p>
<p>Ajit Singh, "Architecture of Data Lake", International Journal of Scientific Research in Computer Science, Engineering and Information Technology (IJSRCSEIT), ISSN : 2456-3307, Volume 5 Issue 2, pp. 411-414, March-April 2019. Available at doi: https://doi.org/10.32628/CSEIT1952121<p><hr/></p>
<p><a name="footnote-1"><a href="concepts/data_lake.html#to-footnote-1">1</a></a>: Prakash, S. S. (2020). Evolution of Data Warehouses to Data Lakes for Enterprise Business Intelligence. Evolution, 8(4).</p>
<p><a name="footnote-2"><a href="concepts/data_lake.html#to-footnote-2">2</a></a>: The article discuss data lake optimizations using AWS products, like CloudWatch, Macie, CloudTrail, and S3 Intelligent-Tiering.</p>
<p><a name="footnote-3"><a href="concepts/data_lake.html#to-footnote-3">3</a></a>: C. Giebler, C. Gröger, E. Hoos, H. Schwarz and B. Mitschang, "A Zone Reference Model for Enterprise-Grade Data Lake Management," 2020 IEEE 24th International Enterprise Distributed Object Computing Conference (EDOC), Eindhoven, Netherlands, 2020, pp. 57-66, doi: 10.1109/EDOC49727.2020.00017.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-warehouse"><a class="header" href="#data-warehouse">Data Warehouse</a></h1>
<blockquote>
<p>A <strong>Data Warehouse</strong> (DWH), also known as Enterprise Data Warehouse (EDW), is a central repository of information that can be analyzed to make more informed decisions.</p>
</blockquote>
<p>Data flows into a data warehouse from Data Lake, transactional systems, relational databases, and other sources, typically on a regular cadence. Business analysts, data engineers, data scientists, and decision-makers access the data through Business Intelligence (BI) tools, SQL clients, and other analytics applications.</p>
<ul>
<li><a href="concepts/data_warehouse.html#goals">Goals</a></li>
<li><a href="concepts/data_warehouse.html#data-warehouse-architecture">Data Warehouse Architecture</a>
<ul>
<li><a href="concepts/data_warehouse.html#data-sources">Data Sources</a></li>
<li><a href="concepts/data_warehouse.html#warehouse">Warehouse</a>
<ul>
<li><a href="concepts/data_warehouse.html#metadata">Metadata</a></li>
<li><a href="concepts/data_warehouse.html#summarized-data">Summarized Data</a></li>
</ul>
</li>
<li><a href="concepts/data_warehouse.html#end-user-access-tools">End-User Access Tools</a></li>
<li><a href="concepts/data_warehouse.html#two-tier-vs-three-tier-architecture">Two-Tier vs Three-Tier Architecture</a></li>
</ul>
</li>
<li><a href="concepts/data_warehouse.html#data-modeling-methodologies">Data Modeling Methodologies</a></li>
<li><a href="concepts/data_warehouse.html#maturity-stages">Maturity Stages</a></li>
<li><a href="concepts/data_warehouse.html#key-components-of-a-data-warehouse">Key Components of a Data Warehouse</a></li>
</ul>
<h2 id="goals-1"><a class="header" href="#goals-1">Goals</a></h2>
<blockquote>
<p>When implementing a data warehouse, the main goals are to achieve consistency, enable data-driven decision-making and improvement, and maintain data Single Source of Truth.</p>
</blockquote>
<p><strong>Consistency</strong>: to maintain a uniform format for all collected data, making it easier for corporate decision-makers to analyze and share data insights with their colleagues. Standardizing data from different sources also reduces the risk of error in interpretation and improves overall accuracy.</p>
<p><strong>Decision-making</strong>: successful business leaders develop data-driven strategies and rarely make decisions without consulting the facts. Data warehousing improves the speed and efficiency of accessing different data sets and makes it easier for corporate decision-makers to derive insights that will guide the business and marketing strategies that set them apart from their competitors.</p>
<p><strong>Improving</strong>: allow business leaders to quickly access the organization's historical activities and evaluate initiatives that have been successful — or unsuccessful — in the past. This allows executives to see where they can adjust their strategy to decrease costs, maximize efficiency, and increase business results.</p>
<p><strong>Single Source of Truth</strong>: the whole organization would benefit from having a single source of truth, especially when there are multiple data sources to a common business dimension.</p>
<h2 id="data-warehouse-architecture"><a class="header" href="#data-warehouse-architecture">Data Warehouse Architecture</a></h2>
<p><img src="concepts/../mdbook-plantuml-img/85627189967d2e5f9857b7b50dfac44b17456eb7.svg" alt="" /></p>
<p style="text-align: center;">Data Warehouse Solution.</p>
<p>There are several data warehouse architecture approaches available. Data warehouses would have in common some key components:</p>
<h3 id="data-sources"><a class="header" href="#data-sources">Data Sources</a></h3>
<blockquote>
<p>In most architecture approaches, it’s the Source Layer, or Data Source Layer, and consists of all the data sources the Warehouse Layer will consume.</p>
</blockquote>
<p><strong>Operational System</strong>: is a method used in data warehousing to refer to a system that is used to process the day-to-day transactions of an organization. Physically, it will normally refer to the databases the organization applications and micro-services create.</p>
<p><strong>Flat Files System</strong>: is a system of files in which transactional data is stored, and every file in the system must have a different name.</p>
<h3 id="warehouse"><a class="header" href="#warehouse">Warehouse</a></h3>
<blockquote>
<p>In most architecture approaches, it’s the Warehouse Layer, or Data Warehouse Layer, and consists of all the data stored in the RDBMS database with available gateway access (ODBC, JDBC, etc.). It also contains the metadata, some degree of data summarization, and business logic applied, which differentiate a <strong>DWH database</strong> from a <strong>Production database</strong>.</p>
</blockquote>
<h4 id="metadata"><a class="header" href="#metadata">Metadata</a></h4>
<blockquote>
<p>Metadata is the road map to a data warehouse, it defines the warehouse objects and acts as a directory. This directory helps the decision support system to locate the contents of a data warehouse.</p>
</blockquote>
<p>It normally contains:</p>
<ol>
<li>A description of the Data Warehouse structure, including the warehouse schema, dimensions, hierarchies, data mart locations, contents, etc.</li>
<li>Operational metadata, which usually describes the currency level of the stored data (for example, active, archived, or purged), and warehouse monitoring information (for example, usage statistics, error reports, audit, etc).</li>
<li>System performance data, which includes indices, is used to improve data access and retrieval performance.</li>
<li>Information about the mapping from operational databases, which provides source RDBMSs and their contents, cleaning and transformation rules, etc.</li>
<li>Summarization algorithms, predefined queries, and reports business data, which include business terms and definitions, ownership information, etc.</li>
</ol>
<p>Metadata management tool examples are Datahub, Open Metadata, and Amundsen.</p>
<h4 id="summarized-data"><a class="header" href="#summarized-data">Summarized Data</a></h4>
<blockquote>
<p>The summarized data is the area of the data warehouse that maintains all the predefined lightly and highly summarized (aggregated) data. The main goal is to speed up query performance, and the summarized records are updated continuously as new information is loaded into the warehouse.</p>
</blockquote>
<h3 id="end-user-access-tools"><a class="header" href="#end-user-access-tools">End-User Access Tools</a></h3>
<blockquote>
<p>The main purpose of a data warehouse is to provide information to the business for strategic decision-making. These end-users interact with the warehouse using end-client access tools.</p>
</blockquote>
<p>Examples of some of the end-user access tools can be:</p>
<ul>
<li>Reporting and Query Tools</li>
<li>Application Development Tools</li>
<li>Executive Information Systems Tools</li>
<li>Online Analytical Processing Tools</li>
<li>Data Mining Tools</li>
</ul>
<h3 id="two-tier-vs-three-tier-architecture"><a class="header" href="#two-tier-vs-three-tier-architecture">Two-Tier vs Three-Tier Architecture</a></h3>
<p>The data warehouse will normally be designed in a Two-Tier or a Three-Tier architecture approach. The details of which one will be explored in the chapter <a href="concepts/./data_warehouse_tier_architecture.html">Data Warehouse Tier Architecture</a>.</p>
<p>In short, the tiers are:</p>
<ol>
<li>Bottom/Data Tier: data warehouse server with functional gateway (ODBC, JDBC, etc.).</li>
<li>Middle/Application Tier: houses the business logic used to process user inputs. Examples: OLAP Servers, Snowflake, Apache Redshift, Databricks Data Lakehouse Platform, Apache Spark.</li>
<li>Top/Presentation Tier: front-end tools.</li>
</ol>
<h2 id="data-modeling-methodologies"><a class="header" href="#data-modeling-methodologies">Data Modeling Methodologies</a></h2>
<blockquote>
<p>Being one of the most important topics of data warehouse design and architecture, the data modeling methodology choosing process is arduous and polemic and will impact the whole design and implementation of the data warehouse solution.</p>
</blockquote>
<p>Especially for startups, the first versions or iterations of a data solution (implemented before the organization even starts discussing the implementation of a data warehouse solution) will be very similar to a <strong>Kimball</strong> (or Bottom-up) methodology approach, though not planned explicitly as such. This means the data marts (or the data to be accessed by the first BI tools adopted in the organization) are first formed based on the business requirements.</p>
<p>There are lots of advantages and disadvantages of priming this approach over a top-down approach (or any of the hybrid or alternative methodologies).</p>
<p>See all the details of the different data modeling methodologies in the chapter <a href="concepts/./data_modelling.html">Data Modeling</a>. See also the implementation of a three-tier data warehouse architecture in the <a href="https://iopscience.iop.org/article/10.1088/1757-899X/306/1/012061/pdf">paper</a>{{Tangkawarow, I. R. H. T., Runtuwene, J. P. A., Sangkop, F. I., &amp; Ngantung, L. V. F. (2018, February). Three Tier-Level Architecture Data Warehouse Design of Civil Servant Data in Minahasa Regency. In IOP Conference Series: Materials Science and Engineering (Vol. 306, No. 1, p. 012061). IOP Publishing.}} "Three Tier-Level Architecture Data Warehouse Design of Civil Servant Data in Minahasa Regency".</p>
<h2 id="maturity-stages-1"><a class="header" href="#maturity-stages-1">Maturity Stages</a></h2>
<p>#TODO</p>
<h2 id="key-components-of-a-data-warehouse"><a class="header" href="#key-components-of-a-data-warehouse">Key Components of a Data Warehouse</a></h2>
<p><strong>Data Ingestion</strong>: Allows connectors to get data from different data sources and load it into the Data Warehouse. The data will normally come from the Data Lake and External Sources connection (Fivetran), through multiple ETLs (Airflow, services, apps, ETL tools and platforms, etc.).</p>
<p><strong>Data Storage</strong>: The data is stored in the data warehouse database, a relational database (RDBMS), like Postgres.</p>
<p><strong>Data Governance</strong>: This is a process of managing the availability, usability, security, and integrity of data used in an organization.</p>
<p><strong>Security</strong>: It needs to be implemented in every layer of the Data Warehouse. It includes setting up the data warehouse read-only by default and setting up custom User Groups. It also includes access to the databases (VPCs, VPNs, Whitelisting, etc.), strong and active DevOps monitoring, and the enforcing of best practices in all levels of the data warehouse environment (data ingestion, data marts consumption, ETLs design, etc.).</p>
<p><strong>Data Quality</strong>: It is an essential component of Data Warehouse architecture. Data is used to exact business value. Extracting insights from poor-quality data will lead to poor-quality insights.</p>
<p><strong>Data Discovery</strong>: This is another important stage before you can begin preparing data or analysis. All this relies on good metadata and data modeling.</p>
<p><strong>Data Auditing</strong>: It helps to evaluate risk and compliance. Two major Data auditing tasks are tracking changes to the key dataset.</p>
<ul>
<li>Tracking changes to important dataset elements.</li>
<li>Captures how/when/who changes to these elements.</li>
</ul>
<p><strong>Data Lineage</strong>: It deals with data’s origins. It mainly deals with where it movers over time and what happens to it. It eases error corrections in a data analytics process from origin to destination. Some data modeling techniques may facilitate lineage in comparison to others (Vault vs Kimball vs Inmon).</p>
<p><strong>Data Exploration</strong>: This is the beginning stage of data analysis. It helps to identify the right dataset, vital before starting Data Exploration. All given components need to work together to play an important part in Data Warehouse building to easily evolve and explore the environment.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tier-architecture"><a class="header" href="#tier-architecture">Tier Architecture</a></h1>
<h2 id="two-tier-architecture"><a class="header" href="#two-tier-architecture">Two-Tier Architecture</a></h2>
<p><img src="concepts/../mdbook-plantuml-img/34e8845308d5034a348498bba9809daa207f0ca8.svg" alt="" /></p>
<p style="text-align: center;">Two-Tier Data Warehouse Architecture.</p>
<p><strong>Data Source Layer</strong>: A data warehouse system uses a heterogeneous source of data. That data is stored initially in the organization's relational databases or legacy databases, or it may come from an information system outside the organization's walls.</p>
<p><strong>Data Staging Layer</strong>: The data stored in the source should be extracted, cleansed to remove inconsistencies and fill gaps, and integrated to merge heterogeneous sources into one standard schema. The ETLs can combine heterogeneous schemata, extract, transform, cleanse, validate, filter, and load source data into a data warehouse. Note that this can be achieved in two ways:</p>
<ol>
<li>Having the Distillation Layer (Silver) in the Data Lake as the Data Staging Layer.</li>
<li>Creating a separate database within the Data Warehouse, or a separate database schema. Following the principle that all the data in the data warehouse should be cleaned and have high-quality standards, a separate database should be preferred.</li>
</ol>
<p><strong>Data Warehouse Layer</strong>: The information is saved to one logically centralized individual repository: a data warehouse. The data warehouse can be directly accessed, but it can also be used as a source for creating data marts, which partially replicate data warehouse contents and are designed for specific enterprise departments. Metadata repositories store information on sources, access procedures, data staging, users, data mart schema, and so on.</p>
<p><strong>Analysis Layer</strong>: In this layer, integrated data is efficiently, and flexibly accessed to issue reports, dynamically analyze information, and simulate hypothetical business scenarios. It should feature aggregated information navigators, complex query optimizers, and customer-friendly GUIs.</p>
<h2 id="three-tier-architecture"><a class="header" href="#three-tier-architecture">Three-Tier Architecture</a></h2>
<p><img src="concepts/../mdbook-plantuml-img/c23cf858fede086a583ebceb59b95433ed0e7a9a.svg" alt="" /></p>
<p style="text-align: center;">Three-Tier Data Warehouse Architecture.</p>
<p>As the name of the architecture suggests, it consists of three tiers (levels):</p>
<ol>
<li>Bottom/Data Tier: data warehouse server with functional gateway (ODBC, JDBC, etc.).</li>
<li>Middle/Application Tier: houses the business logic used to process user inputs. Examples: OLAP Servers, Snowflake, Apache Redshift, Databricks Data Lakehouse Platform, Apache Spark.</li>
<li>Top/Presentation Tier: front-end tools.</li>
</ol>
<p>The Bottom and Top tiers were already discussed in detail in the previous section, so we only have left to cover, the implementation of the Middle Tier, very important to enable fast querying of the data warehouse. It is important to note the solutions to the middle tier are often referred to as the Data Warehouse itself, but they’re only the Application tier/level in a complete data warehouse solution (It’s like saying Snowflake is the DWH when it’s just one part of the complete DWH solution).</p>
<h3 id="reconciled-layer"><a class="header" href="#reconciled-layer">Reconciled Layer</a></h3>
<p>The Reconciled Layer sits between the source data and the data warehouse. The main advantage of the reconciled layer is that it creates a standard reference data model for the whole company. At the same time, it separates the problems of source data extraction and integration from those of the data warehouse population. In some cases, the reconciled layer is also directly used to accomplish better operational tasks, such as producing daily reports that cannot be satisfactorily prepared using the corporate applications or generating data flows to feed external processes periodically to benefit from cleaning and integration.</p>
<p>This architecture is especially useful for extensive, enterprise-wide systems. A disadvantage of this structure is the extra file storage space used through the redundant reconciled layer. It also makes the analytical tools a little further away from being real-time.</p>
<p>Please note that a reconciled layer could be part of the Data Warehouse, or the Data Lake (see Harmonized Zone, in the Data Lake concepts chapter).</p>
<h3 id="middleapplication-tier"><a class="header" href="#middleapplication-tier">Middle/Application Tier</a></h3>
<blockquote>
<p>Houses the business logic used to process user inputs. Example: OLAP Servers, Snowflake, Apache Redshift, Databricks Data Lakehouse Platform.</p>
</blockquote>
<p>See details in the chapter <a href="concepts/./data_warehouse_application_tier.html">Data Warehouse Middle/Application Tier</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-warehouse-middleapplication-tier"><a class="header" href="#data-warehouse-middleapplication-tier">Data Warehouse Middle/Application Tier</a></h1>
<blockquote>
<p>Houses the business logic used to process user inputs.</p>
</blockquote>
<p>One could argue that OLAP is <a href="https://www.kdnuggets.com/2022/10/olap-dead.html">dead</a>, at least in the traditional format (solutions like Mondrian), and cloud/modern solutions should be applied to accompany business fast demand for data.</p>
<p>Given this book aims to explore business scenarios mostly common to startups, in this sense, given cost restrictions, solutions like Apache Kylin seem to be a better approach (details below). When costs are less restricted, and/or data demands increase, other solutions like AWS Redshift, Snowflake, and Databricks Lakehouse are preferred/recommended.</p>
<h2 id="snowflake"><a class="header" href="#snowflake">Snowflake</a></h2>
<h2 id="aws-redshift"><a class="header" href="#aws-redshift">AWS Redshift</a></h2>
<p>#TODO</p>
<h2 id="snowflake-vs-aws-redshift"><a class="header" href="#snowflake-vs-aws-redshift">Snowflake vs AWS Redshift</a></h2>
<h2 id="databricks-lakehouse-platform"><a class="header" href="#databricks-lakehouse-platform">Databricks Lakehouse Platform</a></h2>
<blockquote>
<p>It combines the ACID transactions and data governance of data warehouses with the flexibility and cost-efficiency of data lakes to enable business intelligence (BI) and machine learning (ML) on all data.</p>
</blockquote>
<h3 id="data-lakehouse-vs-data-warehouse-vs-data-lake"><a class="header" href="#data-lakehouse-vs-data-warehouse-vs-data-lake">Data Lakehouse vs. Data Warehouse vs Data Lake</a></h3>
<p>Data in the data warehouse is easy to use, but harder to store. The opposite is true for the data lake: it’s easy to ingest and store data, but a pain to consume and query.</p>
<p>The data lakehouse has a layer design, with a warehouse layer on top of a data lake. This architecture, which enables combining structured and unstructured data, makes it efficient for business intelligence and business analysis. Data lakehouses provide structured storage for some types of data and unstructured storage for others while keeping all data in one place.</p>
<h2 id="olap-servers"><a class="header" href="#olap-servers">OLAP Servers</a></h2>
<h3 id="relational-olap-servers-rolap"><a class="header" href="#relational-olap-servers-rolap">Relational OLAP Servers (ROLAP)</a></h3>
<blockquote>
<p>They use a relational or extended-relational DBMS to save and handle warehouse data, and OLAP middleware to provide missing pieces. They work primarily from the data that resides in a relational database, where the base data and dimension tables are stored as relational tables. This model permits the multidimensional analysis of data.</p>
</blockquote>
<p>This technique relies on manipulating the data stored in the relational database to give the presence of traditional OLAP's slicing and dicing functionality.</p>
<h4 id="advantages"><a class="header" href="#advantages">Advantages</a></h4>
<p><strong>Can handle large amounts of information</strong>: the data size limitation of ROLAP technology depends on the data size of the underlying RDBMS. So, ROLAP itself does not restrict the data amount.</p>
<p>RDBMS already comes with a lot of features. So ROLAP technologies, (works on top of the RDBMS) can control these functionalities.</p>
<h4 id="disadvantages"><a class="header" href="#disadvantages">Disadvantages</a></h4>
<p><strong>Performance can be slow</strong>: each ROLAP report is a SQL query (or multiple SQL queries) in the relational database. Query times can be prolonged if the underlying data size is large.</p>
<p><strong>Limited by SQL functionalities</strong>: ROLAP technology relies on developing SQL statements to query the relational database, and SQL statements do not suit all needs.</p>
<h3 id="multidimensional-olap-servers-molap"><a class="header" href="#multidimensional-olap-servers-molap">Multidimensional OLAP Servers (MOLAP)</a></h3>
<blockquote>
<p>It is based on a native logical model that directly supports multidimensional data and operations. Data are stored physically in multidimensional arrays, and positional techniques are used to access them.</p>
</blockquote>
<p>One of the significant distinctions between MOLAP and ROLAP is that data are summarized and stored in an optimized format in a multidimensional cube, instead of in a relational database. In the MOLAP model, data are structured into proprietary formats by the client's reporting requirements with the calculations pre-generated on the cubes.</p>
<h4 id="advantages-1"><a class="header" href="#advantages-1">Advantages</a></h4>
<p><strong>Excellent Performance</strong>: a MOLAP cube is built for fast information retrieval, and is optimal for slicing and dicing operations.</p>
<p><strong>Can perform complex calculations</strong>: all evaluations have been pre-generated when the cube is created. Hence, complex calculations are not only possible, but they return quickly.</p>
<h4 id="disadvantages-1"><a class="header" href="#disadvantages-1">Disadvantages</a></h4>
<p><strong>Limited in the amount of information it can handle</strong>: Because all calculations are performed when the cube is built, it is not possible to contain a large amount of data in the cube itself.</p>
<h3 id="hybrid-olap-servers-holap"><a class="header" href="#hybrid-olap-servers-holap">Hybrid OLAP Servers (HOLAP)</a></h3>
<blockquote>
<p>It incorporates the best features of MOLAP and ROLAP into a single architecture. It saves more substantial quantities of detailed data in the relational tables while the aggregations are stored in the pre-calculated cubes. HOLAP also can drill through from the cube down to the relational tables for delineated data.</p>
</blockquote>
<h4 id="advantages-2"><a class="header" href="#advantages-2">Advantages</a></h4>
<ul>
<li>It provides the benefits of both MOLAP and ROLAP.</li>
<li>It provides fast access at all levels of aggregation.</li>
<li>It balances the disk space requirement, as it only stores the aggregate information on the OLAP server and the detail record remains in the relational database. So no duplicate copy of the detail record is maintained.</li>
</ul>
<h4 id="disadvantages-2"><a class="header" href="#disadvantages-2">Disadvantages</a></h4>
<p>HOLAP architecture is very complicated because it supports both MOLAP and ROLAP servers.</p>
<h3 id="olap-servers-options"><a class="header" href="#olap-servers-options">OLAP Servers options</a></h3>
<h4 id="apache-kylin"><a class="header" href="#apache-kylin">Apache Kylin</a></h4>
<p>Only supports MOLAP and Offline data storage modes. It supports both SQL and MDX queries, has RESTful API capabilities (also ODBC, and JDBC), and can be integrated/connected with Tableau (also Redash, Superset, Zeppelin, Qlik, and Excel).</p>
<p>It supports Real-time processing, partitioning, usage-based optimizations, load balancing, and clustering. It supports LDAP, SAML, and Kerberos authentication.</p>
<h4 id="mondrian-olap-server"><a class="header" href="#mondrian-olap-server">Mondrian OLAP Server</a></h4>
<p>Only supports ROLAP data storage modes. It supports MDX queries but not SQL and has REST API capabilities. Does not natively connect with Tableau, but queries can be performed via Java APIs. It supports real-time processing and partitioning.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-modelling"><a class="header" href="#data-modelling">Data Modelling</a></h1>
<h2 id="kimballbottom-up"><a class="header" href="#kimballbottom-up">Kimball/Bottom-Up</a></h2>
<blockquote>
<p>The design of the Data Marts comes from the business requirements.</p>
</blockquote>
<p>The primary data sources are then evaluated, and ETL tools are used to fetch data from several sources and load it into a staging area of the relational database server.
Once data is uploaded in the data warehouse staging area, the next phase includes loading data into a dimensional data warehouse model that is denormalized by nature.
This model partitions data into the fact and dimension tables.
Kimball dimensional modeling allows users to construct several star schemas to fulfill various reporting needs.</p>
<div style="position: relative; width: 100%; height: 0; padding-top: 50.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbfV1mwLM&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbfV1mwLM&#x2F;view?utm_content=DAFbfV1mwLM&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Kimball Approach to Data Warehouse Lifecycle.</p></a>
<h3 id="advantages-3"><a class="header" href="#advantages-3">Advantages</a></h3>
<p><strong>Fast to construct (quick initial phase)</strong>: it is fast to construct as no normalization is involved, which means swift execution of the initial phase of the data warehousing design process.</p>
<p><strong>Simplified queries</strong>: in a star schema, most data operators can easily comprehend it because of its denormalized structure, which simplifies querying and analysis.</p>
<p><strong>Simplified business management</strong>: the data warehouse system footprint is trivial because it focuses on individual business areas and processes rather than the whole company. So, it takes less space in the database, simplifying system management.</p>
<p><strong>Fast data retrieval</strong>: as data is segregated into fact tables and dimensions.</p>
<p><strong>Smaller teams</strong>: a smaller team of designers and planners is sufficient for data warehouse management because data source systems are stable, and the data warehouse is process-oriented. Also, query optimization is straightforward, predictable, and controllable.</p>
<p><strong>Deeper insights</strong>: it allows business intelligence tools to deeper across several star schemas and generates reliable insights.</p>
<h3 id="disadvantages-3"><a class="header" href="#disadvantages-3">Disadvantages</a></h3>
<p><strong>No Single Source of Truth</strong>: Data isn’t entirely integrated before reporting, so the idea of a single source of truth is lost.</p>
<p><strong>Too prone to data irregularities</strong>: this is because, in the denormalization technique, redundant data is added to database tables.</p>
<p><strong>Too difficult and expensive to add new columns</strong>: performance issues may occur due to the addition of columns in the fact table, as these tables are quite in-depth. The addition of new columns can expand the fact table dimensions, affecting its performance.</p>
<p><strong>Can't respond (well) to business changes</strong>: it is too difficult to alter the models.</p>
<p><strong>Not BI-friendly</strong>: as it is business process-oriented, instead of focusing on the company as a whole, it cannot handle all the BI reporting requirements.</p>
<p><strong>Inconsistent dimensional view</strong>: this model is not as strong as the top-down approach as the dimensional view of data marts is not as consistent as it is in the Inmon approach.</p>
<p>In brief, the Kimball approach has a <strong>low</strong> start-up cost**, is <strong>faster to deliver</strong> the first phase of the data warehouse design, and is faster to release to production (first version), but is suitable for <strong>Tactical</strong> business decision support requirements (versus Strategic), and <strong>addresses individual business requirements</strong> (vs Enterprise-wide). Another important topic that derives from this methodology approach is the <strong>Data Warehouse Bus Architecture</strong>.</p>
<h2 id="inmontop-down"><a class="header" href="#inmontop-down">Inmon/Top-Down</a></h2>
<blockquote>
<p>Subject-oriented, nonvolatile, integrated, time-variant collection of data in support of management’s decisions.</p>
</blockquote>
<p>On the other hand, Bill Inmon, the father of data warehousing, came up with the concept of developing a data warehouse that identifies the main subject areas and entities the enterprise works with, such as customers, products, vendors, etc. Inmon’s definition of a data warehouse is that it is a “subject-oriented, nonvolatile, integrated, time-variant collection of data in support of management’s decisions”.</p>
<p>The model then creates a <strong>thorough, logical model for every primary entity</strong> (Business Entities). For instance, a logical model is constructed for products with all the attributes associated with that entity. This logical model could include many entities, including all the details, aspects, relationships, dependencies, and affiliations.</p>
<p>The Inmon design approach uses the <strong>normalized form</strong> for building <strong>entity structure</strong>, avoiding data redundancy as much as possible. This results in clearly identifying business requirements and <strong>preventing any data update irregularities</strong>. Moreover, the advantage of this top-down approach in database design is that it is <strong>robust to business changes</strong> and contains a dimensional perspective of data across data mart.</p>
<p>Next, the physical model is constructed, which follows the normalized structure. This Inmon model creates a <strong>Single Source of Truth</strong> for the whole business to consume. Data loading becomes less complex due to the normalized structure of the model. However, using this arrangement for querying is challenging as it includes numerous tables and links.</p>
<p>This Inmon data warehouse methodology proposes constructing data marts separately for each division, such as finance, marketing sales, etc. All the data entering the data warehouse is integrated. The data warehouse acts as a single data source for various data marts to ensure integrity and consistency across the enterprise.</p>
<h3 id="advantages-4"><a class="header" href="#advantages-4">Advantages</a></h3>
<p><strong>Single Source of Truth</strong>: the data warehouse acts as a unified source of truth for the entire business, where all data is integrated.</p>
<p><strong>Very low data redundancy</strong>: there’s less possibility of data update irregularities, making the data warehouse ETL processes more straightforward and less susceptible to failure.</p>
<p><strong>Great flexibility</strong>: it’s easier to update the data warehouse in case there’s any change in the business requirements or source data.</p>
<p><strong>BI-friendly</strong>: It can handle diverse company-wide reporting requirements.</p>
<h3 id="disadvantages-4"><a class="header" href="#disadvantages-4">Disadvantages</a></h3>
<p><strong>Increasing complexity</strong>: it increases as multiple tables are added to the data model with time.</p>
<p><strong>Skilled Human Resources</strong>: resources skilled in data warehouse data modeling are required, which can be expensive and challenging to find.</p>
<p><strong>Slow setup</strong>: the preliminary setup and delivery are time-consuming.</p>
<p><strong>Expert management</strong>: this approach requires experts to manage a data warehouse effectively.</p>
<p>In brief, the Inmon has a <strong>high</strong> start-up cost**, requires <strong>more time to be in production</strong> and meet business needs (very large projects with a very broad scope), and <strong>requires a bigger team of specialists</strong>, but is more <strong>suitable for systems and</strong> business changes**, better <strong>integrates with the whole organization</strong>, favors <strong>Strategic business decision support requirements</strong> (vs Tactical), and <strong>facilitates Business Intelligence development</strong>.</p>
<h2 id="hybrid"><a class="header" href="#hybrid">Hybrid</a></h2>
<blockquote>
<p>In a hybrid model, the data warehouse is built using the Inmon model, and on top of the integrated data warehouse, the business process-oriented data marts are built using the star schema for reporting.</p>
</blockquote>
<p>The hybrid approach provides a <strong>Single Source of Truth</strong> for the data marts, creating highly flexible solutions from a BI point of view.</p>
<p>Based on the <a href="https://www.researchgate.net/publication/261302233_The_Customer-Centric_Data_Warehouse_An_Architectural_Approach_to_Meet_the_Challenges_of_Customer_Orientation">Hub and Spoke Architecture</a>, the hybrid design methodology can also make use of <a href="https://en.wikipedia.org/wiki/Operational_data_store">Operational Data Stores</a> (ODS), integrating and cleaning data from multiple data sources. The information is then parsed into the actual Data Warehouse.</p>
<p>Hybrid methods will normally keep the data in the 3rd normal form, reducing redundancy. Although a normal relational database is not efficient for BI reports. Data marts for specific reports can then be built on top of the data warehouse solution.</p>
<p>When the data is denormalized, all the data available is pulled (as advocated by Inmon) while using a denormalized design (as advocated by Kimball). One example is the <a href="https://web.archive.org/web/20230921221322/https://resilientbiz.com/the-resilient-hybrid-methodology-data-warehouse/">Carry Forward</a> method.</p>
<p>Another hybrid methodology is the Data Vault, discussed below.</p>
<div style="position: relative; width: 100%; height: 0; padding-top: 50.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbfihtfys&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbfihtfys&#x2F;view?utm_content=DAFbfihtfys&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Example of a Hybrid Methodology approach.</p></a>
<h2 id="vault"><a class="header" href="#vault">Vault</a></h2>
<p>The <strong>Vault Data Modelling</strong> is a hybrid design, consisting of the best-of-breed practices from both <strong>3rd normal form</strong> and <strong>star schema</strong>.</p>
<p>It is not a true 3rd normal form and breaks some of the rules that 3NF dictates. It is a top-down architecture with a bottom-up design, geared to be strictly a data warehouse. It is not geared to be end-user accessible, which when built, still requires the user of a data mart or star-schema-based release for business purposes.</p>
<p>Data Vault data modeling breaks data into a small number of standard components – the most common of which are **Hubs, <strong>Links</strong>, and <strong>Satellites</strong>.</p>
<p><strong>Hubs</strong> are entities of interest to the business. They contain just a distinct list of business keys and metadata about when each key was first loaded and from where.</p>
<p><strong>Links</strong> connect Hubs and may record a transaction, composition, or other type of relationship between hubs. They contain details of the hubs involved (as foreign keys) and metadata about when the link was first loaded and from where.</p>
<p><strong>Satellites</strong> connect to Hubs or Links. They are Point in Time: so we can ask and answer the question, "What did we know when?". Satellites contain data about their parent Hub or Link and metadata about when the data was loaded, from where, and a business effectivity date.</p>
<p>The data model of the data warehouse is constructed using these components. These are:</p>
<p><strong>Standard</strong>: each component is always constructed the same way.</p>
<p><strong>Simple</strong>: easy to understand, and with a little practice, easy to apply to model your system.</p>
<p><strong>Connected</strong>: hubs only connect to links and satellites, links only connect to hubs and satellites, and satellites only connect to hubs or links.</p>
<p>Data Vault has staging, vault, and mart layers. Star schemas live in the mart layer, each star schema exposes a subset of the vault for a particular group of users.  Typically, hubs and their satellites form dimensions, and links and their satellites form facts.</p>
<p>A Data Vault complements the Data Lake and is a solution for organizations that need to integrate and add structure to the data held in the Data Lake.</p>
<div style="position: relative; width: 100%; height: 0; padding-top: 50.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbf7fKp9s&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbf7fKp9s&#x2F;view?utm_content=DAFbf7fKp9s&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Example of Data Vault 2.0 Modelling Methodology approach.</p></a>
<h2 id="bus-architecture"><a class="header" href="#bus-architecture">Bus Architecture</a></h2>
<blockquote>
<p>A bus architecture is composed of a set of tightly integrated data marts that get their power from conformed dimensions and fact tables. A conformed dimension is defined and implemented one time so that it means the same thing everywhere it's used.</p>
</blockquote>
<p>A dimension table is the "lookup" table of a dimensional model.
It contains textual data that decodes an identifier in associated fact tables.
A conformed dimension is defined and implemented one time and used throughout the multiple-star schemas that make up the enterprise data mart.
Dimensions define the who, what, where, when, why, and how of a situation, and are laid out for the benefit of business users.</p>
<blockquote>
<p>To conform to a dimension, every stakeholder must agree on a common definition for the dimension, so that the dimension means the same thing no matter where it’s used.</p>
</blockquote>
<p>#TODO: continue from https://www.itprotoday.com/sql-server/data-warehouse-bus-architecture</p>
<p>#TODO. Inflow, Upflow, Downflow, Outflow, and Meta flow.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="slowly-changing-dimensions-scd"><a class="header" href="#slowly-changing-dimensions-scd">Slowly Changing Dimensions (SCD)</a></h1>
<p>Slowly Changing Dimensions (SCDs) are concepts in data warehousing used to manage and track changes in dimension data over time. Dimensions in data warehousing refer to descriptive attributes related to business entities, such as products, customers, or geographical locations, which can change over time. Managing these changes accurately is crucial for historical reporting, trend analysis, and decision-making.</p>
<h2 id="type-0-fixed-dimension"><a class="header" href="#type-0-fixed-dimension">Type 0: Fixed Dimension</a></h2>
<blockquote>
<p>No changes are allowed. The dimension data is static, and any updates to the source data are ignored.</p>
</blockquote>
<p>Suitable for data that doesn't change, such as historical data, fixed identifiers, or regulatory codes.</p>
<h2 id="type-1-overwrite"><a class="header" href="#type-1-overwrite">Type 1: Overwrite</a></h2>
<blockquote>
<p>Updates overwrite existing records, with no history of previous values being kept. This approach is simple but sacrifices historical accuracy.</p>
</blockquote>
<p>Appropriate when historical data isn't necessary for analysis, or for correcting minor errors in dimension attributes.</p>
<h2 id="type-2-add-new-row"><a class="header" href="#type-2-add-new-row">Type 2: Add New Row</a></h2>
<blockquote>
<p>This approach involves adding a new row with the updated values while retaining the old rows to preserve history. Typically, attributes like "valid from," "valid to," and "current indicator" are used to manage the versioning of records.</p>
</blockquote>
<p>Essential for detailed historical tracking where it's important to know the state of the dimension at any point in time, such as tracking address changes for a customer.</p>
<h2 id="type-3-add-new-attribute"><a class="header" href="#type-3-add-new-attribute">Type 3: Add New Attribute</a></h2>
<blockquote>
<p>Involves adding new attributes to store the current and previous values of the changed dimension. It's limited in historical tracking as it usually only keeps the last change.</p>
</blockquote>
<p>Useful when only the most recent historical data is needed, such as tracking the previous and current manager of an employee.</p>
<h2 id="type-4-history-table"><a class="header" href="#type-4-history-table">Type 4: History Table</a></h2>
<blockquote>
<p>Separates the current data from historical data by maintaining a current table (similar to Type 1) and a separate history table (similar to Type 2) to track changes over time.</p>
</blockquote>
<p>Beneficial for performance optimization, as it keeps the main dimension table smaller and more efficient for queries, while still allowing historical analysis.</p>
<h2 id="hybrid-combination-of-types"><a class="header" href="#hybrid-combination-of-types">Hybrid: Combination of Types</a></h2>
<blockquote>
<p>Combines features from different types to suit specific needs. A common hybrid approach is using Type 2 with a current indicator flag or combining Type 2 for historical tracking with Type 1 attributes for frequently changing attributes where history isn't needed.</p>
</blockquote>
<p>A good fit for complex scenarios where different attributes of the dimension require different types of change management. For example, storing a complete history of address changes (Type 2) while only keeping the current phone number (Type 1).</p>
<h2 id="considerations"><a class="header" href="#considerations">Considerations</a></h2>
<ul>
<li>
<p><strong>Data Volume</strong>:
SCD Type 2 and Type 4 can significantly increase data volume due to the historical records they generate.</p>
</li>
<li>
<p><strong>Query Complexity</strong>:
SCD Type 2 and hybrids can introduce complexity into queries, as they require filtering for current or specific historical records.</p>
</li>
<li>
<p><strong>Performance</strong>:
Type 1 and Type 0 are generally more performant for queries due to the lack of versioning but at the cost of historical accuracy.</p>
</li>
</ul>
<p>In practice, the choice of SCD type depends on the specific business requirements, the importance of historical accuracy, query performance needs, and the complexity that the organization can manage. It's not uncommon for a single data warehouse to employ multiple SCD types across different dimensions based on these considerations.</p>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="operational-data-stores-ods--data-operational-stores-dos"><a class="header" href="#operational-data-stores-ods--data-operational-stores-dos">Operational Data Stores (ODS) &amp; Data Operational Stores (DOS)</a></h1>
<p>The terms "Operational Data Stores" (ODS) and "Data Operational Stores" (DOS) are often used interchangeably in the industry, but they can represent slightly different concepts depending on the context in which they are used. Here's a breakdown of the differences based on common interpretations:</p>
<h3 id="operational-data-stores-ods"><a class="header" href="#operational-data-stores-ods">Operational Data Stores (ODS)</a></h3>
<ul>
<li>
<p><strong>Purpose</strong>:
An ODS is primarily designed for integrating data from multiple sources to provide a unified and current view of operational data. It's optimized for routine, operational reporting and queries that require up-to-the-minute data.</p>
</li>
<li>
<p><strong>Data Freshness</strong>:
The data in an ODS is near real-time and reflects the most current state of business operations. It's commonly used for operational reporting and day-to-day decision-making.</p>
</li>
<li>
<p><strong>Data Integration and Cleansing</strong>:
An ODS often involves data integration, cleansing, and consolidation processes to ensure data quality and consistency across different systems.</p>
</li>
<li>
<p><strong>Usage</strong>:
Used by business users and analysts for operational reporting, customer service inquiries, and as an interim store for data that will be loaded into a data warehouse for historical analysis.</p>
</li>
</ul>
<h3 id="data-operational-stores-dos"><a class="header" href="#data-operational-stores-dos">Data Operational Stores (DOS)</a></h3>
<ul>
<li>
<p><strong>Purpose</strong>:
DOS can sometimes refer to the broader category of storage systems used for operational purposes, including databases that support online transaction processing (OLTP) systems.</p>
</li>
<li>
<p><strong>Data Freshness</strong>:
While also dealing with current data, DOS in this context might not necessarily integrate data from multiple sources. Each DOS might be dedicated to a specific application or operational system.</p>
</li>
<li>
<p><strong>Data Integration and Cleansing</strong>:
A DOS might not always include data cleansing and integration functionalities. It may store data as it is generated or captured by operational systems.</p>
</li>
<li>
<p><strong>Usage</strong>:
Used by applications and operational systems for immediate transaction processing, such as order processing systems, inventory management systems, and other OLTP systems.</p>
</li>
</ul>
<h3 id="key-differences"><a class="header" href="#key-differences">Key Differences</a></h3>
<ul>
<li>
<p><strong>Integration</strong>:
ODS typically integrates data from multiple sources and provides a unified view, whereas DOS might refer to individual operational systems or databases optimized for specific applications.</p>
</li>
<li>
<p><strong>Data Processing</strong>:
ODS may include more sophisticated data processing capabilities, such as data cleansing and transformation, to ensure data quality and consistency. DOS, in the broader sense, may focus on efficiently handling transactions and queries for specific operational processes.</p>
</li>
<li>
<p><strong>Use Case</strong>:
ODS is more closely aligned with operational reporting and analytics, providing a comprehensive view of business operations for decision-making. DOS, when referred to as individual operational databases, supports the immediate transactional needs of specific applications.</p>
</li>
</ul>
<p>In practice, the distinction between ODS and DOS can be subtle and depends on the organizational context and specific architecture. Some organizations might use the term DOS to describe what is traditionally known as an ODS, especially when emphasizing the operational aspect of the data store.</p>
<h2 id="operational-data-stores-ods-1"><a class="header" href="#operational-data-stores-ods-1">Operational Data Stores (ODS)</a></h2>
<p>Operational Data Stores (ODS) are centralized databases designed to integrate data from multiple sources for additional operations such as reporting, analysis, and operational support. The ODS is optimized for fast query performance and near real-time analysis, making it a critical component for day-to-day business operations.</p>
<h3 id="goals-2"><a class="header" href="#goals-2">Goals</a></h3>
<ul>
<li>
<p><strong>Data Integration</strong>:
ODS serves as an intermediary between transactional databases and analytical data warehouses, integrating data from various sources into a unified format for operational reporting and decision-making.</p>
</li>
<li>
<p><strong>Real-Time or Near Real-Time Analysis</strong>:
Unlike data warehouses that are optimized for historical data analysis, ODS provides access to current or near real-time data, supporting operational decision-making and reporting.</p>
</li>
<li>
<p><strong>Improved Data Quality</strong>:
Data passing through an ODS is cleansed and transformed, improving overall data quality and consistency across the organization.</p>
</li>
<li>
<p><strong>Reduced Load on Transactional Systems</strong>:
By offloading queries from transactional systems to an ODS, organizations can ensure that their operational systems remain efficient and responsive.</p>
</li>
</ul>
<h3 id="ods-uses-in-modern-data-architecture"><a class="header" href="#ods-uses-in-modern-data-architecture">ODS Uses in Modern Data Architecture</a></h3>
<p>In contemporary data architectures, ODS coexist with data lakes and data warehouses, each serving distinct purposes:</p>
<ul>
<li>
<p><strong>Complementing Data Warehouses</strong>:
While data warehouses store historical, aggregated data for in-depth analysis, ODS provides a snapshot of current operational data, allowing for timely operational reporting and analysis.</p>
</li>
<li>
<p><strong>Feeding Data Lakes and Warehouses</strong>:
ODS can act as a source for data lakes and warehouses, where data is further processed, enriched, and stored for long-term analysis and machine learning applications.</p>
</li>
<li>
<p><strong>Operational Analytics</strong>:
Modern data architectures often include specialized analytical tools that directly query the ODS for operational reporting, dashboarding, and alerting, enabling faster decision-making.</p>
</li>
</ul>
<h3 id="modern-use-cases-of-ods"><a class="header" href="#modern-use-cases-of-ods">Modern Use Cases of ODS</a></h3>
<ul>
<li>
<p><strong>Customer 360 View</strong>:
ODS is used to aggregate data from various customer touchpoints, providing a comprehensive view of customer interactions and behavior in near real-time.</p>
</li>
<li>
<p><strong>Operational Reporting</strong>:
Financial institutions, e-commerce platforms, and other businesses use ODS for operational reports that require the most current data, such as daily sales reports or inventory levels.</p>
</li>
<li>
<p><strong>Data Quality Monitoring</strong>:
Organizations use ODS to monitor data quality, ensuring that operational processes are based on accurate and consistent data.</p>
</li>
<li>
<p><strong>Compliance and Auditing</strong>:
An ODS can store detailed transactional data required for regulatory compliance and auditing purposes, providing easy access to current and historical operational data.</p>
</li>
</ul>
<h3 id="technologies-for-ods"><a class="header" href="#technologies-for-ods">Technologies for ODS</a></h3>
<ul>
<li>
<p><strong>Relational Databases</strong>:
Traditional relational databases like Oracle, SQL Server, and MySQL are commonly used for ODS due to their ACID compliance and robust query capabilities.</p>
</li>
<li>
<p><strong>In-Memory Databases</strong>:
Technologies like SAP HANA and Redis are used for ODS implementations requiring high-speed data access and processing.</p>
</li>
<li>
<p><strong>Cloud-Based Solutions</strong>:
Cloud services like AWS RDS, Azure SQL Database, and Google Cloud SQL offer managed database services suitable for hosting an ODS, providing scalability and high availability.</p>
</li>
</ul>
<p>In the landscape of modern data architecture, ODS plays a vital role in bridging the gap between raw operational data and analytical insights. By providing timely, integrated, and cleansed data, an ODS enhances operational efficiency and decision-making, complementing the deeper, historical insights derived from data lakes and warehouses.</p>
<h2 id="data-operational-stores-dos-1"><a class="header" href="#data-operational-stores-dos-1">Data Operational Stores (DOS)</a></h2>
<p>Data operational stores (DOS) are specialized databases designed to support operational applications with real-time, transactional data requirements. Unlike analytical data stores, such as data warehouses and data lakes that are optimized for large-scale querying and analysis, DOS is optimized for high-performance, transactional workloads where speed and efficiency of read/write operations are critical.</p>
<h3 id="goals-3"><a class="header" href="#goals-3">Goals</a></h3>
<ul>
<li>
<p><strong>Real-Time Operations</strong>:
DOS are used in scenarios where applications need immediate access to current, transactional data, such as e-commerce platforms, online banking systems, and other customer-facing applications.</p>
</li>
<li>
<p><strong>High Transaction Throughput</strong>:
They are designed to handle a high volume of transactions per second, making them suitable for operational systems where data is frequently updated or accessed.</p>
</li>
<li>
<p><strong>Low Latency</strong>:
DOSs provide low-latency access to data, which is essential for applications that require instantaneous responses, such as payment processing systems.</p>
</li>
<li>
<p><strong>Application Integration</strong>:
They often serve as a backend for operational applications, providing a centralized store for application data that can be easily accessed and manipulated by various services.</p>
</li>
</ul>
<h3 id="dos-uses-in-modern-data-architecture"><a class="header" href="#dos-uses-in-modern-data-architecture">DOS Uses in Modern Data Architecture</a></h3>
<p>In modern data architectures, DOSs coexist with data lakes and data warehouses as part of a broader data ecosystem. While data lakes and data warehouses are used for storing and analyzing large volumes of historical data, DOSs are used for operational applications that need real-time access to current data. The interaction between these components might look like this:</p>
<ul>
<li>
<p><strong>Data Ingestion</strong>:
Data generated by operational activities in the DOS can be ingested into data lakes and data warehouses for long-term storage, historical analysis, and reporting.</p>
</li>
<li>
<p><strong>Data Enrichment</strong>:
Data from data lakes or warehouses can be used to enrich the operational data in the DOS, providing additional context or insights to support operational decision-making.</p>
</li>
<li>
<p><strong>Hybrid Processing</strong>:
Some modern architectures use hybrid processing models where transactional and analytical workloads coexist, leveraging technologies like HTAP (Hybrid Transactional/Analytical Processing) systems.</p>
</li>
</ul>
<h3 id="examples-of-data-operational-stores"><a class="header" href="#examples-of-data-operational-stores">Examples of Data Operational Stores</a></h3>
<ul>
<li>
<p><strong>Relational Databases</strong>:
Traditional relational databases like MySQL, PostgreSQL, and Oracle Database often serve as operational stores, offering ACID (Atomicity, Consistency, Isolation, Durability) properties essential for transactional data integrity.</p>
</li>
<li>
<p><strong>NoSQL Databases</strong>:
NoSQL databases like MongoDB, Cassandra, and Couchbase are used for operational stores, especially when dealing with unstructured data, needs for horizontal scalability, or specific data models like key-value, document, or columnar stores.</p>
</li>
<li>
<p><strong>NewSQL Databases</strong>:
Systems like Google Spanner and CockroachDB combine the scalability of NoSQL systems with the ACID guarantees of traditional relational databases, making them suitable for distributed operational stores.</p>
</li>
</ul>
<p>In summary, data operational stores are a critical component of modern data architecture, particularly for applications requiring real-time data access and high transactional throughput. They complement data lakes and data warehouses by providing a layer optimized for operational activities while enabling seamless data flow and integration across the data ecosystem.</p>
<h3 id="dos-vs-microservices-databases-backend"><a class="header" href="#dos-vs-microservices-databases-backend">DOS vs. Microservices Databases (Backend)</a></h3>
<p>Databases used by microservices in a backend architecture can be seen as a specialized form of Data Operational Stores (DOS), tailored to the specific requirements of a microservices architecture. These databases share the operational focus of traditional DOS, aimed at supporting real-time or near-real-time data access and transaction processing. However, there are notable distinctions rooted in the architectural principles and data management strategies of microservices:</p>
<ul>
<li>
<p><strong>Service Autonomy</strong>:
Microservices architectures advocate for a database-per-service model, where each microservice's database is dedicated solely to that service's functionality. This encapsulation ensures service autonomy, a principle that diverges from traditional DOS, which might aggregate and integrate data from various operational systems to provide a unified view.</p>
</li>
<li>
<p><strong>Data Isolation</strong>:
In line with microservices principles, these databases prioritize data isolation, limiting the scope of data to the boundaries of each service. This approach contrasts with DOS's objective of integrating data from multiple sources for comprehensive operational reporting and analytics.</p>
</li>
<li>
<p><strong>Integration and Duplication</strong>:
Data integration in a microservices ecosystem is commonly handled through APIs, events, or messaging systems, respecting the decoupled nature of services. This method differs from DOS, where data integration occurs at the storage level. Moreover, microservices architectures may intentionally duplicate data across services to maintain decoupling, a practice generally minimized in DOS to ensure data consistency.</p>
</li>
<li>
<p><strong>Operational Reporting</strong>:
While traditional DOS is often used for operational reporting and analysis across integrated data sets, microservices databases are typically not designed for this purpose. Their role is more focused on supporting the specific operational needs of individual services rather than providing a cross-functional operational data view.</p>
</li>
</ul>
<p>The operational databases within a microservices architecture can be viewed as a variant of DOS, with a narrower scope aligned with the microservice they support. The key distinction lies in the microservices' emphasis on service autonomy, data encapsulation, and decentralized data management, which contrasts with the broader, integrative purpose of traditional DOS in providing a unified operational data view. This nuanced understanding bridges the conceptual gap between microservices databases and DOS, highlighting their shared operational focus while acknowledging their architectural and functional differences.</p>
<p>In modern architectures, both play crucial roles, with microservices' databases ensuring service independence and a DOS enhancing organizational-wide data accessibility and decision-making.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="systems-reliability"><a class="header" href="#systems-reliability">Systems Reliability</a></h1>
<blockquote>
<p>The reliability of a system is the property that allows the system's service to be justifiably qualified as reliable.
It measures how the system conforms to the complete, consistent, comprehensible, and unambiguous specification of its behavior.</p>
</blockquote>
<p>This chapter introduces the concepts of reliability and safety explored by Alan Burns and Andy Wellings in their book<sup><a name="to-footnote-1"><a href="concepts/systems_reliability.html#footnote-1">1</a></a></sup> "Real-Time Systems and Programming Languages: ADA 95, Real-Time Java, and Real-Time POSIX." These concepts, developed by different industries mainly between the 60s and 90s, and the concepts of Site Reliability Engineering (SRE), developed from the 2000s onwards, in addition to complementing it with reliability concepts worked on in other engineering fields (mechanical, industrial, etc.), as well as contextualizing it with concepts currently worked on in the software and computer systems industry.</p>
<p>I divided this chapter into three parts, each exploring one of these concepts:</p>
<p><a href="concepts/./systems-reliability/impediments.html"><strong>Impediments</strong></a></p>
<blockquote>
<p>Impediments prevent a system from functioning perfectly or are a consequence of it. This chapter covers impediment classification, including <strong>Failures</strong>, <strong>Errors</strong>, and <strong>Defects</strong>.</p>
</blockquote>
<p><a href="concepts/./systems-reliability/attributes.html"><strong>Attributes</strong></a></p>
<blockquote>
<p>Attributes are the ways and measures by which the <strong>quality of a reliable service can be estimated</strong>.</p>
</blockquote>
<p><a href="concepts/./systems-reliability/mechanisms.html"><strong>Mechanisms</strong></a></p>
<blockquote>
<p>This chapter addresses system reliability mechanisms by internalizing and adopting best practices or applying specific methodologies, architectures, or tools. This chapter aims to create a <strong>data system reliability framework</strong> that engineers can adopt from earlier implementation phases, such as the design phase.</p>
</blockquote>
<p align="center">
  <img src="concepts/../assets/concepts/systems-reliability/concept_v1.svg" alt="Systems Reliability - Concepts">
</p><p><hr/>
<p><a name="footnote-1"><a href="concepts/systems_reliability.html#to-footnote-1">1</a></a>: Alan Burns and Andrew J. Wellings. 2001. Real-Time Systems and Programming Languages: ADA 95, Real-Time Java, and Real-Time POSIX (3rd. ed.). Addison-Wesley Longman Publishing Co., Inc., USA.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="impediments"><a class="header" href="#impediments">Impediments</a></h1>
<p align="center">
  <img src="concepts/systems-reliability/../../assets/concepts/systems-reliability/impediments_v1.svg" alt="System Reliability - Impediments">
</p>
<h2 id="failures-errors-and-faults"><a class="header" href="#failures-errors-and-faults">Failures, Errors, and Faults</a></h2>
<blockquote>
<p><strong>Failures</strong> result from unexpected internal problems that a system eventually exhibits in its external behavior. These problems are called <strong>errors</strong>, and their mechanical or algorithmic causes are called <strong>faults</strong>.
When a system's behavior deviates from its specifications, it is said to have a <strong>failure</strong>, or the system has <strong>failed</strong>.</p>
</blockquote>
<p>Systems are composed of <strong>components</strong>, each of which can be considered a system. Thus, a failure in one system can induce a fault in another, which may result in an error and a potential failure of this system. This failure can continue and affect any related system, and so on.</p>
<p>A faulty system component will produce an error under specific circumstances during the system's lifetime. <em>A system is the sum of external and internal states</em> regarding state transitions.</p>
<p>An external state not specified in the system's behavior will be considered a failure. The system consists of many components (each with its many states), all contributing to its external behavior. The combination of these components' states is called the system's internal state. <em>An unspecified internal state is considered an error, and the component that produced the illegal state transition is said to be faulty</em>.</p>
<p>The three types of failures:</p>
<ul>
<li><strong>Transient failures</strong>: Begin at a specific time, remain in the system for some time, and then disappear.</li>
<li><strong>Permanent failures</strong>: Begin at a certain point and stay in the system until they are repaired.</li>
<li><strong>Intermittent failures</strong>: These are transient failures that occur sporadically.</li>
</ul>
<h2 id="failure-modes"><a class="header" href="#failure-modes">Failure Modes</a></h2>
<blockquote>
<p>A system can fail in many ways. A designer may design the system assuming a finite number of failure modes. However, the system may fail in ways that were not anticipated.</p>
</blockquote>
<p>We can classify the failure modes of the services that a system provides, which are:</p>
<ul>
<li><strong>Value failures</strong> (Value Domain): The value associated with the service is incorrect.</li>
<li><strong>Timing failure</strong> (Time Domain): The service is completed at the wrong time.</li>
<li><strong>Arbitrary failure</strong>: A combination of value and timing failures.</li>
</ul>
<p>Failures in the value domain are classified into:</p>
<ul>
<li><strong>Boundary Error (Constraint Error)</strong>: The value is outside the expected range of values, including errors in data typing.</li>
<li><strong>Wrong value</strong>: The incorrect value is within the correct range of values.</li>
</ul>
<p>Failures in the time domain can cause the service to be delivered:</p>
<ul>
<li><strong>Too early</strong> (premature): the service is delivered before it is required.</li>
<li><strong>Too late</strong> (delayed or performance error): the service is delivered after it is required.</li>
<li><strong>Infinitely late</strong> (omission failure): the service is never delivered.</li>
<li><strong>Unexpected</strong> (commission failure or improvisation): the service is delivered without being expected.</li>
</ul>
<p>In general, we can assume the modes in which a system can fail:</p>
<ul>
<li><strong>Uncontrolled failure</strong>: A system that produces arbitrary errors in value and time domains (including improvisation errors).</li>
<li><strong>Delay failure</strong>: A system that produces correct services in the value domain but suffers from timing delays</li>
<li><strong>Silent failure</strong>: A system that produces correct services in value and time domains until it fails. The only possible failure is omission, and when it occurs, all subsequent services will also suffer from omission failures.</li>
<li><strong>Crash failure</strong>: A system presenting all the properties of a silent failure but allowing other systems to detect it has entered the state of silent failure.</li>
<li><strong>Controlled failure</strong>: A system that fails in a specified and controlled manner.</li>
</ul>
<p>A system consistently producing the correct services is classified as failure-free.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="attributes"><a class="header" href="#attributes">Attributes</a></h1>
<h2 id="reliability"><a class="header" href="#reliability">Reliability</a></h2>
<blockquote>
<p><em>Reliability</em> is the probability <em>R(t)</em> that the system will <strong>continue functioning at the end of the process</strong>.</p>
</blockquote>
<p>The time <em>t</em> is measured in continuous working hours between diagnostics. The constant failure rate λ is measured in <em>failures/hour</em>. The useful life of a system component is the constant region (on a logarithmic scale) of the curve between the component's age and its failure rate. The region of the graph before equilibrium is the burn-in phase, and the region where the failure rate starts to increase is the end-of-life phase. Thus, we have <em>R(t) = exp(-λt)</em>.</p>
<h2 id="availability"><a class="header" href="#availability">Availability</a></h2>
<blockquote>
<p><em>Availability</em> is the measure of the <strong>frequency of incorrect service periods</strong>.</p>
</blockquote>
<h2 id="dependability"><a class="header" href="#dependability">Dependability</a></h2>
<blockquote>
<p>Continuity of service delivery.</p>
</blockquote>
<p>It is a measure (probability) of the <strong>success with which the system conforms to the definitive specification of its behavior</strong>.</p>
<h2 id="safety"><a class="header" href="#safety">Safety</a></h2>
<blockquote>
<p><em>Safety</em> is the absence of conditions that can cause damage and the propagation of <strong>catastrophic damage</strong> in production.</p>
</blockquote>
<p>However, as this definition can classify virtually any process as unsafe, we often consider the term <strong>mishap</strong>.</p>
<blockquote>
<p>A <em>Mishap</em> is an <strong>unplanned event</strong> or sequence of events that can produce catastrophic damage.</p>
</blockquote>
<p>Despite its similarity to the definition of <em>Dependability</em>, there is a crucial difference in emphasis:  <em>Dependability</em> is the measure of success with which the system conforms to the specification of its behavior, typically in terms of probability, while <em>Safety</em> is the improbability of conditions leading to a mishap occurring, regardless of whether the intended function is performed.</p>
<h2 id="integrity"><a class="header" href="#integrity">Integrity</a></h2>
<blockquote>
<p><em>Integrity</em> is the absence of conditions that can lead to inappropriate alterations of data in production.</p>
</blockquote>
<h2 id="confidentiality"><a class="header" href="#confidentiality">Confidentiality</a></h2>
<blockquote>
<p><em>Confidentiality</em> is the absence of unauthorized data access.</p>
</blockquote>
<h2 id="maintainability"><a class="header" href="#maintainability">Maintainability</a></h2>
<blockquote>
<p><em>Maintainability</em> is the ability to undergo repairs and evolve.</p>
</blockquote>
<h2 id="scalability"><a class="header" href="#scalability">Scalability</a></h2>
<blockquote>
<p><em>Scalability</em> is the ability to adapt to business needs.</p>
</blockquote>
<h2 id="deficiencies"><a class="header" href="#deficiencies">Deficiencies</a></h2>
<blockquote>
<p><em>Deficiencies</em> are circumstances that cause or are a product of <strong>unreliability</strong>.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mechanisms"><a class="header" href="#mechanisms">Mechanisms</a></h1>
<p align="center">
  <img src="concepts/systems-reliability/../../assets/concepts/systems-reliability/mechanisms_v1.svg" alt="Systems Reliability - Mechanisms">
</p>
<ul>
<li><a href="concepts/systems-reliability/./fault_prevention_avoidance.html">Fault Prevention: Avoidance</a></li>
<li><a href="concepts/systems-reliability/./fault_tolerance.html">Fault Tolerance</a></li>
<li><a href="concepts/systems-reliability/./fault_prevention_elimination.html">Fault Prevention: Elimination</a></li>
<li><a href="concepts/systems-reliability/./fault_prediction.html">Fault Predictions</a></li>
<li><a href="concepts/systems-reliability/./reliability_tools.html">Reliability Tools</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fault-prevention-avoidance"><a class="header" href="#fault-prevention-avoidance">Fault Prevention: Avoidance</a></h1>
<p>There are two phases in fault prevention: <strong>avoidance</strong> and <strong>elimination</strong>.</p>
<blockquote>
<p>Avoidance aims to limit the introduction of potentially defective data and objects during the execution of the process.</p>
</blockquote>
<p>Such as:</p>
<ul>
<li>The use of validated and clean information sources, when possible.</li>
<li>The implementation of data cleaning and validation processes for raw data.</li>
<li>The validation of table and column availability within databases.</li>
<li>The introduction of branch operators for effective data management.</li>
<li>Implementing rigorous code review processes to maintain a clean and secure codebase.</li>
<li>Adopting standardized coding practices and safe coding guidelines to minimize errors and security vulnerabilities.</li>
<li>The utilization of automated testing frameworks for continuous testing (unit, integration, system) throughout the development cycle.</li>
<li>Configuration management tools and practices are applied to oversee changes in software and hardware, ensuring all modifications are authorized and tested.</li>
<li>The engagement in detailed requirement analysis and system design reviews to affirm the system's resilience against potential faults, including modeling and simulation tools.</li>
<li>Incorporating fail-safe and fail-soft designs to maintain system safety in case of failure, including redundancy strategies for critical components.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fault-tolerance"><a class="header" href="#fault-tolerance">Fault Tolerance</a></h1>
<blockquote>
<p>Given the limitations in fault prevention, especially as data and processes frequently change, it becomes necessary to resort to fault tolerance.</p>
</blockquote>
<p>There are different levels of fault tolerance:</p>
<ul>
<li><strong>Full tolerance</strong>: there is no management of adverse or unwanted conditions; the process does not adapt to internal or external values.</li>
<li><strong>Controlled degradation</strong> (or graceful degradation): notifications are triggered in the presence of faults, and if they are significant enough to interrupt the task flow (thresholds, non-existence, or unavailability of data), branch operators will select the subsequent tasks.</li>
<li><strong>Fail-safe</strong>: detected faults are significant enough to determine that the process should not occur; a short-circuit or circuit breaker operator cancels the execution of subsequent tasks, stakeholders are notified, and if there is no automatic process to deal with the problem, the data team can take actions such as rerunning the processes that generate the necessary inputs or escalating the case.</li>
</ul>
<p>The design of fault-tolerant processes assumes:</p>
<ul>
<li>The task algorithms have been correctly designed.</li>
<li>All possible failure modes of the components are known.</li>
<li>All possible interactions between the process and its environment have been considered.</li>
</ul>
<h2 id="redundancy"><a class="header" href="#redundancy">Redundancy</a></h2>
<blockquote>
<p>All available fault techniques include adding external elements to the system to detect and recover from faults. These elements are redundant in the sense that they are not necessary for the system's normal operation; this is called <strong>protective redundancy</strong>. The goal of tolerance is to minimize redundancy while maximizing reliability, always under system complexity and size constraints. <em>Care must be taken when designing fault-tolerant systems, as components increase the complexity and maintenance of the entire system, which can in itself lead to less reliable systems</em>.</p>
</blockquote>
<p align="center">
  <img src="concepts/systems-reliability/../../assets/concepts/systems-reliability/fault_tolerance_v1.svg" alt="Systems Reliability - Mechanisms - Fault Tolerance - Redudancy">
</p>
<p>Systems Redundancy is classified into static and dynamic. <strong>Static redundancy</strong>, or masking, involves using redundant components to hide the effects of faults. <strong>Dynamic redundancy</strong> is redundancy within a component that makes it indicate, implicitly or explicitly, that the output is erroneous; another component must provide recovery. This fault tolerance technique has four phases:</p>
<ol>
<li><strong>Error detection</strong>: no fault tolerance action will be taken until an error has been detected.</li>
<li><strong>Damage confinement and assessment</strong>: when an error is detected, the extent of the system that has been corrupted and its scope must be estimated (error diagnosis).</li>
<li><strong>Error recovery</strong>: one of the most critical aspects of fault tolerance. Error recovery techniques should direct the corrupted system to a state from which it can continue its normal operation (perhaps with functional degradation).</li>
<li><strong>Failure treatment and service continuation</strong>: an error is a failure symptom; although the damage might have been repaired, the failure still exists, and therefore, the error may recur unless some form of maintenance is performed.</li>
</ol>
<h3 id="1-error-detection"><a class="header" href="#1-error-detection">1. Error Detection</a></h3>
<blockquote>
<p>The effectiveness of a fault-tolerant system depends on the <strong>effectiveness of error detection</strong>.</p>
</blockquote>
<p>Error detection is classified into:</p>
<ul>
<li><strong>Environmental detections</strong>: Errors are detected in the application's environment and handled by exceptions.</li>
<li><strong>Application detection</strong>: Errors are identified in the application itself.
<ul>
<li><strong>Reverse checks</strong>: Applied in components with an isomorphic relationship (one-to-one) between input and output. This method calculates an input value from the output value, which is then compared with the original. Inexact comparison techniques must be adopted when dealing with real numbers.</li>
<li><strong>Rationality checks</strong>: Based on the design and construction knowledge of the system. They verify that the state of the data or the value of an object is reasonable based on its intended use.</li>
</ul>
</li>
</ul>
<h3 id="2-damage-confinement-and-assessment"><a class="header" href="#2-damage-confinement-and-assessment">2. Damage Confinement and Assessment</a></h3>
<blockquote>
<p>There will always be a time magnitude between the occurrence of a defect and the detection of the error, making it essential to assess any damage that may have occurred in this time interval.</p>
</blockquote>
<p>Although the type of error detected can help evaluate the damage - when performing the error handling routine - erroneous information could have been disseminated through the system and its environment. Thus, damage assessment is directly related to the precautions taken by the system designer for damage confinement. Damage confinement refers to structuring the system in such a way as to minimize the damage caused by a faulty component.</p>
<p><strong>Modular decomposition</strong> and <strong>atomic actions</strong> are two main techniques for structuring systems to facilitate damage confinement. Modular decomposition means that systems should be broken down into components, each represented by one or more modules. The interaction of the components occurs through well-defined interfaces, and the internal details of the modules are hidden and not directly accessible from the outside. This structuring makes it more difficult for an error in one component to propagate to another.</p>
<p>Modular decomposition provides a static structure, while atomic actions structure the system dynamically. An action is said to be atomic if there are no interactions between the activity and the system during the action. These actions move the system from one consistent state to another and restrict information flow between components.</p>
<h3 id="3-error-recovery"><a class="header" href="#3-error-recovery">3. Error Recovery</a></h3>
<blockquote>
<p>Error recovery procedures begin once the detected error state and its possible damages have been assessed. This phase is the most important within fault tolerance techniques, which must transform an erroneous state of the system into another from which it can continue its normal operation, perhaps with some service degradation.</p>
</blockquote>
<p><strong>Forward recovery</strong> and <strong>backward recovery</strong> are the most common error recovery strategies. The forward error recovery attempts to continue from the erroneous state by making selective corrections to the system's state, including protecting any aspect of the controlled environment that could be put at risk or damaged by the failure.</p>
<p>The backward recovery strategy consists of restoring the system to a safe state before the one in which the error occurred and then executing an alternative section of the task. This section will have the same functionality as the section that produced the defect but using a different algorithm. It is expected that this alternative will not produce the same defect as the previous version so that it will rely on the designer's knowledge of the possible failure modes of this component.</p>
<p>The designer must be clear about the service degradation levels, considering the services and processes that depend on it. Error recovery is part of the <a href="concepts/systems-reliability/./corrective_actions.html">Corrective Action and Preventive Action processes (CAPA)</a>.</p>
<h3 id="4-failure-treatment-and-continued-service"><a class="header" href="#4-failure-treatment-and-continued-service">4. Failure Treatment and Continued Service</a></h3>
<blockquote>
<p>An error is a manifestation of a defect, and although the error recovery phase may have brought the system to an error-free state, the error can recur. Therefore, the final phase of fault tolerance is to eradicate the failure from the system so that normal service can continue.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fault-prevention-elimination"><a class="header" href="#fault-prevention-elimination">Fault Prevention: Elimination</a></h1>
<blockquote>
<p>The second phase of fault prevention is fault elimination. This phase typically involves procedures to find and eliminate the causes of errors.</p>
</blockquote>
<p>Although techniques such as code reviews (e.g. linters) and local debugging are used, peer reviews and exhaustive testing with various combinations of input states and environments are not always carried out.</p>
<p>QA testing cannot verify that output values are compatible with the business and its applications, so it usually focuses on time-related failure modes (such as timeouts) and <strong>defects</strong>. Unfortunately, system testing cannot be exhaustive and eliminate all potential faults, mainly due to:</p>
<ul>
<li>
<p>Tests are used to demonstrate the presence of faults, not their absence.</p>
</li>
<li>
<p>The difficulty of performing tests in production. Testing failures in production are akin to <strong>live combat</strong>, meaning the consequences of errors can directly impact the business, leading to potentially poor decisions. For example, an incorrect calculation of a KPI can lead to erroneous actions and decrease the business's confidence in the data processes.</p>
</li>
<li>
<p>Errors introduced during the system requirements stage may not manifest until the system is operational. For example, a DAG (Directed Acyclic Graph) is scheduled to run when the data source is not yet available or complete. For this specific example, sensors might be implemented to only continue the execution when the data source is available or fail if not available within a particular timeframe (timeout).</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="failure-prediction"><a class="header" href="#failure-prediction">Failure Prediction</a></h1>
<blockquote>
<p>Accurate and rapid prediction of failures allows those of us maintaining processes to ensure higher service availability. Unfortunately, failure prediction is much more complex than detection.</p>
</blockquote>
<p>To predict a failure, it must be identified and classified. Failures must also be predictable, meaning there are system (and component) state changes that lead to failure, or the failure occurs regularly following some pattern. Both cases can be translated into time series prediction problems, and sensor and log data can be used to train prediction models.</p>
<p>The collected data will hardly be ready for use by prediction models, so one or more preprocessing tasks must be carried out:</p>
<ul>
<li><strong>Data synchronization</strong>: metrics collected by various agents must be aligned in time.</li>
<li><strong>Data cleaning</strong>: removing unnecessary data and generating missing data (e.g., interpolation).</li>
<li><strong>Data normalization</strong>: metric values are normalized to make magnitudes comparable.</li>
<li><strong>Feature selection</strong>: relevant metrics are identified for use in the models.</li>
</ul>
<p>Once the data is preprocessed, it will be used in two pipelines: a training pipeline and an inference pipeline. The training pipeline uses bulk data to train the model to be made available to the inference pipeline. The inference results will indicate the presence or absence of specific types of failures in the monitored metric sample.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reliability-tools"><a class="header" href="#reliability-tools">Reliability Tools</a></h1>
<p>Many tools, patterns, techniques, and ideas help with reliability engineering, especially when discussing keeping data systems reliable. In this section, we'll look at some of these. You'll find more information about them in the appendices of this book. We won't go into every detail here because we'll use real-life examples in the <strong>Use Cases</strong> chapter to show how these tools work in practice.</p>
<p>Some tools we'll talk about come from different areas of engineering, like software, mechanical, or industrial engineering. These tools might be used sparingly in data reliability engineering, mainly because the data industry has developed tools that fit what it needs better. But, looking at these different tools can give us new ideas for making data systems more reliable.</p>
<p>In this section:</p>
<ul>
<li><a href="concepts/systems-reliability/./observability_tools.html">Observability Tools</a></li>
</ul>
<blockquote>
<p>Observability tools are crucial for monitoring, logging, and tracing data systems to understand their behavior and identify issues early. DataDog is highly adopted.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./data_quality_automation_tools.html">Data Quality Automation Tools</a></li>
</ul>
<blockquote>
<p>Data quality automation tools are essential for maintaining high data quality, which is fundamental to data reliability. Great Expectations and dbt are viable options in this category.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./version_control_systems.html">Version Control Systems</a></li>
</ul>
<blockquote>
<p>Version control systems are vital for tracking changes in data pipelines, configurations, and codebases to ensure consistency and facilitate collaboration. Git-based solutions are the go-to options for most scenarios, except for data versioning control or database versioning.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./data_lineage_tools.html">Data Lineage Tools</a></li>
</ul>
<blockquote>
<p>Data lineage is essential for understanding the flow of data through systems, which is critical to diagnosing issues and ensuring data integrity. Metadata Management Systems offer a complete solution for it.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./workflow_orchestration_tools.html">Workflow Orchestration Tools</a></li>
</ul>
<blockquote>
<p>Workflow orchestration tools are central to managing data pipelines and dependencies, ensuring data tasks are executed reliably and efficiently. Apache Airflow is the most commonly employed tool.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./data_transformation_tools.html">Data Transformation and Testing Tools</a></li>
</ul>
<blockquote>
<p>Data transformation and testing tools are critical for ensuring that data processing logic is correct and data remains accurate and consistent through transformations. The options are endless and available for the most diverse data stacks, from libraries for many programming languages to open-source platforms to fully managed enterprise platforms.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./infrastructure_as_code_tools.html">Infrastructure as Code Tools</a></li>
</ul>
<blockquote>
<p>Infrastructure as Code (IaC) tools are fundamental for managing infrastructure reliably and consistently, enabling scalable and repeatable environments. Hashicorp Terraform is hugely used today, but many open-source alternatives exist.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./container_orchestration_tools.html">Container Orchestration Tools</a></li>
</ul>
<blockquote>
<p>Container orchestration tools are essential for managing containerized applications, supporting scalability, and ensuring consistent environments. Most cloud providers offer exclusive solutions, but many cloud-agnostic options are available.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./fracas.html">Failure Reporting, Analysis, and Corrective Action System (FRACAS)</a></li>
</ul>
<blockquote>
<p>FRACAS is a valuable system for systematically addressing and learning from failures to improve system reliability. Traditional engineering industries commonly implement this system, but it is rare in software and data engineering, especially for startups and small companies.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./corrective_actions.html">Corrective Actions</a></li>
</ul>
<blockquote>
<p>In data engineering, Corrective Actions are about fixing problems and improving processes and systems for long-term reliability and efficiency. The Corrective Action and Preventive Action Process (CAPA) is an example of a process implementing Corrective Actions.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./reliability_block_diagrams.html">Reliability Block Diagrams (RBD)</a></li>
</ul>
<blockquote>
<p>RBDs help visualize and analyze the reliability of complex systems and their components. Traditional engineering industries often adopt it, especially for critical systems like automotive, aerospace, military, and medical machines.</p>
</blockquote>
<p>In the appendices:</p>
<ul>
<li><a href="concepts/systems-reliability/./chaos_engineering_tools.html">Chaos Engineering Tools</a></li>
</ul>
<blockquote>
<p>Chaos engineering tools are helpful for proactively identifying potential points of failure by intentionally introducing chaos into systems.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./high_availability.html">High Availability</a></li>
</ul>
<blockquote>
<p>The High Availability principle consists of strategies and practices to ensure that systems and data are accessible when needed, minimizing downtime.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./antifragility.html">Antifragility</a></li>
</ul>
<blockquote>
<p>Concepts and practices that go beyond resilience, ensuring systems improve in response to stressors and challenges.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./bulkhead_pattern.html">Bulkhead Pattern</a></li>
</ul>
<blockquote>
<p>The Bulkhead Pattern is an architectural pattern for isolating and preventing failures from cascading through systems.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./cold_standby.html">Cold Standby</a></li>
</ul>
<blockquote>
<p>The Cold Standby is a redundancy strategy where backup systems are kept on standby and are only activated when the primary system fails.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./single_point_of_failure.html">Single Point of Failure (SPOF)</a></li>
</ul>
<blockquote>
<p>Identifying and mitigating SPOFs is critical to prevent entire system failures due to the failure of a single component.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./grdhl.html">General Reliability Development Hazard Logs (GRDHL)</a></li>
</ul>
<blockquote>
<p>GRDHL is a proactive approach to identifying and managing potential system hazards.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./spare_parts_stocking_strategy.html">Spare Parts Stocking Strategy</a></li>
</ul>
<blockquote>
<p>Having critical components on hand can be helpful in quickly addressing hardware failures.</p>
</blockquote>
<ul>
<li><a href="concepts/systems-reliability/./availability_controls.html">Availability Controls</a></li>
</ul>
<blockquote>
<p>Measures to ensure data and systems remain available, including backups, redundancy, and failover systems.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="observability-and-data-observability-tools"><a class="header" href="#observability-and-data-observability-tools">Observability and Data Observability Tools</a></h1>
<p>In the context of software and systems, observability refers to the ability to infer the internal states of a system based on its external outputs.
It extends beyond monitoring by capturing what's going wrong and providing insights into why it's happening.</p>
<p>Data Observability specifically applies observability principles to data and data systems.
It involves monitoring the health of the data flowing through systems, identifying anomalies, pipeline failures, and schema changes, and ensuring data quality and reliability.</p>
<h3 id="justifications"><a class="header" href="#justifications">Justifications</a></h3>
<p>Data observability is crucial for businesses that rely heavily on data-driven decision-making processes. It ensures that data quality and consistency are maintained across pipelines.
Secondly, it reduces downtime by enabling users to quickly identify and resolve data issues.
Finally, it enhances trust in data by providing transparency into data lineage, health, and usage.</p>
<h3 id="what-they-solve"><a class="header" href="#what-they-solve">What They Solve</a></h3>
<p>Data observability tools are designed to address common issues with data, including data downtime, which occurs when data is missing, erroneous, or otherwise unusable.
These tools can also help detect schema changes that may break downstream analytics and identify data drifts and anomalies that can lead to incorrect analytics.
Data observability tools can also optimize data pipelines and improve resource utilization, leading to more efficient data processing.</p>
<h3 id="challenges"><a class="header" href="#challenges">Challenges</a></h3>
<p>Implementing data observability can be challenging due to the vast volume and variety of data, which makes comprehensive observability difficult. Integrating observability tools with existing data systems and workflows can also be daunting; balancing observability overhead with system performance is critical.</p>
<h3 id="methods"><a class="header" href="#methods">Methods</a></h3>
<p>Data observability is achieved through:</p>
<ul>
<li><strong>Monitoring</strong>: Tracking key metrics and logs to understand the system's health.</li>
<li><strong>Tracing</strong>: Following data through its entire lifecycle to understand its flow and transformations.</li>
<li><strong>Alerting</strong>: Setting up real-time notifications for anomalies or issues detected in the data.</li>
</ul>
<h3 id="data-observability-tools"><a class="header" href="#data-observability-tools">Data Observability Tools</a></h3>
<p>Several tools and platforms provide data observability capabilities, ranging from open-source projects to commercial solutions. They include:</p>
<ul>
<li><a href="https://prometheus.io/"><strong>Prometheus</strong></a> &amp; <a href="https://grafana.com/"><strong>Grafana</strong></a>: Often used together, Prometheus is used for metrics collection, and Grafana is used for visualization; they can monitor data systems' performance and health.</li>
<li><a href="https://www.elastic.co/elastic-stack/"><strong>Elastic Stack (ELK)</strong></a>: Elasticsearch for search and data analytics, Logstash for data processing, and Kibana for data visualization offer a powerful stack for observability.</li>
<li><a href="https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/index.html"><strong>Apache Airflow</strong></a>: While primarily a workflow orchestration tool, Airflow provides extensive logging and monitoring capabilities for data pipelines. Airflow can be set up to send metrics to StatsD or OpenTelemetry.</li>
<li><a href="https://www.datadoghq.com/"><strong>DataDog</strong></a>: Offers a SaaS-based monitoring platform with capabilities for monitoring cloud-scale applications, including data pipelines. DataDog dashboards and metrics can be deployed using Terraform.</li>
<li><a href="https://www.montecarlodata.com/"><strong>Monte Carlo</strong></a>: A data observability platform that uses machine learning to identify, evaluate, and remedy data reliability issues across data products.</li>
</ul>
<p>Many contemporary data tools, including ELT and ETL platforms, support exporting metrics to <a href="https://github.com/etsy/statsd">StatsD</a> and <a href="https://opentelemetry.io/">OpenTelemetry</a>.
Numerous tools (e.g., Airbyte) allow Prometheus integration within their Kubernetes deployment configurations.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-quality-automation-tools"><a class="header" href="#data-quality-automation-tools">Data Quality Automation Tools</a></h1>
<p>Tools like Great Expectations or Deequ allow data engineers to define and automate data quality checks within data pipelines. By continuously testing data for anomalies, inconsistencies, or deviations from defined quality rules, these tools help maintain high data quality standards.</p>
<p>This topic will be explored in depth in the chapter on <a href="concepts/systems-reliability/../data_quality.html"><strong>Data Quality</strong></a>, and there will be many use cases and examples throughout the book. Additionally, I recommend using some tools, platforms, and libraries that might help automate and test data quality, including:</p>
<ul>
<li><a href="https://www.getdbt.com/"><strong>dbt (Data Build Tool)</strong></a></li>
</ul>
<blockquote>
<p>An open-source tool that enables data analysts and engineers to transform data in their warehouses more effectively by defining data models, testing data quality, and documenting data.</p>
</blockquote>
<ul>
<li><a href="https://greatexpectations.io/"><strong>Great Expectations</strong></a></li>
</ul>
<blockquote>
<p>An open-source tool that allows data teams to write tests for their data, ensuring it meets defined expectations for quality.</p>
</blockquote>
<ul>
<li><a href="https://github.com/awslabs/deequ"><strong>AWS Deequ</strong></a></li>
</ul>
<blockquote>
<p>An open-source library built on top of Apache Spark for defining 'unit tests' for data, which allows for large-scale data quality verification.</p>
</blockquote>
<ul>
<li><a href="https://github.com/sodadata/soda-core"><strong>Soda Core</strong></a></li>
</ul>
<blockquote>
<p>An open-source framework for scanning, validating, and monitoring data quality, ensuring datasets meet quality standards.</p>
</blockquote>
<p>Other options, which I haven't personally tried but frequently appear in online rankings, including those from enterprise-level solutions:</p>
<ul>
<li>
<p><a href="https://www.talend.com/products/data-catalog/"><strong>Talend Data Catalog</strong></a> &amp; <a href="https://www.talend.com/products/data-fabric/"><strong>Data Fabric</strong></a>: These tools offer comprehensive data quality management, including discovery, cleansing, enrichment, and monitoring to ensure data integrity.</p>
</li>
<li>
<p><a href="https://www.sas.com/en_gb/software/data-preparation-and-quality.html"><strong>SAS Data Quality</strong></a>: A suite of tools by SAS that helps cleanse, monitor, and enhance the quality of data within an organization.</p>
</li>
<li>
<p><a href="https://www.sap.com/products/technology-platform/master-data-governance.html"><strong>SAP Master Data Governance</strong></a>: A platform that provides centralized governance for master data, ensuring compliance, data quality, and consistency across business processes.</p>
</li>
<li>
<p><a href="https://www.oracle.com/big-data/data-catalog/"><strong>Oracle Cloud Infrastructure Data Catalog</strong></a>: A metadata management service that helps organize, find, access, and govern data using a comprehensive data catalog.</p>
</li>
<li>
<p><a href="https://www.ataccama.com/platform"><strong>Ataccama ONE Platform</strong></a>: A comprehensive data management platform offering data quality, governance, and stewardship capabilities to ensure data is accurate and usable.</p>
</li>
<li>
<p><a href="https://firsteigen.com/"><strong>First Eigen</strong></a>: A data quality management tool that provides analytics and monitoring to maintain high data quality standards across systems.</p>
</li>
<li>
<p><a href="https://www.bigeye.com/"><strong>BigEye</strong></a>: A monitoring platform designed for data engineers, providing automated data quality checks to ensure real-time data reliability.</p>
</li>
<li>
<p><a href="https://dataladder.com/"><strong>Data Ladder</strong></a>: A data quality software that provides cleansing, matching, deduplication, and enrichment features to improve data quality.</p>
</li>
<li>
<p><a href="https://www.dqlabs.ai/platform/"><strong>DQLabs Data Quality Platform</strong></a>: An AI-driven platform for managing data quality, offering features like profiling, cataloging, and anomaly detection.</p>
</li>
<li>
<p><a href="https://www.precisely.com/product/precisely-trillium/trillium-quality"><strong>Precisely Trillium Quality</strong></a>: A data quality solution that offers profiling, cleansing, matching, and enrichment capabilities to ensure high-quality data.</p>
</li>
<li>
<p><a href="https://www.syniti.com/solutions/master-data-management/"><strong>Syniti Master Data Management</strong></a>: A solution to maintain and synchronize high-quality master data across the organizational ecosystem.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="version-control-systems-for-data"><a class="header" href="#version-control-systems-for-data">Version Control Systems for Data</a></h1>
<p>Version Control Systems (VCS) are essential tools in software development, enabling developers to track and manage changes to code over time. Regarding data, the concept of version control is equally important but can be more complex due to the data's dynamic and voluminous nature.</p>
<h2 id="importance-of-version-control-for-data"><a class="header" href="#importance-of-version-control-for-data">Importance of Version Control for Data</a></h2>
<p>In data projects, changes are often made to the code, such as data transformation scripts or analysis models, as well as to the data itself. Version control for data is a crucial process that ensures every change made to datasets and data processing scripts is tracked, documented, and reversible. This process is vital for three main reasons:</p>
<ul>
<li><strong>Reproducibility</strong>: Version control for data ensures that data analyses can be reproduced over time, even as data and code change.</li>
<li><strong>Collaboration</strong>: It facilitates collaboration among data professionals by managing changes from multiple contributors without conflict.</li>
<li><strong>Auditability</strong>: Version control for data provides a historical record of data and code changes, essential for satisfying audit requirements, especially in regulated industries.</li>
</ul>
<h2 id="version-control-systems-adapted-for-data"><a class="header" href="#version-control-systems-adapted-for-data">Version Control Systems Adapted for Data</a></h2>
<p>While traditional VCS tools like Git are widely used for code, adapting them for data poses challenges due to many datasets' size and binary format. However, several tools and practices have been developed to address these challenges:</p>
<ul>
<li>
<p><strong>Data Versioning Tools</strong>:
Tools like <a href="https://dvc.org/">DVC (Data Version Control)</a> and <a href="https://www.pachyderm.com/">Pachyderm</a> offer functionalities designed explicitly for data versioning. They allow data scientists and engineers to track versions of data and models, often storing metadata and changes in a Git repository while keeping large datasets in dedicated storage.</p>
</li>
<li>
<p><strong>Data Catalogs with Versioning Features</strong>:
Some data catalog tools provide versioning capabilities and tracking changes to data definitions, schemas, and metadata, which is crucial for understanding how data evolves.</p>
</li>
<li>
<p><strong>Database Versioning</strong>:
Techniques like event sourcing and ledger databases can be used to maintain a historical record of data changes directly within databases, allowing for versioning at the data storage level.</p>
</li>
</ul>
<h2 id="best-practices-for-data-version-control"><a class="header" href="#best-practices-for-data-version-control">Best Practices for Data Version Control</a></h2>
<p>Implementing version control for data involves several best practices:</p>
<ul>
<li><strong>Automate Versioning</strong>: Automate the tracking of changes to data and code as much as possible to ensure consistency and completeness of the version history.</li>
<li><strong>Separate Code and Data</strong>: Store code in a traditional VCS like Git and use data versioning tools to manage datasets, linking them with code versions.</li>
<li><strong>Use Lightweight References</strong>: Store lightweight references or metadata in the version control system for large datasets and keep the actual data in suitable storage solutions to avoid performance issues.</li>
<li><strong>Maintain Clear Documentation</strong>: Document changes comprehensively, including the rationale for changes and their impact on analyses or models.</li>
</ul>
<h2 id="challenges-1"><a class="header" href="#challenges-1">Challenges</a></h2>
<ul>
<li><strong>Data Size and Format</strong>: Large datasets and binary data formats can be challenging to manage with traditional VCS tools.</li>
<li><strong>Performance</strong>: Versioning large datasets can impact the performance of version control operations and require significant storage space.</li>
<li><strong>Complex Dependencies</strong>: Data projects often involve complex dependencies between datasets, code, and computational environments, which can complicate versioning.</li>
</ul>
<p>Version control systems for data are evolving to address the unique needs of data projects, enabling more reliable, collaborative, and auditable data workflows. As the field matures, adopting version control practices tailored for data will become an increasingly critical aspect of data reliability engineering.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-lineage-tools"><a class="header" href="#data-lineage-tools">Data Lineage Tools</a></h1>
<blockquote>
<p>Data lineage tools are essential in comprehending the flow and lifecycle of data within an organization. They track data from its origin through various transformations until it reaches its final form, providing visibility into how data is created, modified, and consumed. These tools are crucial in diagnosing and correcting errors, ensuring that data is reliable and trustworthy.</p>
</blockquote>
<h3 id="importance-of-data-lineage"><a class="header" href="#importance-of-data-lineage">Importance of Data Lineage</a></h3>
<p>Data lineage is vital for several reasons:</p>
<ul>
<li>
<p><strong>Transparency</strong>:
It offers a clear view of how data moves and transforms across systems, essential for debugging, auditing, and understanding complex data ecosystems.</p>
</li>
<li>
<p><strong>Compliance</strong>:
In many regulated industries, understanding the origin and transformations of data is necessary to meet compliance requirements regarding data handling and privacy.</p>
</li>
<li>
<p><strong>Data Quality</strong>:
By tracing data back to its sources, organizations can identify and address issues at their root, improving overall data quality.</p>
</li>
<li>
<p><strong>Impact Analysis</strong>:
Data lineage allows organizations to assess the potential impact of changes in data sources or processing logic on downstream systems and reports.</p>
</li>
</ul>
<h3 id="key-features-of-data-lineage-tools"><a class="header" href="#key-features-of-data-lineage-tools">Key Features of Data Lineage Tools</a></h3>
<p>Effective data lineage tools typically offer the following capabilities:</p>
<ul>
<li>
<p><strong>Automated Lineage Capture</strong>:
They automatically track data flows and transformations across various platforms and tools, from databases and data lakes to ETL processes and business intelligence reports.</p>
</li>
<li>
<p><strong>Visualization</strong>:
These tools provide graphical representations of data flows, making understanding complex relationships and dependencies easier.</p>
</li>
<li>
<p><strong>Integration with Data Ecosystem</strong>:
They integrate with various data sources, processing engines, and analytics tools to ensure comprehensive lineage tracking.</p>
</li>
<li>
<p><strong>Metadata Management</strong>:
Beyond just tracking data flow, these tools manage metadata, including data definitions, schemas, and usage information, enriching the lineage information.</p>
</li>
</ul>
<h3 id="popular-data-lineage-tools-and-metadata-management-systems"><a class="header" href="#popular-data-lineage-tools-and-metadata-management-systems">Popular Data Lineage Tools and Metadata Management Systems</a></h3>
<ul>
<li><a href="https://github.com/apache/atlas"><strong>Apache Atlas</strong></a>: An open-source tool designed for scalable governance and metadata management, providing rich lineage visualization and tracking.</li>
<li><a href="https://www.informatica.com/products/data-catalog/enterprise-data-catalog.html"><strong>Informatica Enterprise Data Catalog</strong></a>: A commercial solution offering advanced lineage tracking, metadata management, discovery, and analytics.</li>
<li><a href="https://www.collibra.com/"><strong>Collibra Data Governance Center</strong></a>: A data governance platform with comprehensive data lineage tracking to help organizations understand their data's journey.</li>
<li><a href="https://datahubproject.io/"><strong>DataHub</strong></a>: An open-source metadata and lineage platform aggregating metadata, lineage, and usage information across various data ecosystems.</li>
<li><a href="https://www.amundsen.io/"><strong>Amundsen</strong></a>: An open-source data discovery and metadata platform initially developed by Lyft, which includes data lineage visualization among its features.</li>
<li><a href="https://www.alation.com/product/data-catalog/"><strong>Alation Data Catalog</strong></a>: A data catalog tool that provides metadata management, data discovery, and lineage visualization to improve data literacy across organizations.</li>
<li><a href="https://cloud.google.com/data-catalog/docs/"><strong>Google Cloud Data Catalog</strong></a>: A fully managed and scalable metadata management service that offers discovery, understanding, and governance of data assets in Google Cloud.</li>
</ul>
<h3 id="best-practices-for-implementing-data-lineage"><a class="header" href="#best-practices-for-implementing-data-lineage">Best Practices for Implementing Data Lineage</a></h3>
<ul>
<li>
<p><strong>Start with Critical Data Elements</strong>:
Focus lineage efforts on the most critical data elements, expanding coverage over time.</p>
</li>
<li>
<p><strong>Ensure Cross-Team Collaboration</strong>:
Data lineage impacts multiple teams, from data engineers to business analysts. Collaboration ensures that lineage information meets the needs of all stakeholders.</p>
</li>
<li>
<p><strong>Leverage Automation</strong>:
Automate the capture and updating of lineage information as much as possible to keep it accurate and up-to-date without excessive manual effort.</p>
</li>
<li>
<p><strong>Integrate with Data Governance</strong>:
Data lineage should be an integral part of broader data governance initiatives, ensuring alignment with data quality, privacy, and compliance efforts.</p>
</li>
</ul>
<p>Data lineage tools are indispensable for maintaining transparency, ensuring compliance, enhancing data quality, and facilitating impact analysis in complex data environments. As data ecosystems continue to grow in complexity, the role of data lineage in ensuring data reliability and trustworthiness becomes increasingly essential.</p>
<h2 id="metadata-management-systems"><a class="header" href="#metadata-management-systems">Metadata Management Systems</a></h2>
<p>Metadata management systems are specialized tools designed to handle metadata - data about data. Metadata includes details like data source, structure, content, usage, and policies, providing context that helps users understand and work with actual data.</p>
<h3 id="importance-of-metadata-management"><a class="header" href="#importance-of-metadata-management">Importance of Metadata Management</a></h3>
<p>Effective metadata management is crucial for:</p>
<ul>
<li>
<p><strong>Data Understanding</strong>:
It helps users comprehend the structure, origins, and meaning of data, essential for accurate analysis and decision-making.</p>
</li>
<li>
<p><strong>Data Governance</strong>:
Metadata is foundational for implementing data governance policies, including data privacy, quality, and security standards.</p>
</li>
<li>
<p><strong>Searchability and Discoverability</strong>:
By tagging and cataloging data assets with metadata, these systems make finding and accessing relevant data across large and complex data landscapes easier.</p>
</li>
<li>
<p><strong>Compliance</strong>:
Metadata management supports compliance with regulatory requirements by documenting data lineage, privacy labels, and access controls.</p>
</li>
</ul>
<h3 id="key-features-of-metadata-management-systems"><a class="header" href="#key-features-of-metadata-management-systems">Key Features of Metadata Management Systems</a></h3>
<p>These systems typically offer:</p>
<ul>
<li>
<p><strong>Metadata Repository</strong>:
A centralized storage for collecting, storing, and managing metadata from various data sources and tools.</p>
</li>
<li>
<p><strong>Metadata Harvesting and Integration</strong>:
Automated tools for extracting metadata from databases, data lakes, ETL tools, and BI platforms, ensuring a comprehensive metadata inventory.</p>
</li>
<li>
<p><strong>Data Cataloging</strong>:
Features to organize and categorize data assets, making it easier for users to search and find the necessary data.</p>
</li>
<li>
<p><strong>Lineage and Impact Analysis</strong>:
Visualization of data lineage, showing how data flows and transforms, and analysis tools to assess the impact of changes in data structures or sources.</p>
</li>
</ul>
<h3 id="best-practices-for-metadata-management"><a class="header" href="#best-practices-for-metadata-management">Best Practices for Metadata Management</a></h3>
<ul>
<li>
<p><strong>Standardize Metadata</strong>:
Develop a standardized approach to metadata across the organization to ensure consistency and interoperability.</p>
</li>
<li>
<p><strong>Encourage User Participation</strong>:
Engage users from various departments to contribute to and maintain metadata, ensuring it remains relevant and up-to-date.</p>
</li>
<li>
<p><strong>Integrate with Existing Tools</strong>:
Metadata management systems should integrate seamlessly with existing data tools and platforms to automate metadata collection and utilization.</p>
</li>
<li>
<p><strong>Focus on Usability</strong>:
The system should be user-friendly, enabling non-technical users to quickly search, understand, and leverage metadata in their daily tasks.</p>
</li>
</ul>
<p>Metadata management systems are essential for making data more understandable, usable, and governable. They play a pivotal role in modern data ecosystems by enhancing data discovery, ensuring compliance, and facilitating effective data governance and analytics.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="workflow-orchestration-tools"><a class="header" href="#workflow-orchestration-tools">Workflow Orchestration Tools</a></h1>
<blockquote>
<p>Workflow orchestration tools are software solutions designed to automate and manage complex data workflows across various systems and environments. These tools help coordinate and execute multiple interdependent tasks, ensuring they run in the correct order, are completed successfully, and recover gracefully from failures, improving the reliability of data processing workflows.</p>
</blockquote>
<h3 id="importance-of-workflow-orchestration"><a class="header" href="#importance-of-workflow-orchestration">Importance of Workflow Orchestration</a></h3>
<p>Effective workflow orchestration is critical for the following:</p>
<ul>
<li>
<p><strong>Efficiency</strong>:
Automating routine data tasks reduces manual effort and speeds up data processes.</p>
</li>
<li>
<p><strong>Reliability</strong>:
Orchestrators ensure tasks are executed consistently, handle failures and retries, and maintain the integrity of data workflows.</p>
</li>
<li>
<p><strong>Scalability</strong>:
As data operations grow, orchestration tools help manage increasing volumes of tasks and complexity without linear increases in manual oversight.</p>
</li>
<li>
<p><strong>Visibility</strong>:
Most orchestrators provide monitoring and logging features, giving insights into workflow performance and issues.</p>
</li>
</ul>
<h3 id="key-features-of-workflow-orchestration-tools"><a class="header" href="#key-features-of-workflow-orchestration-tools">Key Features of Workflow Orchestration Tools</a></h3>
<p>These tools typically offer:</p>
<ul>
<li><strong>Task Scheduling</strong>: Ability to schedule tasks based on time or event triggers.</li>
<li><strong>Dependency Management</strong>: Managing task dependencies to ensure they execute in the correct sequence.</li>
<li><strong>Error Handling and Retry Logic</strong>: Automated handling of task failures, including retries and alerting.</li>
<li><strong>Resource Management</strong>: Allocating and managing resources required for tasks, ensuring optimal utilization.</li>
<li><strong>Monitoring and Logging</strong>: Tracking the progress and performance of workflows and logging activity for audit and troubleshooting.</li>
</ul>
<h3 id="popular-workflow-orchestration-tools"><a class="header" href="#popular-workflow-orchestration-tools">Popular Workflow Orchestration Tools</a></h3>
<p>There are several workflow orchestration tools, each with unique features:</p>
<ul>
<li><a href="https://airflow.apache.org/"><strong>Apache Airflow</strong></a>: An open-source platform designed to programmatically author, schedule, and monitor workflows with a rich user interface and extensive integration capabilities.</li>
<li><a href="https://github.com/spotify/luigi"><strong>Luigi</strong></a>: Developed by Spotify, Luigi is a Python-based tool that can manage complex pipelines of batch jobs, handle dependency resolution, manage workflows, and visualize workflows.</li>
<li><a href="https://nifi.apache.org/"><strong>Apache NiFi</strong></a>: Provides an easy-to-use, web-based interface for designing, controlling, and monitoring data flows. It supports data routing, transformation, and system mediation logic.</li>
<li><a href="https://www.prefect.io/"><strong>Prefect</strong></a>: A newer tool that simplifies the automation and monitoring of data workflows, strongly emphasizing error handling and recovery.</li>
<li><a href="https://aws.amazon.com/step-functions/"><strong>AWS Step Functions</strong></a>: A serverless orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications through a visual interface.</li>
<li><a href="https://argoproj.github.io/workflows/"><strong>Argo</strong></a>: A Kubernetes-native workflow orchestration tool that enables the definition and execution of complex, parallel workflows directly within a Kubernetes cluster, making it ideal for containerized jobs and applications.</li>
</ul>
<h3 id="best-practices-for-workflow-orchestration"><a class="header" href="#best-practices-for-workflow-orchestration">Best Practices for Workflow Orchestration</a></h3>
<ul>
<li>
<p><strong>Modular Design</strong>:
Break down workflows into modular, reusable tasks to simplify maintenance and scaling.</p>
</li>
<li>
<p><strong>Comprehensive Testing</strong>:
Thoroughly test workflows and individual tasks to ensure they handle data correctly and recover from failures as expected.</p>
</li>
<li>
<p><strong>Documentation</strong>:
Maintain clear documentation for workflows, including task purposes, dependencies, and parameters, to support collaboration and troubleshooting.</p>
</li>
<li>
<p><strong>Security and Compliance</strong>:
Ensure that orchestration tools and workflows comply with data security and privacy standards relevant to your organization.</p>
</li>
</ul>
<p>Workflow orchestration tools are essential for building efficient, reliable, scalable data processes. They enable organizations to automate complex data workflows, providing the foundation for advanced data operations and analytics.</p>
<h3 id="dags"><a class="header" href="#dags">DAGs</a></h3>
<p>Directed Acyclic Graphs (DAGs) are used extensively in computing and data processing to model tasks and their dependencies. In a DAG, nodes represent tasks, and directed edges represent dependencies between these tasks, indicating the order in which tasks must be executed. The "acyclic" part means that there are no cycles in the graph, ensuring that you can't return to a task once it's completed, which helps prevent infinite loops in workflows. DAGs are particularly useful in workflow orchestration tools for defining complex data processing pipelines, where specific tasks must be completed before others can begin, allowing for efficient scheduling and parallel execution of non-dependent tasks.</p>
<h2 id="apache-airflow"><a class="header" href="#apache-airflow">Apache Airflow</a></h2>
<p>Airflow is designed for authoring, scheduling, and monitoring workflows programmatically. It enables data engineers to define, execute, and manage complex data pipelines, ensuring that data tasks are executed in the correct order, adhering to dependencies, and handling retries and failures gracefully. By providing robust scheduling and monitoring capabilities for data workflows, Airflow plays a pivotal role in maintaining the reliability and consistency of data processing operations.</p>
<p>Apache Airflow contributes significantly to data reliability through its robust workflow orchestration capabilities. Here's how Airflow enhances the reliability of data processes:</p>
<h3 id="scheduled-and-automated-workflows"><a class="header" href="#scheduled-and-automated-workflows">Scheduled and Automated Workflows</a></h3>
<p>Airflow allows for the scheduling of complex data workflows, ensuring that data processing tasks are executed at the right time and in the correct order. This automation reduces the risk of human error and ensures that critical data processes, such as ETL jobs, data validation, and reporting, are run consistently and reliably.</p>
<h3 id="dependency-management"><a class="header" href="#dependency-management">Dependency Management</a></h3>
<p>Airflow's ability to define dependencies between tasks means that data workflows are executed in a manner that respects the logical sequence of data processing steps, ensuring that upstream failures are appropriately handled before proceeding with downstream tasks and maintaining the integrity and reliability of the data pipeline.</p>
<h3 id="retries-and-failure-handling"><a class="header" href="#retries-and-failure-handling">Retries and Failure Handling</a></h3>
<p>Airflow provides built-in mechanisms for retrying failed tasks and alerting when issues occur. This resilience in the face of failures helps to ensure that temporary issues, such as network outages or transient system failures, do not lead to incomplete or incorrect data processing, thereby enhancing data reliability.</p>
<h3 id="extensive-monitoring-and-logging"><a class="header" href="#extensive-monitoring-and-logging">Extensive Monitoring and Logging</a></h3>
<p>With Airflow's comprehensive monitoring and logging capabilities, data engineers can quickly identify and diagnose issues within their data pipelines. This visibility is crucial for maintaining high data quality and reliability, as it allows for prompt intervention and resolution of problems that could compromise data integrity.</p>
<h3 id="dynamic-pipeline-generation"><a class="header" href="#dynamic-pipeline-generation">Dynamic Pipeline Generation</a></h3>
<p>Airflow supports dynamic pipeline generation, allowing workflows that adapt to changing data or business requirements. This flexibility ensures that data processes remain relevant and reliable, even as the underlying data or the processing needs evolve.</p>
<h3 id="scalability-1"><a class="header" href="#scalability-1">Scalability</a></h3>
<p>Airflow's architecture supports scaling up to handle large volumes of data and complex workflows. This scalability ensures that as data volumes grow, the data processing pipelines can continue to operate efficiently and reliably without degradation in performance.</p>
<p>By orchestrating data workflows with these capabilities, Airflow plays a critical role in ensuring that data processes are reliable, efficient, and aligned with business needs, making it an essential tool in the data engineer's toolkit for maintaining data reliability.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-transformation-tools"><a class="header" href="#data-transformation-tools">Data Transformation Tools</a></h1>
<p>Data transformation is a critical process in data workflows, which involves converting data from one format, structure, or value to another. This is done to ensure that the data is in the proper form for analysis, reporting, or further processing and to maintain data quality, integrity, and compatibility across different systems and platforms.</p>
<p>This chapter will explore various tools specifically designed to facilitate data transformation. These tools range from open-source projects to commercial solutions, each with unique features, capabilities, and use cases. Some of the tools we will be discussing include:</p>
<ul>
<li><a href="https://www.getdbt.com/"><strong>dbt (Data Build Tool)</strong></a>: An open-source tool that enables data analysts and engineers to transform data in their warehouses by writing modular SQL queries.</li>
<li><a href="https://nifi.apache.org/"><strong>Apache NiFi</strong></a>: A robust, scalable data ingestion and distribution system designed to automate data flow between systems.</li>
<li><a href="https://camel.apache.org/"><strong>Apache Camel</strong></a>: An open-source integration framework that provides a rule-based routing and mediation engine.</li>
<li><a href="https://www.talend.com/products/talend-open-studio/"><strong>Talend Open Studio</strong></a>: A robust suite of open-source tools for data integration, quality, and management.</li>
<li><a href="https://flink.apache.org/"><strong>Apache Flink</strong></a>: An open-source stream processing framework for high-performance, scalable, and accurate data processing.</li>
<li><a href="https://www.singer.io/"><strong>Singer</strong></a>: An open-source standard for writing scripts that move data between databases, web APIs, and files.</li>
<li><a href="https://airbyte.com/"><strong>Airbyte</strong></a>: An open-source data integration platform that standardizes data movement and collection.</li>
<li><a href="https://github.com/transferwise/pipelinewise"><strong>PipelineWise</strong></a>: A data pipeline framework created by TransferWise that automates data replication from various sources into data warehouses.</li>
<li><a href="https://meltano.com/"><strong>Meltano</strong></a>: An open-source platform for the whole data lifecycle, including extraction, loading, and transformation (ELT).</li>
<li><a href="https://github.com/spotify/luigi"><strong>Luigi</strong></a>: An open-source Python framework for building complex pipelines of batch jobs.</li>
<li><a href="https://www.bonobo-project.org/"><strong>Bonobo</strong></a>: A lightweight Python ETL framework for transforming data in data processing pipelines.</li>
<li><a href="https://spring.io/projects/spring-batch"><strong>Spring Batch</strong></a>: A comprehensive lightweight framework designed to develop batch applications crucial for daily operations.</li>
<li><a href="https://github.com/aws/aws-sdk-pandas"><strong>AWS DataWrangler</strong></a>: A tool for cleaning and transforming data for more straightforward analysis.</li>
<li><a href="https://aws.amazon.com/dms/"><strong>AWS Database Migration Service</strong></a>: A managed migration and replication service that helps move your database and analytics workloads to AWS quickly, securely, and with minimal downtime and zero data loss.</li>
</ul>
<p>Each tool offers distinct advantages and may better suit specific scenarios, from simple data transformations in small projects to handling complex data workflows in large-scale enterprise environments. In this chapter, we'll delve into the features, use cases, and considerations for selecting and implementing these data transformation tools, equipping you with the knowledge to choose the right tool for your data projects.</p>
<h2 id="dbt-data-build-tool"><a class="header" href="#dbt-data-build-tool">dbt (Data Build Tool)</a></h2>
<p>Data Build Tool (dbt) specializes in managing, testing, and documenting data transformations within modern data warehouses. dbt enables data engineers and analysts to write scalable, maintainable SQL code for transforming raw data into structured and reliable datasets suitable for analysis, thereby crucial in maintaining and enhancing data reliability.</p>
<p>It plays a significant role in enhancing data reliability within modern data engineering practices. It is a command-line tool that enables data analysts and engineers to transform data in their warehouses more effectively by writing, testing, and deploying SQL queries. Here's how dbt contributes to data reliability:</p>
<h3 id="version-control-and-collaboration"><a class="header" href="#version-control-and-collaboration">Version Control and Collaboration</a></h3>
<p>dbt encourages using version control systems like Git for managing transformation scripts, which enhances collaboration among team members and maintains a historical record of changes. This practice ensures consistency and reliability in data transformations as changes are tracked, reviewed, and documented.</p>
<h3 id="testing-and-validation"><a class="header" href="#testing-and-validation">Testing and Validation</a></h3>
<p>dbt allows for the implementation of data tests that automatically validate the quality and integrity of the transformed data. These tests can include not-null checks, uniqueness tests, referential integrity checks among tables, and custom business logic validations. By catching issues early in the data transformation stage, dbt helps prevent the propagation of errors downstream, thereby improving the reliability of the data used for reporting and analytics.</p>
<h3 id="data-documentation"><a class="header" href="#data-documentation">Data Documentation</a></h3>
<p>With dbt, data documentation is treated as a first-class citizen. dbt generates documentation for the data models, including descriptions of tables and columns and the relationships between different models. This documentation is crucial for understanding the data transformations and ensuring that all stakeholders have a clear and accurate view of the data, its sources, and transformations, which is essential for data reliability.</p>
<h3 id="data-lineage-1"><a class="header" href="#data-lineage-1">Data Lineage</a></h3>
<p>dbt generates a visual representation of data lineage, showing how different data models are connected and how data flows through the transformations. This visibility into data lineage helps in understanding the impact of changes, troubleshooting issues, ensuring that data transformations are reliable, and maintaining the integrity of the data throughout the pipeline.</p>
<h3 id="incremental-processing"><a class="header" href="#incremental-processing">Incremental Processing</a></h3>
<p>dbt supports incremental data processing, allowing more efficient transformations by only processing new or changed data since the last run. This approach reduces the likelihood of processing errors due to handling smaller volumes of data at a time and ensures that the data remains up-to-date and reliable.</p>
<h3 id="modular-and-reusable-code"><a class="header" href="#modular-and-reusable-code">Modular and Reusable Code</a></h3>
<p>Modular and reusable SQL code is encouraged by dbt, which helps to prevent redundancy and potential errors in data transformation scripts. Standardization of common logic and reuse of macros and packages across projects further enhances the reliability of data transformations.</p>
<p>By incorporating these features and best practices into the data transformation process, dbt plays a vital role in ensuring data accuracy, consistency, and reliability. This is critical for making well-informed business decisions and maintaining trust in data systems.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="infrastructure-as-code-iac-tools"><a class="header" href="#infrastructure-as-code-iac-tools">Infrastructure as Code (IaC) Tools</a></h1>
<blockquote>
<p>IaC tools like Terraform allow data engineers to define and manage infrastructure using code, ensuring that data environments are reproducible, consistent, and maintainable. This reduces the risk of environment-related inconsistencies and errors.</p>
</blockquote>
<p>Infrastructure as Code (IaC) is a crucial practice in DevOps and cloud computing that involves managing and provisioning computing infrastructure through machine-readable definition files rather than physical hardware configuration or interactive configuration tools. IaC enables developers and IT operations teams to automatically manage, monitor, and provision resources through code, which can be versioned and reused, ensuring consistency and efficiency across environments.
Key IaC Tools:</p>
<ul>
<li><a href="https://www.terraform.io/"><strong>HashiCorp Terraform</strong></a>: An open-source tool that allows you to define cloud and on-premises resources in human-readable configuration files that can be versioned and reused.</li>
<li><a href="https://spacelift.io/"><strong>Spacelift</strong></a>: Provides continuous integration and delivery (CI/CD) for infrastructure as code, with support for Terraform, CloudFormation, and Pulumi, integrating version control systems for automation.</li>
<li><a href="https://opentofu.org/"><strong>OpenTofu</strong></a>: Previously named OpenTF, OpenTofu is a fork of Terraform that is open-source, community-driven, and managed by the Linux Foundation.</li>
<li><a href="https://terragrunt.gruntwork.io/"><strong>Terragrunt</strong></a>: A thin wrapper for Terraform that provides extra tools for working with multiple Terraform modules, enhancing Terraform's capabilities for managing complex configurations.</li>
<li><a href="https://www.pulumi.com/"><strong>Pulumi</strong></a>: Allows you to create, deploy, and manage infrastructure on any cloud using familiar programming languages, offering an alternative to declarative configuration languages.</li>
<li><a href="https://aws.amazon.com/cloudformation/"><strong>AWS CloudFormation</strong></a>: Provides a common language for describing and provisioning all the infrastructure resources in AWS cloud environments.</li>
<li><a href="https://azure.microsoft.com/en-us/get-started/azure-portal/resource-manager/"><strong>Azure Resource Manager (ARM)</strong></a>: Enables you to provision and manage Azure resources using declarative JSON templates.</li>
<li><a href="https://cloud.google.com/deployment-manager/"><strong>Google Cloud Deployment Manager (CDM)</strong></a>: Automates creating and managing Google Cloud resources using template or configuration files.</li>
<li><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/"><strong>Kubernetes Operators</strong></a>: Extend Kubernetes' capabilities by automating the deployment and management of complex applications on Kubernetes.</li>
<li><a href="https://www.crossplane.io/"><strong>Crossplane</strong></a>: An open-source Kubernetes add-on that extends clusters to manage and provision infrastructure from multiple cloud providers and services using Kubernetes API.</li>
<li><a href="https://www.ansible.com/"><strong>Ansible</strong></a>: An open-source tool focusing on simplicity and ease of use for automating software provisioning, configuration management, and application deployment.</li>
<li><a href="https://www.chef.io/"><strong>Chef (Progress Chef)</strong></a>: Provides a way to define infrastructure as code, automating how infrastructure is configured, deployed, and managed across your network, regardless of its size.</li>
<li><a href="https://spectralops.io/"><strong>SpectralOps</strong></a>: Aims at securing infrastructure as code by identifying and mitigating risks in configuration files.</li>
<li><a href="https://www.puppet.com/"><strong>Puppet</strong></a>: Enables the automatic management of your infrastructure's configuration, ensuring consistency and reliability across your systems.</li>
<li><a href="https://www.vagrantup.com/"><strong>HashiCorp Vagrant</strong></a>: Provides a simple and easy-to-use command-line client for managing environments, along with a configuration file for automating the setup of virtual machines.</li>
<li><a href="https://www.brainboard.co/"><strong>Brainboard</strong></a>: Offers a visual interface for designing cloud architectures and generating infrastructure as code, simplifying cloud infrastructure provisioning.</li>
</ul>
<p>IaC has become a cornerstone of modern infrastructure management, allowing for the rapid, consistent, and safe deployment of environments. By treating infrastructure as code, organizations can streamline the setup and maintenance of their infrastructure, reduce errors, and increase reproducibility across development, testing, and production environments.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="container-orchestration-tools"><a class="header" href="#container-orchestration-tools">Container Orchestration Tools</a></h1>
<p>Container orchestration tools are essential in managing the lifecycles of containers, especially in large, dynamic environments. They automate containerized applications' deployment, scaling, networking, and management, ensuring that the infrastructure supporting data-driven applications is reliable, scalable, and efficient.</p>
<p>In data reliability engineering, container orchestration tools facilitate the consistent deployment and operation of data pipelines, databases, and analytics tools within containers, enhancing the reliability and availability of data services.</p>
<p>Main Container Orchestration Tools:</p>
<ul>
<li><a href="https://kubernetes.io/"><strong>Kubernetes</strong></a>: An open-source platform that has become the de facto standard for container orchestration, offering powerful capabilities for automating deployment, scaling, and operations of application containers across clusters of hosts.</li>
<li><a href="https://openshift.com/"><strong>OpenShift</strong></a>: Based on Kubernetes, OpenShift adds features such as developer and operational-centric tools and extended security to streamline the development, deployment, and management of containerized applications.</li>
<li><a href="https://www.nomadproject.io/"><strong>HashiCorp Nomad</strong></a>: A simple yet flexible orchestrator that handles containerized applications and supports non-containerized applications, providing unified workflow automation across different environments.</li>
<li><a href="https://docs.docker.com/engine/swarm/"><strong>Docker Swarm</strong></a>: Docker's native clustering and orchestration tool, designed for simplicity and ease of use, enabling the management of Docker containers as a single, virtual Docker engine.</li>
<li><a href="https://www.rancher.com/"><strong>Rancher</strong></a>: An open-source platform for managing Kubernetes in production, providing a complete container management platform that simplifies the deployment and operation of Kubernetes.</li>
<li><a href="https://mesos.apache.org/"><strong>Apache Mesos</strong></a>: A high-performance, flexible resource manager designed to facilitate the efficient sharing and isolation of resources in a distributed environment, often used with Marathon for container orchestration.</li>
<li><a href="https://cloud.google.com/kubernetes-engine/"><strong>Google Kubernetes Engine (GKE)</strong></a>: A managed environment in Google Cloud Platform for deploying, managing, and scaling containerized applications using Kubernetes.</li>
<li><a href="https://cloud.google.com/run/"><strong>Google Cloud Run</strong></a>: A managed platform that automatically scales stateless containers and abstracts infrastructure management, focusing on simplicity and developer productivity.</li>
<li><a href="https://aws.amazon.com/eks/"><strong>AWS Elastic Kubernetes Service (EKS)</strong></a>: A managed Kubernetes service that simplifies running Kubernetes applications on AWS without installing or operating Kubernetes control plane instances.</li>
<li><a href="https://aws.amazon.com/ecs/"><strong>AWS Elastic Container Service (ECS)</strong></a>: A highly scalable, fast container management service that makes it easy to run, stop, and manage Docker containers.</li>
<li><a href="https://aws.amazon.com/fargate/"><strong>AWS Fargate</strong></a>: A serverless compute engine for containers that work with Amazon ECS and EKS, eliminating the need to manage servers or clusters.</li>
<li><a href="https://azure.microsoft.com/en-us/products/kubernetes-service/"><strong>Azure Kubernetes Service (AKS)</strong></a>: A managed Kubernetes service in Azure that simplifies Kubernetes's deployment, management, and operations.</li>
<li><a href="https://azure.microsoft.com/en-us/products/openshift/"><strong>Azure Managed OpenShift Service</strong></a>: Offers an enterprise-grade Kubernetes platform managed by Microsoft and Red Hat, providing a more secure and compliant environment.</li>
<li><a href="https://azure.microsoft.com/en-us/products/container-instances/"><strong>Azure Container Instances</strong></a>: A service providing the fastest and most straightforward way to run a container in Azure without having to manage any virtual machines or adopt a higher-level service.</li>
<li><a href="https://www.digitalocean.com/products/kubernetes"><strong>Digital Ocean Kubernetes Service</strong></a>: A simple and cost-effective way to deploy, manage, and scale containerized applications in the cloud with Kubernetes.</li>
<li><a href="https://www.linode.com/products/kubernetes/"><strong>Linode Kubernetes Engine</strong></a>: A fully managed container orchestration engine for deploying and managing containerized applications and workloads.</li>
</ul>
<p>By leveraging these tools, data reliability engineers can ensure that data-centric applications and services are robust, resilient to failures, and capable of handling fluctuating workloads. This is crucial for maintaining high data quality and availability in modern data ecosystems.</p>
<h2 id="workflow-orchestration-tools--kubernetes-operators"><a class="header" href="#workflow-orchestration-tools--kubernetes-operators">Workflow Orchestration Tools &amp; Kubernetes Operators</a></h2>
<p>Using workflow orchestration tools like Apache Airflow to trigger tasks inside containers managed by Kubernetes, rather than processing and transforming data locally, offers several advantages:</p>
<ul>
<li>
<p><strong>Scalability</strong>:
Containers can be easily scaled up or down in Kubernetes based on the workload, meaning that as data processing demands increase, the system can dynamically allocate more resources to maintain performance, which is more challenging with local processing.</p>
</li>
<li>
<p><strong>Resource Efficiency</strong>:
Kubernetes optimizes underlying resources, ensuring containers use only the resources they need, leading to more efficient resource utilization compared to running processes locally, where resource allocation might not be as finely tuned.</p>
</li>
<li>
<p><strong>Isolation</strong>:
Running tasks in containers ensures that each task operates in its isolated environment. This isolation reduces the risk of conflicts between dependencies of different tasks and improves security by limiting the scope of access for each task.</p>
</li>
<li>
<p><strong>Consistency</strong>:
Containers package not only the application but also its dependencies, ensuring consistency across development, testing, and production environments. This consistency reduces the "it works on my machine" problem that can arise with local processing.</p>
</li>
<li>
<p><strong>Portability</strong>:
Containers can run on any system that supports Docker and Kubernetes, making it easy to move workloads between different environments, from local development machines to cloud providers, without needing to reconfigure or adapt the processing tasks.</p>
</li>
<li>
<p><strong>Fault Tolerance and High Availability</strong>:
Kubernetes provides built-in health checking, failover, and self-healing mechanisms. If a containerized task fails, Kubernetes can automatically restart it, ensuring higher availability than local processing, where failures might require manual intervention.</p>
</li>
<li>
<p><strong>Declarative Configuration and Automation</strong>:
Kubernetes and Airflow support declarative configurations, allowing you to define your workflows and infrastructure as code. This approach facilitates automation and versioning, making deploying, replicating, and managing complex data pipelines easier.</p>
</li>
<li>
<p><strong>Continuous Integration and Continuous Deployment (CI/CD)</strong>:
Integrating containers in CI/CD pipelines is straightforward, enabling automated testing and deployment of data processing tasks. This seamless integration supports more agile and responsive development practices.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="failure-reporting-analysis-and-corrective-action-system-fracas"><a class="header" href="#failure-reporting-analysis-and-corrective-action-system-fracas">Failure Reporting, Analysis, and Corrective Action System (FRACAS)</a></h1>
<blockquote>
<p>FRACAS is a defined system or process for reporting, classifying, and analyzing failures and planning corrective actions for such shortcomings. Keeping a history of analyses and actions taken is part of the process.</p>
</blockquote>
<p>The FRACAS process is cyclical and follows the adapted FRACAS Kaizen Loop:</p>
<ul>
<li><strong>Failure Mode Analysis</strong>: Analysis of failure modes.</li>
<li><strong>Failure Codes Creation</strong>: Creation of failure codes or the methodology for classifying them.</li>
<li><strong>Work Order History Analysis</strong>: Analysis of the history of tickets sent to the data team.</li>
<li><strong>Root Cause Analysis</strong>: Analysis of root causes.</li>
<li><strong>Strategy Adjustment</strong>: Strategy adjustment.</li>
</ul>
<h2 id="implementation"><a class="header" href="#implementation">Implementation</a></h2>
<p>Implementing this process involves automating the analysis of data process logs, commits, pull requests, and tickets. In the context of data reliability engineering, implementing it involves establishing a structured approach to systematically identifying, analyzing, and resolving data-related failures.</p>
<p>Here's how it can be adapted and adopted:</p>
<ol>
<li><strong>Failure Identification</strong></li>
</ol>
<ul>
<li>
<p><strong>Automated Monitoring</strong>:
Use observability and monitoring tools to detect anomalies, failures, or performance issues in data pipelines, databases, or data processing tasks automatically. Configure all data tools to collect and send metrics.</p>
</li>
<li>
<p><strong>Alerting Mechanisms</strong>:
Set up alerts to notify relevant teams or individuals when potential data issues are detected, ensuring prompt attention.</p>
</li>
</ul>
<ol start="2">
<li><strong>Reporting</strong></li>
</ol>
<ul>
<li>
<p><strong>Centralized Reporting Platform</strong>:
Implement a system to report, document, and track all identified issues. This platform should capture details about the failure, including when it occurred, its impact, and any immediate observations.</p>
</li>
<li>
<p><strong>User Reporting</strong>:
Encourage users and stakeholders to report data discrepancies or issues, providing a clear and straightforward mechanism.</p>
</li>
</ul>
<ol start="3">
<li><strong>Analysis</strong></li>
</ol>
<ul>
<li>
<p><strong>Root Cause Analysis</strong>:
For each reported failure, conduct a thorough analysis to determine the underlying cause. This might involve reviewing data logs, pipeline configurations, or recent changes to the data systems.</p>
</li>
<li>
<p><strong>Collaboration</strong>:
Involve cross-functional teams in the analysis to gain diverse perspectives, especially when dealing with complex data ecosystems.</p>
</li>
</ul>
<ol start="4">
<li><strong>Corrective Actions</strong></li>
</ol>
<ul>
<li>
<p><strong>Develop Solutions</strong>:
Based on the root cause analysis, develop appropriate solutions to address the identified issues. This could range from fixing data quality errors to redesigning aspects of the data pipeline for greater resilience.</p>
</li>
<li>
<p><strong>Implement Changes</strong>:
Roll out the corrective measures, ensuring that changes are tested and monitored to confirm they effectively resolve the issue.</p>
</li>
</ul>
<ol start="5">
<li><strong>Follow-Up</strong></li>
</ol>
<ul>
<li>
<p><strong>Verification</strong>:
After implementing corrective actions, verify that the issue has been resolved and that the solution hasn't introduced new problems.</p>
</li>
<li>
<p><strong>Documentation</strong>:
Document the issue, the analysis process, the corrective action taken, and the implementation results for future reference.</p>
</li>
</ul>
<ol start="6">
<li><strong>Continuous Improvement</strong></li>
</ol>
<ul>
<li>
<p><strong>Feedback Loop</strong>:
Use insights gained from FRACAS to identify areas for improvement in data processes and systems, aiming to prevent similar issues from occurring in the future.</p>
</li>
<li>
<p><strong>Training and Knowledge Sharing</strong>:
Share lessons learned from failure analyses and corrective actions with the broader team to build a continuous learning and improvement culture.</p>
</li>
</ul>
<h3 id="notes-on-failure-identification-and-reporting-steps"><a class="header" href="#notes-on-failure-identification-and-reporting-steps">Notes on <em>Failure Identification</em> and <em>Reporting</em> Steps</a></h3>
<p>These steps can be done through automated monitoring tools that alert the team to issues such as failed ETL jobs, discrepancies in data validation checks, or performance bottlenecks.</p>
<h3 id="notes-on-analysis-steps"><a class="header" href="#notes-on-analysis-steps">Notes on <em>Analysis</em> Steps</a></h3>
<p>Once a failure is reported, it is analyzed to understand its nature, scope, and impact. This involves digging into logs, reviewing the data processing steps where the failure occurred, and identifying the specific point of failure. The analysis aims to classify the failure (e.g., data corruption, process failure, infrastructure issue) and understand the underlying reasons for the failure.</p>
<h3 id="notes-on-corrective-action-steps"><a class="header" href="#notes-on-corrective-action-steps">Notes on <em>Corrective Action</em> Steps</a></h3>
<p>Based on the analysis, corrective actions are determined and implemented to fix the immediate issue. This could involve rerunning a failed job with corrected parameters, fixing a bug in the data transformation logic, or updating data validation rules to catch similar issues in the future.</p>
<h3 id="notes-on-follow-up-steps"><a class="header" href="#notes-on-follow-up-steps">Notes on <em>Follow-Up</em> Steps</a></h3>
<p>All steps of the FRACAS process, from initial failure reporting to final corrective actions and system improvements, are documented. This documentation serves as a knowledge base for the data engineering team, helping them understand common failure modes, effective corrective actions, and best practices for designing more reliable data systems.</p>
<h3 id="notes-on-continuous-improvement-steps"><a class="header" href="#notes-on-continuous-improvement-steps">Notes on <em>Continuous Improvement</em> Steps</a></h3>
<p>Beyond immediate corrective actions, FRACAS also focuses on systemic improvements to prevent similar failures from occurring. This could involve redesigning parts of the data pipeline for greater resilience, adding additional checks and balances in data validation, improving data quality monitoring, or enhancing the infrastructure for better performance and reliability.</p>
<p>FRACAS is an iterative process. The learnings from each incident are fed back into the data engineering processes, leading to continuous improvement in data pipeline reliability and efficiency. Over time, this reduces the incidence of failures and improves the overall quality and trustworthiness of the data.</p>
<h2 id="tools-and-integration"><a class="header" href="#tools-and-integration">Tools and Integration</a></h2>
<p>Integrate FRACAS with existing data management and DevOps tools to streamline the workflow. This integration can range from linking FRACAS with project management tools to automating specific steps in the process using scripts or bots.</p>
<p>Implementing FRACAS in data reliability engineering helps resolve data issues more effectively and contributes to building a more reliable, resilient, and high-quality data infrastructure over time.</p>
<h2 id="use-case"><a class="header" href="#use-case">Use Case</a></h2>
<p>Although complete use cases will be explored in the book's next section, here's a small use case to understand the implementation and importance of FRACAS.</p>
<h3 id="background"><a class="header" href="#background">Background:</a></h3>
<p>A mature startup, "PaTech," has a complex data ecosystem with Airflow orchestrating ELT jobs via Airbyte, ETL processes through dbt models deployed in Kubernetes, and various data quality and observability tools like DataDog in place. Hundreds of engineers access the company's data lake and warehouse, while its data marts serve thousands of employees across all departments.</p>
<h3 id="challenge"><a class="header" href="#challenge">Challenge:</a></h3>
<p>Despite having advanced tools and processes, DataTech Innovations faces recurring data issues affecting data quality and availability, leading to decision-making delays and decreased trust in data systems.</p>
<h3 id="fracas-implementation"><a class="header" href="#fracas-implementation">FRACAS Implementation</a></h3>
<ol>
<li>
<p><strong>Failure Identification</strong>:
An anomaly detected by DataDog in the data warehouse triggers an alert. The issue involves a significant discrepancy in sales data reported by the ETL process, impacting downstream data marts and reports.</p>
</li>
<li>
<p><strong>Initial Reporting</strong>:
The alert automatically generates a Jira ticket, categorizing the issue as a high-priority data quality incident. The ticket includes initial diagnostic information from DataDog and Airflow logs.</p>
</li>
<li>
<p><strong>Data Collection and Analysis</strong>:
The data reliability engineering team, using logs from Airflow and Airbyte, identifies that a recent schema change in the source CRM system wasn't reflected in the ELT job, leading to incomplete sales data extraction.</p>
</li>
<li>
<p><strong>Root Cause Analysis (RCA)</strong>:
Further investigation reveals that the change notification from the CRM team was overlooked due to a communication gap, preventing the necessary adjustments in the ELT job.</p>
</li>
<li>
<p>Corrective Actions:</p>
</li>
</ol>
<ul>
<li><strong>Immediate</strong>:
The ELT job is temporarily halted, and the schema change is manually incorporated to restore the integrity of the sales data. The corrected data is re-processed, and the affected data marts and reports are updated.</li>
<li><strong>Systemic</strong>:
The team implements a new protocol for schema change notifications, including automated alerts and a checklist in the Airflow job deployment process to verify source system schemas.</li>
</ul>
<ol start="6">
<li><strong>Preventive Measures</strong>:</li>
</ol>
<ul>
<li>Introducing automated schema detection and validation in Airbyte to flag discrepancies before data extraction.</li>
<li>Establishing a cross-functional data schema change committee to ensure all schema changes are reviewed and communicated effectively across teams.</li>
</ul>
<ol start="7">
<li>
<p><strong>Documentation and Knowledge Sharing</strong>:
The incident, RCA, corrective, and preventive measures are documented in the company's knowledge base. A company-wide presentation is conducted to share learnings, emphasizing the importance of communication and automated checks in preventing similar incidents.</p>
</li>
<li>
<p><strong>Monitoring and Review</strong>:
DataDog alerts are fine-tuned to detect similar anomalies more effectively. The effectiveness of the new schema change protocol and automated checks are monitored over the next quarter to ensure no repeat incidents.</p>
</li>
</ol>
<h3 id="outcome"><a class="header" href="#outcome">Outcome</a></h3>
<p>By implementing FRACAS, PaTech resolves the immediate data discrepancy issue and strengthens its data reliability framework, reducing the likelihood of similar failures. The incident fosters a culture of continuous improvement and cross-departmental collaboration, enhancing overall data trustworthiness and decision-making efficiency across the organization.</p>
<h2 id="final-thoughts"><a class="header" href="#final-thoughts">Final Thoughts</a></h2>
<p>By applying FRACAS, data teams can move from reactive problem-solving to a proactive stance on improving data systems' reliability and efficiency, ultimately supporting better decision-making and operational performance across the organization.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="corrective-actions"><a class="header" href="#corrective-actions">Corrective Actions</a></h1>
<h2 id="corrective-action-and-preventive-action-process-capa--corrective-action-process-cap"><a class="header" href="#corrective-action-and-preventive-action-process-capa--corrective-action-process-cap">Corrective Action and Preventive Action Process (CAPA) &amp; Corrective Action Process (CAP)</a></h2>
<blockquote>
<p>As part of the Corrective Action and Preventive Action Process (CAPA), the Corrective Action Process (CAP) aims to identify failures, determine their root causes, and take corrective actions. This process also involves implementing preventive measures to avoid the recurrence of the same failure for the same reasons. You can find the complete definition in ISO 9001.</p>
</blockquote>
<p>Different tools and techniques are used for their application in various industries, such as PDCA (Plan, Do, Check, Act), DMAIC (Define, Measure, Analyze, Improve, Control), 8D, etc. Typically, any tool, technique, or methodology is summarized in ISO 9001 in seven "steps":</p>
<ol>
<li><strong>Define the problem</strong>. This step involves confirming the problem is real and identifying the Who, What, When, Where, and Why. This step should be automated as much as possible, with the failure detected through sensors.</li>
<li><strong>Define the scope</strong>. It involves measuring the problem to be solved, knowing its frequency, which processes or tasks it affects, and which stakeholders are impacted. For data processes, many scope details should already be known from the design of the processes and tasks, and the frequency can be determined from observability and FRACAS processes.</li>
<li><strong>Containment actions</strong>. These are specific measures adopted for the shortest possible time while working on a definitive solution to the failure. Such measures should already be designed in advance for each task or sub-task. The selection of measures should be automated, or if not, they should be implemented immediately.</li>
<li><strong>Root cause identification</strong>. A clear, precise, and comprehensive failure diagnosis. Its documentation is part of the FRACAS.</li>
<li><strong>Corrective action planning</strong>. Plan corrective actions based explicitly on the root cause.</li>
<li><strong>Implementation of corrective actions</strong>. This involves the final implementation of corrective actions in the process, which should automatically be available when similar failures occur.</li>
<li><strong>Follow-up on results</strong>. Documentation, communication, complete FRACAS.</li>
</ol>
<p>Corrective Actions in data engineering involve identifying, addressing, and mitigating the root causes of identified problems within data processes and systems to prevent their recurrence. This systematic approach is crucial for maintaining the integrity, reliability, and efficiency of data operations. Here's how Corrective Actions can be applied in data engineering:</p>
<h3 id="identification-of-issues"><a class="header" href="#identification-of-issues">Identification of Issues</a></h3>
<p>The first step in the Corrective Action process is accurately identifying issues within data systems. This could range from data quality problems, pipeline failures, and performance bottlenecks to security vulnerabilities. Automated monitoring tools, data quality frameworks, and alerting systems are vital in early detection.</p>
<h3 id="root-cause-analysis-rca"><a class="header" href="#root-cause-analysis-rca">Root Cause Analysis (RCA)</a></h3>
<p>Once an issue is identified, a thorough Root Cause Analysis is conducted to understand the underlying cause of the problem. Techniques such as the Five Whys, fishbone diagrams, or Pareto analysis can be employed. For instance, if a data pipeline fails frequently due to specific data format inconsistencies, RCA would seek to uncover why these inconsistencies occur.</p>
<h3 id="planning-corrective-actions"><a class="header" href="#planning-corrective-actions">Planning Corrective Actions</a></h3>
<p>Based on the RCA findings, a corrective action plan is developed. This plan outlines the steps needed to address the root cause of the problem. In the data pipeline example, if the root cause is incorrect data formatting at the source, a corrective action could involve, for example, implementing stricter data validation checks at the data ingestion stage.</p>
<h3 id="implementation-of-corrective-actions"><a class="header" href="#implementation-of-corrective-actions">Implementation of Corrective Actions</a></h3>
<p>The planned corrective actions are then implemented. This might involve modifying data validation rules, updating ETL scripts, enhancing data quality checks, or even redesigning parts of the data pipeline for better error handling and resilience.</p>
<h3 id="verification-and-monitoring"><a class="header" href="#verification-and-monitoring">Verification and Monitoring</a></h3>
<p>After the corrective actions are implemented, verifying their effectiveness in resolving the issue and monitoring the system for unintended consequences is crucial. This could involve running test cases, monitoring data pipeline runs for a certain period, or employing data quality dashboards to ensure the issue does not recur.</p>
<h3 id="documentation-and-knowledge-sharing"><a class="header" href="#documentation-and-knowledge-sharing">Documentation and Knowledge Sharing</a></h3>
<p>All steps taken, from issue identification to implementing corrective actions and their outcomes, should be thoroughly documented. This documentation is a knowledge base for future reference and helps share learnings across the data engineering team and organization. It contributes to building a culture of continuous improvement.</p>
<h3 id="preventive-measures"><a class="header" href="#preventive-measures">Preventive Measures</a></h3>
<p>Beyond addressing the immediate issue, the insights gained during the corrective action process can inform preventive measures to avoid similar problems. This might include revising data handling policies, enhancing training for data engineers, or adopting new tools and technologies for better data management.</p>
<p>In data engineering, Corrective Actions are about fixing problems and improving processes and systems for long-term reliability and efficiency. By systematically addressing the root causes of issues, data teams can enhance the quality, security, and performance of their data infrastructure, supporting better decision-making and operational outcomes across the organization.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reliability-block-diagrams"><a class="header" href="#reliability-block-diagrams">Reliability Block Diagrams</a></h1>
<blockquote>
<p>Reliability Block Diagrams (RBD) are a method for diagramming and identifying how the reliability of components (or subsystems) <em>R(t)</em> contributes to the success or failure of a redundancy. This method can be used to design and optimize components and select redundancies, aiming to lower failure rates.</p>
</blockquote>
<p>An RBD is a series of connected blocks (in series, parallel, or a combination thereof), indicating redundant components, the type of redundancy, and their respective failure rates.</p>
<p>The diagram displays the components that failed and the ones that did not. If it is possible to identify a path between the beginning and end of the process with components that did not fail, it can be concluded that the process can be successfully executed.</p>
<p>Each RBD should include statements listing all relationships between components, i.e., what conditions led to using one component over another in the process execution.</p>
<h2 id="rbd-implementation-in-data-engineering"><a class="header" href="#rbd-implementation-in-data-engineering">RBD Implementation in Data Engineering</a></h2>
<p>RBDs can be particularly useful in data engineering to ensure the reliability and availability of data pipelines and storage systems. Here's how RBDs could be applied in the context of data engineering:</p>
<h3 id="designing-data-pipelines"><a class="header" href="#designing-data-pipelines">Designing Data Pipelines</a></h3>
<p>Data pipelines include stages like data collection, processing, transformation, and loading (ETL processes). An RBD can represent each stage as a block, with connections illustrating the data flow. This helps identify critical components whose failure could disrupt the entire pipeline, allowing engineers to implement redundancy or failovers specifically for those components.</p>
<h3 id="infrastructure-reliability"><a class="header" href="#infrastructure-reliability">Infrastructure Reliability</a></h3>
<p>In data engineering, the infrastructure includes databases, servers, network components, and storage systems. An RBD can help visualize the relationship between these components and their impact on overall system reliability. For example, a database cluster might be set up with redundancy to ensure that the failure of a single node doesn't result in data loss or downtime, represented in an RBD by parallel blocks for each redundant component.</p>
<h3 id="dependency-analysis"><a class="header" href="#dependency-analysis">Dependency Analysis</a></h3>
<p>RBDs can help data engineers understand how different data sources and processes depend on each other. For instance, if a data pipeline relies on multiple external APIs or data sources, the RBD can illustrate these dependencies, highlighting potential points of failure if one of the external sources becomes unreliable.</p>
<h3 id="optimizing-redundancies"><a class="header" href="#optimizing-redundancies">Optimizing Redundancies</a></h3>
<p>By using RBDs, data engineers can identify areas where redundancies are necessary to maintain data availability and system performance. This is crucial for critical systems where data must be available at all times. For example, in a data replication strategy, the RBD can help determine the number of replicas needed to achieve the desired level of reliability.</p>
<h3 id="failure-mode-analysis"><a class="header" href="#failure-mode-analysis">Failure Mode Analysis</a></h3>
<p>RBDs allow for the identification of single points of failure within the system. Understanding how individual components contribute to the overall system reliability enables data engineers to prioritize efforts in mitigating risks, such as adding backups, introducing data validation steps, or improving error-handling mechanisms.</p>
<h3 id="scalability-and-maintenance-planning"><a class="header" href="#scalability-and-maintenance-planning">Scalability and Maintenance Planning</a></h3>
<p>As data systems scale, RBDs can be updated to reflect new components and dependencies, helping engineers plan for maintenance and scalability while minimizing the impact on reliability. This foresight ensures the system can grow without compromising performance or data integrity.
In summary, Reliability Block Diagrams offer a systematic approach for data engineers to design, analyze, and optimize data systems for reliability. By visualizing component dependencies and identifying critical points of failure, RBDs facilitate informed decision-making to enhance system robustness and ensure continuous data availability.</p>
<p>In summary, Reliability Block Diagrams offer a systematic approach for data engineers to design, analyze, and optimize data systems for reliability. By visualizing component dependencies and identifying critical points of failure, RBDs facilitate informed decision-making to enhance system robustness and ensure continuous data availability.</p>
<h2 id="rbd-implementation-in-data-reliability-engineering"><a class="header" href="#rbd-implementation-in-data-reliability-engineering">RBD Implementation in Data Reliability Engineering</a></h2>
<p>While data engineering primarily uses Reliability Block Diagrams (RBDs) to design and detail the individual tasks within data pipelines, data reliability engineering adopts RBDs to assess and enhance the overall system's robustness. In the data reliability context, RBDs extend beyond the pipeline to encompass the entire data ecosystem, including data sources, storage, and processing components, focusing on how these elements collectively contribute to the system's reliability and pinpointing potential vulnerabilities that could impact data integrity and availability.</p>
<h3 id="component-identification"><a class="header" href="#component-identification">Component Identification</a></h3>
<p>Start by identifying all critical components of your data ecosystem that contribute to the overall reliability of data services. This includes data ingestion mechanisms, transformation processes (like ETL/ELT jobs), data storage systems (databases, data lakes, data warehouses), data processing applications, and data access layers.</p>
<h3 id="diagram-construction"><a class="header" href="#diagram-construction">Diagram Construction</a></h3>
<p>Construct the RBD by representing each identified component as a block. The arrangement of these blocks should reflect the logical relationship and dependencies between components, with connections indicating the data flow. For example, an ETL job block might be connected to both a source database block and a data warehouse block, showing the data flow from source to target.</p>
<h3 id="reliability-representation"><a class="header" href="#reliability-representation">Reliability Representation</a></h3>
<p>Assign reliability values to each block based on historical performance data, such as uptime, failure rates, or mean time between failures (MTBF). These values can be derived from monitoring and logging tools, past incident reports, or vendor specifications for managed services.</p>
<h3 id="analysis"><a class="header" href="#analysis">Analysis</a></h3>
<p>Use the RBD to analyze the overall system reliability. This can involve calculating the reliability of serial and parallel configurations within the diagram. The system's reliability is the product of the individual reliabilities for serial configurations (where components depend on each other). For parallel configurations (where components can compensate for each other's failure), the system's reliability is enhanced and requires a different calculation approach.</p>
<h3 id="identification-of-weak-points"><a class="header" href="#identification-of-weak-points">Identification of Weak Points</a></h3>
<p>The RBD can help identify system parts that significantly impact overall reliability. Components with lower reliability values or critical single points of failure become evident, guiding where improvements or redundancies are needed.</p>
<h3 id="redundancy-planning"><a class="header" href="#redundancy-planning">Redundancy Planning</a></h3>
<p>Based on the analysis, plan for redundancy and fault tolerance in critical components. For example, if a data storage system is identified as a weak point, consider introducing replication or a failover system to enhance reliability.</p>
<h3 id="continuous-improvement"><a class="header" href="#continuous-improvement">Continuous Improvement</a></h3>
<p>As the data system evolves, continuously update the RBD to reflect changes and improvements. Regularly revisiting the RBD can help maintain an up-to-date understanding of the system's reliability and make informed decisions about further enhancements.</p>
<h3 id="example-use-case"><a class="header" href="#example-use-case">Example Use Case</a></h3>
<p>Imagine a data platform where raw data is ingested from various sources into a data lake, processed through a series of transformation jobs in Apache Spark, and then loaded into a data warehouse for analytics. An RBD for this platform would include blocks for each data source, the data lake, Spark jobs, and the data warehouse. By analyzing the RBD, the data reliability engineering team might find that the transformation jobs are a reliability bottleneck. To address this, they could introduce redundancy by parallelizing the Spark jobs across multiple clusters, thereby enhancing the overall reliability of the data platform.</p>
<h3 id="example-diagram"><a class="header" href="#example-diagram">Example Diagram</a></h3>
<p align="center">
  <img src="concepts/systems-reliability/../../assets/concepts/systems-reliability/rbd_v1.svg" alt="RBD Example">
</p>
<p>Reliability Block Diagrams offer a systematic approach to understanding and improving the reliability of data systems, making them a valuable tool in the arsenal of data reliability engineering.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-quality-2"><a class="header" href="#data-quality-2">Data Quality</a></h1>
<blockquote>
<p>Data quality refers to how well-suited data is for its intended use, focusing on aspects like accuracy, completeness, and consistency. In data reliability engineering, data quality is crucial because it ensures that the data systems an organization relies on are dependable and can support accurate decision-making and efficient operations.</p>
</blockquote>
<p>For those interested in data reliability engineering, understanding data quality is essential. High-quality data leads to reliable systems that businesses can trust for their critical operations and strategic decisions. This chapter will dive into the practical side of maintaining and improving data quality, making it a key skill set for data professionals.</p>
<p>We'll cover important topics like master data management, which helps keep data consistent across the organization, and data governance, ensuring data remains accurate and secure. We'll also look at different data quality models that provide frameworks for assessing and improving data quality. These topics are geared towards giving you actionable insights and tools to enhance the reliability of your data systems.</p>
<p>The goal of this chapter is to bridge the gap between theoretical data quality concepts and their practical application in data reliability engineering, providing actionable insights for improving data systems' robustness and dependability and introducing a variety of data quality models, standards, and best practices, enabling data professionals to assess, monitor, and enhance the quality of data within their organizations, thus contributing to overall system reliability.</p>
<p>The topics in this chapter on Data Quality are based on ideas from the book "Calidad de Datos" (Data Quality) by Ismael Caballero Muñoz-Reja and others. The book is published by "Ediciones de la U" and "Ra-Ma". We chose to follow this book's approach to make sure we cover data quality thoroughly and in a way that's useful for Data Reliability Engineering. This way, we're using trusted information from experts to help you understand data quality clearly and systematically.</p>
<p>As a very special note, this chapter mentions a lot the term <strong>Data Reliability</strong>, which is not the same as <strong>Data Reliability Engineering</strong>. Data reliability refers to the trustworthiness and dependability of data, while data reliability engineering is the practice of designing, implementing, and maintaining systems and processes to ensure data remains reliable. Both terms were oversimplified here, but both will be explored further in the book.</p>
<p>This chapter is divided into five parts:</p>
<p><a href="concepts/./data-quality/foundations.html"><strong>Foundations of Data Quality</strong></a></p>
<blockquote>
<p>This section explains how governance, data management, and data quality management differ and work together, highlighting their importance in aligning with ISO/IEC 38500 standards to meet organizational goals and manage data risks efficiently. We'll also explore the concept of data lifecycle.</p>
</blockquote>
<p><a href="concepts/./data-quality/master_data.html"><strong>Master Data</strong></a></p>
<blockquote>
<p>Master data is the core information an organization uses across its systems, and master data management is the process of organizing, securing, and maintaining this information to ensure it's accurate and consistent. This section explores entities resolution, master data architecture, maturity models, and standards.</p>
</blockquote>
<p><a href="concepts/./data-quality/management.html"><strong>Data Management</strong></a></p>
<blockquote>
<p>Here we'll explore various frameworks and models that guide how organizations can systematically improve the handling and quality of their data. Including DAMA DMBOK, Aiken's Model, Data Management Maturity Model (DMM), Gartner's Model, Total Quality Data Management (TQDM), Data Management Capability Assessment Model (DCAM), and the Model for Assessing Data Management (MAMD).</p>
</blockquote>
<p><a href="concepts/./data-quality/models.html"><strong>Data Quality Models</strong></a></p>
<blockquote>
<p>Data Quality Models are fundamental frameworks that define, measure, and evaluate the quality of data within an organization. Here we'll explore various criteria, known as dimensions, that help evaluate and enhance the quality of organizational data.</p>
</blockquote>
<p><a href="concepts/./data-quality/final_thoughts.html"><strong>Final Thoughts on Data Quality</strong></a></p>
<blockquote>
<p>This section emphasizes that good data quality, covering aspects like accuracy and completeness, is essential for data reliability and underlies trustworthy business decisions, with a focus on proactive measures to ensure data integrity during integration, influenced by solid data architecture and metadata management.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="foundations-of-data-quality"><a class="header" href="#foundations-of-data-quality">Foundations of Data Quality</a></h1>
<h2 id="data-lifecycle"><a class="header" href="#data-lifecycle">Data Lifecycle</a></h2>
<h3 id="dama"><a class="header" href="#dama">DAMA</a></h3>
<p>The Data Management Association International (DAMA) provides a comprehensive framework for understanding and managing the data lifecycle within organizations. This lifecycle encompasses all stages through which data passes, from its initial creation or capture to its eventual archiving or deletion. DAMA emphasizes the importance of managing each stage with best practices to ensure the overall quality and reliability of data.</p>
<h3 id="posmad-data-flow-model"><a class="header" href="#posmad-data-flow-model">POSMAD Data Flow Model</a></h3>
<p>The POSMAD model, which stands for Plan, Obtain, Store, Maintain, Apply, and Dispose, offers a structured approach to managing the data lifecycle:</p>
<ol>
<li>
<p><strong>Plan</strong>:
Define the objectives and requirements for data collection, including what data is needed, for what purpose, and how it will be managed throughout its lifecycle.</p>
</li>
<li>
<p><strong>Obtain</strong>:
Acquire data from various sources, ensuring that the data collection methods maintain the integrity and quality of the data.</p>
</li>
<li>
<p><strong>Store</strong>:
Securely store the data in a manner that maintains its accuracy, accessibility, and compliance with any regulatory requirements.</p>
</li>
<li>
<p><strong>Maintain</strong>:
Regularly update and cleanse the data to ensure it remains accurate, relevant, and of high quality over time.</p>
</li>
<li>
<p><strong>Apply</strong>:
Utilize the data in analyses, decision-making processes, or operational workflows, applying it in a way that maximizes its value and utility.</p>
</li>
<li>
<p><strong>Dispose</strong>:
When data is no longer needed or has reached the end of its useful life, it should be securely archived or destroyed per data governance policies and regulatory requirements.</p>
</li>
</ol>
<p>Understanding and managing the data lifecycle is crucial for data teams to ensure that the data they work with is accurate, timely, and relevant. Each stage of the POSMAD model presents opportunities to enhance data quality and mitigate risks associated with data mismanagement. For instance, during the "Maintain" stage, data teams can implement quality checks and balances to correct any inaccuracies, ensuring the data's reliability for downstream applications.</p>
<p>The data lifecycle directly influences the design and structure of an organization's data architecture. Data architecture must accommodate the requirements of each lifecycle stage, providing the necessary infrastructure, tools, and processes to support data collection, storage, maintenance, and usage. For example, the "Store" stage necessitates a robust data storage solution that can handle the volume, velocity, and variety of data, while ensuring its accessibility and security.</p>
<p>The management of the data lifecycle, as outlined by DAMA and the POSMAD model, is inherently tied to data reliability. Each stage of the lifecycle offers a checkpoint for ensuring data quality and integrity, which are foundational to data reliability. By adhering to best practices throughout the data lifecycle, data teams can significantly reduce the risk of data errors, inconsistencies, and losses, thereby enhancing the overall reliability of data systems and the insights derived from them.</p>
<p>In summary, a thorough understanding and management of the data lifecycle, from the perspective of DAMA and the POSMAD model, are essential for maintaining data quality and reliability. It ensures that data remains a valuable asset for the organization, supporting informed decision-making and efficient operations.</p>
<h3 id="cobit"><a class="header" href="#cobit">COBIT</a></h3>
<p>The data lifecycle according to the COBIT (Control Objectives for Information and Related Technologies) framework involves a structured approach to managing and governing information and technology in an enterprise. COBIT's perspective on the data lifecycle focuses on governance and management practices that ensure data integrity, security, and availability throughout its lifecycle stages. While COBIT does not explicitly define a "data lifecycle" in the same way as DAMA's POSMAD model, its principles and processes can be applied across various stages of data management to enhance data quality and reliability.</p>
<p>Data Lifecycle Stages in the Context of COBIT:</p>
<ol>
<li>
<p><strong>Identification and Classification</strong>:
In this initial stage, data is identified, classified, and categorized based on its importance, sensitivity, and relevance to the business objectives. COBIT emphasizes the need for clear governance structures and policies to manage data effectively from the outset.</p>
</li>
<li>
<p><strong>Acquisition and Creation</strong>:
Data acquisition and creation involve collecting data from various sources and generating new data. COBIT recommends implementing strong control measures and practices to ensure the accuracy, completeness, and reliability of the collected and created data.</p>
</li>
<li>
<p><strong>Storage and Organization</strong>:
Once data is acquired, it needs to be stored securely and organized efficiently. COBIT suggests designing and maintaining data storage solutions that ensure data integrity, confidentiality, and availability, aligning with the enterprise's information security policies.</p>
</li>
<li>
<p><strong>Usage and Processing</strong>:
Data is then used and processed for various business operations, decision-making, and analytics. COBIT advocates for robust IT processes and controls to manage data access, processing, and usage, ensuring that data is utilized effectively and responsibly within the organization.</p>
</li>
<li>
<p><strong>Maintenance and Quality Assurance</strong>:
Regular maintenance, including data cleansing, deduplication, and quality checks, is vital to preserve data quality. COBIT stresses continuous improvement and quality assurance practices to ensure that data remains accurate, relevant, and reliable over time.</p>
</li>
<li>
<p><strong>Archiving and Retention</strong>:
Data that is no longer actively used but needs to be retained for legal, regulatory, or historical reasons is archived. COBIT recommends establishing clear data retention policies and secure archiving solutions that comply with legal and regulatory requirements.</p>
</li>
<li>
<p><strong>Disposal and Destruction</strong>:
Finally, data that is no longer needed or has surpassed its retention period should be securely disposed of or destroyed. COBIT emphasizes the importance of secure data disposal practices to protect sensitive information and ensure compliance with data protection regulations.</p>
</li>
</ol>
<p>For data teams, applying COBIT's governance and management frameworks to the data lifecycle ensures that data handling practices are aligned with broader enterprise governance objectives, enhancing data security, quality, and reliability. By adopting COBIT's principles, data teams can implement structured, standardized processes for managing data, reducing risks, and ensuring that data remains a reliable asset for informed decision-making.</p>
<p>In summary, COBIT's approach to the data lifecycle underscores the importance of governance, risk management, and compliance practices in every stage of data management. By integrating these practices, organizations can enhance the reliability and value of their data, supporting strategic objectives and operational efficiency.</p>
<h2 id="governance-vs-data-management-vs-data-quality-management"><a class="header" href="#governance-vs-data-management-vs-data-quality-management">Governance vs. Data Management vs. Data Quality Management</a></h2>
<p>Understanding the distinctions between governance, data management, and data quality management is crucial for data teams to effectively organize their roles, responsibilities, and processes. Aligning these activities with the ISO/IEC 38500 standards can further ensure that data practices contribute positively to the organization's strategic objectives, manage risks associated with IT and data, and optimize the performance of data and IT resources.</p>
<p>By integrating these frameworks, organizations can create a cohesive and efficient approach to data handling that not only ensures high data quality but also aligns with broader governance goals and compliance requirements, thereby enhancing overall data reliability.</p>
<h3 id="governance"><a class="header" href="#governance">Governance</a></h3>
<p>Data Governance refers to the overarching framework or system of decision rights and accountabilities regarding data and information assets within an organization. It involves setting policies, standards, and principles for data usage, security, and compliance, ensuring that data across the organization is managed as a valuable resource. Governance encompasses the strategies and policies that dictate how data is acquired, stored, accessed, and used, ensuring alignment with business objectives and regulatory requirements.</p>
<h3 id="data-management"><a class="header" href="#data-management">Data Management</a></h3>
<p>Data Management is the implementation of architectures, policies, practices, and procedures that manage the information lifecycle needs of an enterprise. It's more tactical and operational compared to governance and involves the day-to-day activities and technical aspects of handling data, including data architecture, modeling, storage, security, and integration. Data management ensures that data is available, reliable, consistent, and accessible to meet the needs of the organization.</p>
<h3 id="data-quality-management"><a class="header" href="#data-quality-management">Data Quality Management</a></h3>
<p>Data Quality Management (DQM) is a subset of data management focused specifically on maintaining high-quality data throughout the data lifecycle. It involves the processes, methodologies, and systems used to measure, monitor, and improve the quality of data. DQM covers various dimensions of data quality such as accuracy, completeness, consistency, reliability, and timeliness. It includes activities like data profiling, cleansing, validation, and enrichment to ensure that data meets the quality standards set by the organization.</p>
<h3 id="isoiec-38500-family"><a class="header" href="#isoiec-38500-family">ISO/IEC 38500 Family</a></h3>
<p>The ISO/IEC 38500 family provides standards for corporate governance of information technology (IT). It offers guidance to those advising, informing, or assisting directors on the effective and acceptable use of IT within the organization. The ISO/IEC 38500 standards are designed to help organizations ensure that their IT investments are aligned with their business objectives, that IT risks are managed appropriately, and that the organization realizes the full potential of its IT resources.</p>
<p>Key Principles of ISO/IEC 38500:</p>
<ul>
<li><strong>Responsibility</strong>: Everyone in the organization has some responsibility for IT, from top-level executives to end-users.</li>
<li><strong>Strategy</strong>: IT strategy should align with the organization's overall business strategy, supporting its goals and objectives.</li>
<li><strong>Acquisition</strong>: IT acquisitions should be made for valid reasons, with clear and transparent decision-making processes.</li>
<li><strong>Performance</strong>: IT should be used efficiently to deliver value to the organization, with its performance regularly monitored and evaluated.</li>
<li><strong>Conformance</strong>: IT usage should comply with all relevant laws, regulations, and internal policies.</li>
<li><strong>Human Behavior</strong>: IT policies and practices should respect the needs and rights of all stakeholders, including employees, customers, and partners.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="master-data"><a class="header" href="#master-data">Master Data</a></h1>
<p>Master Data refers to the core data within an organization that is essential for its operations and decision-making processes. This data is non-transactional and represents the business's key entities such as customers, products, employees, suppliers, and more. Master data is characterized by its stability and consistency across the organization and is used across various systems, applications, and processes.</p>
<p>Master data is critical because it provides a common point of reference for the organization, ensuring that everyone is working with the same information. Consistency in master data across different business units and systems reduces ambiguity and errors, leading to more accurate analytics, reporting, and business intelligence.</p>
<h2 id="master-data-management-mdm"><a class="header" href="#master-data-management-mdm">Master Data Management (MDM)</a></h2>
<p>Master Data Management (MDM) is a comprehensive method of defining, managing, and controlling master data entities, processes, policies, and governance to ensure that master data is consistent, accurate, and available throughout the organization. MDM involves the integration, cleansing, enrichment, and maintenance of master data across various systems and platforms within the enterprise.</p>
<p>Key Components of MDM:</p>
<ul>
<li><strong>Data Governance</strong>: Establishing policies, standards, and procedures for managing master data, including data ownership, data quality standards, and data security.</li>
<li><strong>Data Stewardship</strong>: Assigning responsibility for managing, maintaining, and ensuring the quality of master data to specific roles within the organization.</li>
<li><strong>Data Integration</strong>: Aggregating and consolidating master data from disparate sources to create a single source of truth.</li>
<li><strong>Data Quality Management</strong>: Implementing processes and tools to ensure the accuracy, completeness, consistency, and timeliness of master data.</li>
<li><strong>Data Enrichment</strong>: Enhancing master data with additional attributes or corrections to increase its value to the organization.</li>
</ul>
<h2 id="resolving-entities"><a class="header" href="#resolving-entities">Resolving Entities</a></h2>
<p>Resolving entities in the context of Master Data and Master Data Management (MDM) is crucial for ensuring consistency, accuracy, and a single source of truth for core business entities such as customers, products, employees, suppliers, etc. Entity resolution involves identifying, linking, and merging records that refer to the same real-world entities across different systems and datasets.</p>
<p>Here's how entity resolution can be approached:</p>
<ol>
<li>
<p><strong>Identification</strong>:
The first step involves identifying potential matches among entities across different systems or datasets. This can be challenging due to variations in data entry, abbreviations, misspellings, or incomplete records. Techniques Used: Pattern matching, fuzzy matching, and using algorithms that can handle variations and typos.</p>
</li>
<li>
<p><strong>Deduplication</strong>:
Deduplication involves removing duplicate records of the same entity within a single dataset or system. This step is crucial to prevent redundancy and ensure each entity is represented once. Techniques Used: Hashing, similarity scoring, and machine learning models to recognize duplicates even when data is not identical.</p>
</li>
<li>
<p><strong>Linking</strong>:
Linking is the process of associating related records across different datasets or systems that refer to the same real-world entity. This step creates a unified view of each entity. Techniques Used: Record linkage techniques, probabilistic matching, and reference matching where a common identifier or set of identifiers is used to link records.</p>
</li>
<li>
<p><strong>Merging</strong>:
Merging involves consolidating linked records into a single, comprehensive record that provides a complete view of the entity. Decisions must be made about which data elements to retain, merge, or discard. Techniques Used: Survivorship rules that define which attributes to keep (e.g., most recent, most complete, source-specific priorities).</p>
</li>
<li>
<p><strong>Data Enrichment</strong>:
After resolving and merging entities, data enrichment can be applied to enhance the master records with additional information from external sources, improving the depth and value of the master data. Techniques Used: Integrating third-party data, leveraging public datasets, and using APIs to fetch additional information.</p>
</li>
<li>
<p><strong>Continuous Monitoring and Updating</strong>:
Entity resolution is not a one-time task. Continuous monitoring and updating are necessary to accommodate new data, changes to existing entities, and evolving relationships among entities. Techniques Used: Implementing feedback loops, periodic reviews, and automated monitoring systems to identify and resolve new or changed entities.</p>
</li>
</ol>
<h2 id="master-data-architecture"><a class="header" href="#master-data-architecture">Master Data Architecture</a></h2>
<p>Master Data Architecture refers to the framework and models used to manage and organize an organization's master data, which typically includes core business entities like customers, products, employees, and suppliers. The architecture aims to ensure that master data is consistent, accurate, and available across the enterprise.</p>
<p>Key Components:</p>
<ul>
<li><strong>Master Data Hub</strong>: A central repository where master data is consolidated, managed, and maintained. It ensures a single source of truth for master data entities across the organization.</li>
<li><strong>Data Integration Layer</strong>: Mechanisms for extracting, transforming, and loading (ETL) data from various source systems into the master data hub. This layer handles data cleansing, deduplication, and standardization.</li>
<li><strong>Data Governance Framework</strong>: Policies, standards, and procedures that govern how master data is collected, maintained, and utilized, ensuring data quality and compliance.</li>
<li><strong>Data Quality Services</strong>: Tools and processes for continuously monitoring and improving the quality of master data, including validation, enrichment, and error correction.</li>
<li><strong>Application Interfaces</strong>: APIs and services that enable other systems and applications within the organization to access and interact with the master data.</li>
</ul>
<h3 id="4-variants-of-master-data-architecture"><a class="header" href="#4-variants-of-master-data-architecture">4 Variants of Master Data Architecture</a></h3>
<p>Jochen and Weisbecker (2014) proposed four variants of master data architecture to address different organizational needs and data management strategies. Each variant offers a unique approach to handling master data, considering factors like centralization, data governance, and system integration. Here's a summary of each:</p>
<ol>
<li><strong>Centralized Master Data Management</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: This architecture involves a single, centralized repository where all master data is stored and managed. It serves as the authoritative source for all master data across the organization.</li>
<li><strong>Advantages</strong>: Ensures consistency and uniformity of master data across the enterprise, simplifies governance, and reduces data redundancy.</li>
<li><strong>Challenges</strong>: Requires significant investment in a centralized system, can lead to bottlenecks, and may be less responsive to local or departmental needs.</li>
</ul>
<ol start="2">
<li><strong>Decentralized Master Data Management</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: In this variant, master data is managed locally within different departments or business units without a central repository. Each unit maintains its master data.</li>
<li><strong>Advantages</strong>: Offers flexibility and allows departments to manage data according to their specific needs and processes, enabling quicker responses to local requirements.</li>
<li><strong>Challenges</strong>: Increases the risk of data inconsistencies across the organization, complicates data integration efforts, and makes enterprise-wide data governance more challenging.</li>
</ul>
<ol start="3">
<li><strong>Registry Model</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: The registry model uses a centralized registry that stores references (links or keys) to master data but not the master data itself. The actual data remains in local systems.</li>
<li><strong>Advantages</strong>: Provides a unified view of where master data is located across the organization without centralizing the data itself, facilitating data integration and consistency checks.</li>
<li><strong>Challenges</strong>: Does not eliminate data redundancies and may require complex synchronization mechanisms to ensure data consistency across systems.</li>
</ul>
<ol start="4">
<li><strong>Hub and Spoke Model</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: This architecture features a central hub where master data is consolidated, synchronized, and distributed to various "spoke" systems throughout the organization.</li>
<li><strong>Advantages</strong>: Balances centralization and decentralization by allowing data to be managed centrally while also supporting local system requirements. It facilitates data sharing and consistency.</li>
<li><strong>Challenges</strong>: Can be complex to implement and maintain, requiring robust integration and data synchronization capabilities between the hub and spoke systems.</li>
</ul>
<p>Each of these master data architecture variants offers distinct benefits and poses unique challenges, making them suitable for different organizational contexts and data management objectives. The choice among these variants depends on factors such as the organization's size, complexity, data governance maturity, and specific business needs.</p>
<h3 id="information-architecture-principles"><a class="header" href="#information-architecture-principles">Information Architecture Principles</a></h3>
<p>Information Architecture (IA) principles guide the design and organization of information to make it accessible and usable. In the context of master data management, these principles help ensure that master data is effectively organized and can support business needs.</p>
<p>Key Principles:</p>
<ul>
<li><strong>Clarity and Understandability</strong>: Information should be presented clearly and understandably, with consistent terminology and categorization that aligns with business operations.</li>
<li><strong>Accessibility</strong>: Master data should be easily accessible to authorized users and systems, with appropriate interfaces and query capabilities.</li>
<li><strong>Scalability</strong>: The architecture should be able to accommodate growth in data volume, variety, and usage, ensuring that it can support future business requirements.</li>
<li><strong>Flexibility</strong>: The architecture should be flexible enough to adapt to changes in business processes, data models, and technology landscapes.</li>
<li><strong>Security and Privacy</strong>: Ensuring that master data is protected from unauthorized access and breaches and that it complies with data protection regulations.</li>
<li><strong>Integration</strong>: The architecture should facilitate the integration of master data with other business processes and systems, ensuring seamless data flow and interoperability.</li>
<li><strong>Data Quality Focus</strong>: A continual emphasis on maintaining and improving the quality of master data through validation, cleansing, and governance practices.</li>
</ul>
<h2 id="master-data-management-maturity-models"><a class="header" href="#master-data-management-maturity-models">Master Data Management Maturity Models</a></h2>
<p>Master Data Management (MDM) maturity models are frameworks that help organizations assess their current state of MDM practices and identify areas for improvement to achieve more effective management of their master data.</p>
<p>MDM maturity models typically outline a series of stages or levels through which an organization progresses as it improves its master data management capabilities. These models often start with an initial stage characterized by ad-hoc and uncoordinated master data efforts and progress through more sophisticated stages involving standardized processes, integrated systems, and eventually, optimized and business-aligned MDM practices.</p>
<p>The levels in an MDM maturity model might include:</p>
<ul>
<li><strong>Initial/Ad-Hoc</strong>: Master data is managed in an uncoordinated way, often within siloed departments.</li>
<li><strong>Repeatable</strong>: Some processes are defined, and there might be local consistency within departments, but efforts are not yet standardized across the organization.</li>
<li><strong>Defined</strong>: Organization-wide standards and policies for MDM are established, leading to greater consistency and control.</li>
<li><strong>Managed</strong>: MDM processes are monitored and measured, and data quality is actively managed across the enterprise.</li>
<li><strong>Optimized</strong>: Continuous improvement processes are in place, and MDM is fully aligned with business strategy, driving value and innovation.</li>
</ul>
<h3 id="loshins-mdm-maturity-model"><a class="header" href="#loshins-mdm-maturity-model">Loshin's MDM Maturity Model</a></h3>
<p>David Loshin's MDM maturity model is particularly insightful because it not only outlines stages of maturity but also focuses on the alignment of MDM processes with business objectives, emphasizing the strategic role of master data in driving business success.</p>
<p>Loshin's model includes the following key stages:</p>
<ul>
<li><strong>Awareness</strong>: The organization recognizes the importance of master data but lacks formal management practices.</li>
<li><strong>Concept/Definition</strong>: Initial efforts to define master data and understand its impact on business processes are undertaken.</li>
<li><strong>Construction and Integration</strong>: Systems and processes are developed for managing master data, with a focus on integrating MDM into existing IT infrastructure.</li>
<li><strong>Operationalization</strong>: MDM processes are put into operation, and the organization starts to see benefits in terms of data consistency and quality.</li>
<li><strong>Governance</strong>: Formal governance structures are established to ensure ongoing data quality, compliance, and alignment with business objectives.</li>
<li><strong>Optimization</strong>: The organization continuously improves its MDM practices, leveraging master data as a strategic asset to drive business innovation and value.</li>
</ul>
<p>Loshin emphasizes the importance of not just the technical aspects of MDM but also the governance, organizational, and strategic components. The model encourages organizations to progress from merely managing data to leveraging it as a key factor in strategic decision-making and business process optimization.</p>
<h2 id="iso-8000"><a class="header" href="#iso-8000">ISO 8000</a></h2>
<p>The ISO 8000 standard series is focused on data quality and master data management, providing guidelines and best practices to ensure that data is accurate, complete, and fit for use in various business contexts. This series covers a wide range of topics related to data quality, from terminology and principles to data provenance and master data exchange.</p>
<p>Let's explore some of the key parts of the ISO 8000 series relevant to Master Data and Data Quality:</p>
<h3 id="iso-8000-100-data-quality-management-principles"><a class="header" href="#iso-8000-100-data-quality-management-principles">ISO 8000-100: Data Quality Management Principles</a></h3>
<blockquote>
<p>This part of the ISO 8000 series outlines the foundational principles for managing data quality; establishing a framework for assessing, improving, and maintaining the quality of data within an organization.</p>
</blockquote>
<h3 id="iso-8000-102-data-quality-provenance"><a class="header" href="#iso-8000-102-data-quality-provenance">ISO 8000-102: Data Quality Provenance</a></h3>
<blockquote>
<p>Focuses on the provenance of data, detailing how to document the source of data and its lineage. This is crucial for understanding the origins of data, assessing its reliability, and ensuring traceability.</p>
</blockquote>
<h3 id="iso-8000-110-syntax-and-semantic-encoding"><a class="header" href="#iso-8000-110-syntax-and-semantic-encoding">ISO 8000-110: Syntax and Semantic Encoding</a></h3>
<blockquote>
<p>Addresses the importance of using standardized syntax and semantics to ensure that data is consistently understood and interpreted across different systems and stakeholders.</p>
</blockquote>
<h3 id="iso-8000-115-master-data-exchange-of-characteristic-data"><a class="header" href="#iso-8000-115-master-data-exchange-of-characteristic-data">ISO 8000-115: Master Data: Exchange of characteristic data</a></h3>
<blockquote>
<p>Provides guidelines for the exchange of master data, particularly focusing on the characteristics of products and services. It emphasizes the standardization of data formats to facilitate accurate and efficient data exchange.</p>
</blockquote>
<h3 id="iso-8000-116-data-quality-information-and-data-quality-vocabulary"><a class="header" href="#iso-8000-116-data-quality-information-and-data-quality-vocabulary">ISO 8000-116: Data Quality: Information and Data Quality Vocabulary</a></h3>
<blockquote>
<p>Defines a set of terms and definitions related to data and information quality, helping organizations to establish a common understanding of key concepts in data quality management.</p>
</blockquote>
<h3 id="iso-8000-120-master-data-quality-prerequisites-for-data-quality"><a class="header" href="#iso-8000-120-master-data-quality-prerequisites-for-data-quality">ISO 8000-120: Master Data Quality: Prerequisites for data quality</a></h3>
<blockquote>
<p>Discusses the prerequisites for achieving high-quality master data, including the establishment of data governance, data quality metrics, and continuous monitoring processes.</p>
</blockquote>
<h3 id="iso-8000-130-data-quality-management-process-reference-model"><a class="header" href="#iso-8000-130-data-quality-management-process-reference-model">ISO 8000-130: Data Quality Management: Process reference model</a></h3>
<blockquote>
<p>Introduces a process reference model for data quality management, outlining the key processes involved in establishing, implementing, maintaining, and improving data quality within an organization.</p>
</blockquote>
<h3 id="iso-8000-140-data-quality-management-assessment-and-measurement"><a class="header" href="#iso-8000-140-data-quality-management-assessment-and-measurement">ISO 8000-140: Data Quality Management: Assessment and measurement</a></h3>
<blockquote>
<p>Focuses on the assessment and measurement of data quality, providing methodologies for evaluating data quality against defined criteria and metrics.</p>
</blockquote>
<h3 id="iso-8000-150-master-data-quality-master-data-quality-assessment-framework"><a class="header" href="#iso-8000-150-master-data-quality-master-data-quality-assessment-framework">ISO 8000-150: Master Data Quality: Master data quality assessment framework</a></h3>
<blockquote>
<p>Offers a comprehensive framework for assessing the quality of master data, including methodologies for evaluating data against specific quality dimensions such as accuracy, completeness, and consistency.</p>
</blockquote>
<h2 id="isoiec-22745"><a class="header" href="#isoiec-22745">ISO/IEC 22745</a></h2>
<p>The ISO/IEC 22745 standard, titled "Industrial automation systems and integration — Open technical dictionaries and their application to master data," is a series of international standards developed to facilitate the exchange and understanding of product data. This standard is particularly significant in the context of industrial automation and integration, providing a framework for creating, managing, and deploying open technical dictionaries. These dictionaries ensure that product data is consistent, interoperable, and can be seamlessly exchanged between different systems and organizations, enhancing data quality and reliability across the supply chain.</p>
<p>ISO/IEC 22745 is crucial for organizations involved in manufacturing, supply chain management, and industrial automation because it standardizes the way product and service data is described, categorized, and exchanged. This standardization supports more efficient procurement processes, reduces the risk of misinterpretation of product data, and enhances interoperability between different IT systems and platforms. By implementing ISO/IEC 22745, organizations can improve the accuracy and reliability of their master data, leading to better decision-making and operational efficiencies.</p>
<h3 id="part-1-overview-and-fundamental-principles"><a class="header" href="#part-1-overview-and-fundamental-principles">Part 1: Overview and Fundamental Principles</a></h3>
<blockquote>
<p>Provides a general introduction to the standard, outlining its scope, objectives, and fundamental principles. It sets the foundation for the development and use of open technical dictionaries.</p>
</blockquote>
<h3 id="part-2-vocabulary"><a class="header" href="#part-2-vocabulary">Part 2: Vocabulary</a></h3>
<blockquote>
<p>Establishes the terms and definitions used throughout the ISO/IEC 22745 series, ensuring a common understanding of key concepts related to open technical dictionaries and master data exchange.</p>
</blockquote>
<h3 id="part-10-exchange-of-characteristic-data-syntax-and-semantic-encoding-rules"><a class="header" href="#part-10-exchange-of-characteristic-data-syntax-and-semantic-encoding-rules">Part 10: Exchange of characteristic data: Syntax and semantic encoding rules</a></h3>
<blockquote>
<p>Specifies the syntax and semantic encoding rules for exchanging characteristic data, ensuring that data exchanged between systems maintains its meaning and integrity.</p>
</blockquote>
<h3 id="part-11-methodology-for-the-development-and-validation-of-open-technical-dictionaries"><a class="header" href="#part-11-methodology-for-the-development-and-validation-of-open-technical-dictionaries">Part 11: Methodology for the development and validation of open technical dictionaries</a></h3>
<blockquote>
<p>Details the methodology for developing and validating open technical dictionaries, including processes for creating, approving, and maintaining dictionary entries.</p>
</blockquote>
<h3 id="part-13-identification-and-referencing-of-requirements-of-product-data"><a class="header" href="#part-13-identification-and-referencing-of-requirements-of-product-data">Part 13: Identification and referencing of requirements of product data</a></h3>
<blockquote>
<p>Focuses on the identification and referencing of product data requirements, providing guidelines for documenting and referencing product specifications and standards.</p>
</blockquote>
<h3 id="part-14-guidelines-for-the-formulation-of-requests-for-master-data"><a class="header" href="#part-14-guidelines-for-the-formulation-of-requests-for-master-data">Part 14: Guidelines for the formulation of requests for master data</a></h3>
<blockquote>
<p>Provides guidelines for formulating requests for master data, ensuring that data requests are clear, structured, and capable of being fulfilled accurately.</p>
</blockquote>
<h3 id="part-20-presentation-of-characteristic-data"><a class="header" href="#part-20-presentation-of-characteristic-data">Part 20: Presentation of characteristic data</a></h3>
<blockquote>
<p>Addresses the presentation of characteristic data, outlining how data should be formatted and displayed to ensure clarity and usability.</p>
</blockquote>
<h3 id="part-30-registration-and-publication-of-open-technical-dictionaries"><a class="header" href="#part-30-registration-and-publication-of-open-technical-dictionaries">Part 30: Registration and publication of open technical dictionaries</a></h3>
<blockquote>
<p>Covers the registration and publication processes for open technical dictionaries, ensuring that dictionaries are accessible, authoritative, and maintained over time.</p>
</blockquote>
<h3 id="part-35-identification-and-referencing-of-terminology"><a class="header" href="#part-35-identification-and-referencing-of-terminology">Part 35: Identification and referencing of terminology</a></h3>
<blockquote>
<p>Discusses the identification and referencing of terminology within open technical dictionaries, ensuring consistent use of terms and definitions.</p>
</blockquote>
<h3 id="part-40-master-data-repository"><a class="header" href="#part-40-master-data-repository">Part 40: Master data repository</a></h3>
<blockquote>
<p>Describes the requirements and structure of a master data repository, a centralized system for storing and managing master data in accordance with the principles of ISO/IEC 22745.</p>
</blockquote>
<h2 id="mdm-tools-implementation-considerations"><a class="header" href="#mdm-tools-implementation-considerations">MDM Tools Implementation Considerations</a></h2>
<p>There are several MDM tools available, including SAP Master Data Governance (MDG), Informatica MDM, IBM InfoSphere MDM, Microsoft SQL Server Master Data Services (MDS), Oracle MDM, Talend MDM, ECCMA, PILOG, TIBCO MDM, Ataccama MDC, VisionWare Multivue MDM, and many others.</p>
<p>When implementing these master data tools, companies typically go through a series of steps including:</p>
<ul>
<li><strong>Assessment</strong>: Evaluating the current state of master data, identifying key data domains, and understanding the data lifecycle.</li>
<li><strong>Strategy Development</strong>: Defining objectives, governance structures, and key performance indicators (KPIs) for the MDM initiative.</li>
<li><strong>Tool Selection</strong>: Choosing an MDM tool that aligns with the company's IT infrastructure, data domains, and business objectives.</li>
<li><strong>Integration</strong>: Integrating the MDM tool with existing systems and data sources to ensure seamless data flow and synchronization.</li>
<li><strong>Data Cleansing and Migration</strong>: Cleaning existing data to remove duplicates and inconsistencies before migrating it into the MDM system.</li>
<li><strong>Governance and Maintenance</strong>: Establishing ongoing data governance practices to maintain data quality, including monitoring, auditing, and updating data as needed.</li>
</ul>
<p>Master data tools are essential for organizations to maintain a "<strong>single source of truth</strong>" for their critical business entities, enabling more informed decision-making, improved customer experiences, and streamlined operations.</p>
<h2 id="using-a-commercial-mdm-tool-vs-building-an-in-house-mdm-service"><a class="header" href="#using-a-commercial-mdm-tool-vs-building-an-in-house-mdm-service">Using a Commercial MDM Tool vs. Building an In-House MDM Service</a></h2>
<p>Deciding between using a commercial Master Data Management (MDM) tool and building an in-house MDM service involves weighing various factors, including cost, scalability, customization, and maintenance. Each approach has its unique set of challenges, advantages, and disadvantages.</p>
<h3 id="using-a-commercial-mdm-tool"><a class="header" href="#using-a-commercial-mdm-tool">Using a Commercial MDM Tool</a></h3>
<p><strong>Pros</strong>:</p>
<ul>
<li><strong>Speed of Deployment</strong>: Commercial MDM tools offer out-of-the-box solutions that can be quickly deployed, allowing organizations to benefit from improved data management in a shorter timeframe.</li>
<li><strong>Proven Reliability</strong>: These tools are developed by experienced vendors, and tested across diverse industries and scenarios, ensuring a level of reliability and robustness.</li>
<li><strong>Support and Updates</strong>: Vendors provide ongoing support, regular updates, and enhancements, which helps keep the MDM system current with the latest data management trends and technologies.</li>
<li><strong>Built-in Best Practices</strong>: Commercial tools often incorporate industry best practices in data governance, data quality, and data integration, reducing the learning curve and implementation risk.</li>
<li><strong>Scalability</strong>: Most commercial MDM solutions are designed to scale with the growth of the business, accommodating increasing data volumes and complexity without significant rework.</li>
</ul>
<p><strong>Cons</strong>:</p>
<ul>
<li><strong>Cost</strong>: Licensing fees for commercial MDM tools can be substantial, especially for large enterprises or when scaling up, and there might be additional costs for support and customization.</li>
<li><strong>Limited Customization</strong>: While these tools offer configuration options, there may be limitations to how much they can be tailored to meet unique business requirements.</li>
<li><strong>Vendor Lock-in</strong>: Relying on a vendor's tool can lead to dependency, making it challenging to switch solutions or integrate with non-supported platforms and data sources in the future.</li>
</ul>
<p><strong>Challenges</strong>:</p>
<ul>
<li>Navigating complex licensing structures and ensuring the tool fits within the budget constraints.</li>
<li>Integrating the MDM tool with legacy systems and diverse data sources.</li>
</ul>
<h3 id="building-an-in-house-mdm-service"><a class="header" href="#building-an-in-house-mdm-service">Building an In-House MDM Service</a></h3>
<p><strong>Pros</strong>:</p>
<ul>
<li><strong>Customization</strong>: Building an MDM service in-house allows for complete customization to the specific needs, processes, and data models of the organization.</li>
<li><strong>Integration</strong>: An in-house solution can be designed to integrate seamlessly with existing systems and data sources, providing a more cohesive data ecosystem.</li>
<li><strong>Control</strong>: Organizations maintain full control over the development, maintenance, and evolution of the MDM service, making it easier to adapt to changing business needs.</li>
</ul>
<p><strong>Cons</strong>:</p>
<ul>
<li><strong>Resource Intensive</strong>: Developing an MDM service requires significant upfront investment in terms of time, skilled personnel, and infrastructure.</li>
<li><strong>Maintenance and Support</strong>: The organization is responsible for ongoing maintenance, updates, and support, which can divert resources from other critical IT functions or business initiatives.</li>
<li><strong>Risk of Obsolescence</strong>: Without continuous investment in keeping the MDM service up-to-date with the latest data management trends and technologies, there's a risk it could become obsolete.</li>
<li><strong>Longer Time to Value</strong>: Designing, developing, and deploying an in-house MDM solution can take considerably longer, delaying the realization of benefits.</li>
</ul>
<p><strong>Challenges</strong>:</p>
<ul>
<li>Ensuring the in-house team has the required expertise in data management best practices, technologies, and regulatory compliance.</li>
<li>Balancing the ongoing resource requirements for development, maintenance, and upgrades of the MDM service.</li>
</ul>
<p>When creating a Master Data Management (MDM) service, organizations need to consider various architectural options to best meet their business requirements, data governance policies, and technical landscape. These options range from centralized systems to more distributed approaches, each with its advantages and challenges. Here are some common MDM architecture options:</p>
<ol>
<li><strong>Centralized MDM Architecture</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: A single, central MDM system serves as the authoritative source for all master data across the organization. All applications and systems that require master data integrate with this central repository.</li>
<li><strong>Pros</strong>: Ensures consistency and a single version of the truth for master data; simplifies governance and data quality management.</li>
<li><strong>Cons</strong>: Can create bottlenecks; may be less responsive to local or department-specific needs; single point of failure risk.</li>
<li><strong>Challenges</strong>: Requires significant upfront investment and effort to integrate disparate systems and data sources.</li>
</ul>
<ol start="2">
<li><strong>Decentralized MDM Architecture</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: Master data is managed locally within different departments or business units, with no overarching central MDM system. Each unit maintains its master data according to its specific needs.</li>
<li><strong>Pros</strong>: Offers flexibility; allows departments to manage data according to their unique requirements; can be quicker to implement within individual departments.</li>
<li><strong>Cons</strong>: Risk of data inconsistencies and duplication across the organization; challenges in achieving a unified view of data; more complex data integration efforts.</li>
<li><strong>Challenges</strong>: Coordinating data governance and ensuring data quality across decentralized systems can be complex.</li>
</ul>
<ol start="3">
<li><strong>Registry MDM Architecture</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: A centralized registry holds references (links or keys) to master data but not the master data itself. Actual data remains in source systems, and the registry provides a unified view.</li>
<li><strong>Pros</strong>: Reduces data redundancy; easier to implement than a fully centralized model; provides a unified view without moving data.</li>
<li><strong>Cons</strong>: Data quality and consistency must still be managed in each source system; requires robust integration and synchronization mechanisms.</li>
<li><strong>Challenges</strong>: Ensuring real-time synchronization and maintaining the accuracy of links or references in the registry.</li>
</ul>
<ol start="4">
<li><strong>Hub and Spoke MDM Architecture</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: Combines elements of centralized and decentralized architectures. A central hub manages core master data, which is then synchronized with "spoke" systems where additional, local master data management may occur.</li>
<li><strong>Pros</strong>: Balances central control with flexibility for local departments; facilitates data sharing and consistency.</li>
<li><strong>Cons</strong>: Complexity in managing and synchronizing data between the hub and spokes; potential for data conflicts between central and local systems.</li>
<li><strong>Challenges</strong>: Designing effective synchronization and conflict resolution mechanisms; managing the scalability of the system.</li>
</ul>
<ol start="5">
<li><strong>Federated MDM Architecture</strong></li>
</ol>
<ul>
<li>**Description: A federated approach integrates multiple MDM systems, each managing master data for specific domains (e.g., customers, products) or regions, without a single central system.</li>
<li><strong>Pros</strong>: Allows specialized management of different data domains; can accommodate different governance models; suitable for large, geographically dispersed organizations.</li>
<li><strong>Cons</strong>: Complex data integration and interoperability challenges; risk of inconsistencies between federated systems.</li>
<li><strong>Challenges</strong>: Ensuring seamless data integration and consistent governance across federated MDM systems.</li>
</ul>
<ol start="6">
<li><strong>Multi-Domain MDM Architecture</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: A single MDM system is designed to manage multiple master data domains (e.g., customers, and products) within one platform, providing a unified approach to managing diverse data types.</li>
<li><strong>Pros</strong>: Simplifies the IT landscape; reduces integration complexity; offers a consistent approach to data governance and quality across domains.</li>
<li><strong>Cons</strong>: Requires a flexible and scalable MDM solution; may be challenging to meet the specific needs of each data domain within a single system.</li>
<li><strong>Challenges</strong>: Balancing the flexibility needed for different data domains with the desire for a unified MDM platform.</li>
</ul>
<h2 id="mdm-ownership"><a class="header" href="#mdm-ownership">MDM Ownership</a></h2>
<p>Responsibility for Master Data Management (MDM) within an organization can vary significantly depending on the company's size, structure, and how data-driven its operations are. Regardless of company size, MDM responsibilities must involve collaboration between IT departments (who understand the technical aspects of data management and integration) and business units (who understand the data's practical use and business implications). This collaborative approach ensures that MDM efforts are aligned with business objectives and that master data is both technically sound and relevant to business needs.</p>
<h3 id="small-companies"><a class="header" href="#small-companies">Small Companies</a></h3>
<p>In smaller companies, MDM responsibilities might fall to a single individual or a small team. This could be the IT Manager, a Data Analyst, or even a Business Manager who has a good understanding of the company's data needs.</p>
<p>A startup with a lean team might have its CTO or a senior developer overseeing MDM as part of its broader responsibilities. They might focus on essential MDM tasks such as defining key data entities and ensuring data quality in critical systems like CRM and ERP.</p>
<h3 id="medium-sized-companies"><a class="header" href="#medium-sized-companies">Medium-sized Companies</a></h3>
<p>As companies grow, they often establish dedicated roles or departments for data management. This might include a Data Manager, MDM Specialist, or a small Data Governance team.</p>
<p>A mid-sized retail company might have an MDM Specialist within the IT department responsible for coordinating master data across various systems like inventory management, customer databases, and supplier information. This role might work closely with department heads to ensure data consistency and quality.</p>
<h3 id="large-enterprises"><a class="header" href="#large-enterprises">Large Enterprises</a></h3>
<p>In large enterprises, MDM is typically a significant function that involves multiple roles and departments. This can include a Chief Data Officer (CDO) at the strategic level, Data Stewards who oversee data quality and compliance in specific domains, and an MDM team that handles the day-to-day management of master data.</p>
<p>A multinational corporation, for example, might have a CDO responsible for the overall data strategy, including MDM. Under the CDO, there might be Data Stewards for different data domains (e.g., customer data, product data) and a dedicated MDM team that works on integrating, cleansing, and maintaining master data across global systems.</p>
<h3 id="industry-specific-considerations"><a class="header" href="#industry-specific-considerations">Industry-specific Considerations</a></h3>
<ul>
<li><strong>Healthcare</strong>: In a hospital or healthcare provider, the responsibility for MDM might fall to a Health Information Manager or a dedicated team within the medical records department, ensuring patient data accuracy across systems.</li>
<li><strong>Finance</strong>: In a bank or financial services firm, MDM might be overseen by a Chief Information Officer (CIO) or a specific data governance committee that ensures compliance with financial regulations and data consistency across customer accounts and transactions.</li>
</ul>
<h2 id="master-data-and-the-data-warehouse"><a class="header" href="#master-data-and-the-data-warehouse">Master Data and the Data Warehouse</a></h2>
<blockquote>
<p>In a data warehouse, master data is often managed through dimension tables. These tables store attributes about the business entities and are used to filter, group, and label data in the warehouse, enabling comprehensive and consistent analytics.</p>
</blockquote>
<p>A data warehouse is a centralized repository designed for query and analysis, integrating data from multiple sources into a consistent format. Master data is critical in a data warehouse to ensure consistency across various subject areas like sales, finance, and customer relations. Master data entities like customers, products, and employees provide a unified reference that ensures different data sources are aligned and can be analyzed together effectively.</p>
<h2 id="master-data-and-the-data-lake"><a class="header" href="#master-data-and-the-data-lake">Master Data and the Data Lake</a></h2>
<blockquote>
<p>Master data in a data lake context is used to tag and organize data, making it searchable and useful for specific business purposes. It can help in categorizing and relating different pieces of data within the lake, ensuring that users can find and interpret the data correctly.</p>
</blockquote>
<p>A data lake is a more extensive repository that stores structured and unstructured data in its native format. While data lakes offer flexibility in handling vast amounts of diverse data, master data is essential for adding structure and meaning to this data, enabling effective analysis and utilization.</p>
<h2 id="master-data-and-data-marts"><a class="header" href="#master-data-and-data-marts">Master Data and Data Marts</a></h2>
<blockquote>
<p>Master data ensures that each data mart, whether for marketing, finance, or operations, uses a consistent definition and format for key business entities. This consistency is crucial for comparing and combining data across different parts of the organization.</p>
</blockquote>
<p>Data marts are subsets of data warehouses designed to meet the needs of specific business units or departments. Master data is vital for data marts to ensure that the data presented is consistent with the enterprise's overall data strategy and with other departments' data marts.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-quality-data-management-and-data-process-quality"><a class="header" href="#data-quality-data-management-and-data-process-quality">Data Quality, Data Management, and Data Process Quality</a></h1>
<blockquote>
<p>These three pillars form the foundation upon which reliable, actionable insights are built, driving business strategies and operational efficiencies. This chapter delves into the core concepts and frameworks that govern these critical areas, exploring established models and methodologies designed to elevate an organization's data capabilities.</p>
</blockquote>
<h3 id="data-quality-the-bedrock-of-trustworthy-data"><a class="header" href="#data-quality-the-bedrock-of-trustworthy-data">Data Quality: The Bedrock of Trustworthy Data</a></h3>
<p>Data quality encompasses the characteristics that determine the <strong>reliability and effectiveness of data</strong>, including <strong>accuracy, completeness, consistency, timeliness, and relevance</strong>. High-quality data is indispensable for accurate analytics, reporting, and business intelligence, directly impacting strategic decisions and operational processes. The pursuit of data quality involves continuous monitoring, cleansing, and validation to ensure data integrity across the data lifecycle.</p>
<h3 id="data-management-the-framework-for-data-excellence"><a class="header" href="#data-management-the-framework-for-data-excellence">Data Management: The Framework for Data Excellence</a></h3>
<p>Data management represents the overarching discipline that encompasses all the processes, policies, practices, and architectures involved in managing an organization's data assets. Effective data management <strong>ensures that data is accessible, secure, usable, and stored efficiently, facilitating its optimal use across the organization</strong>. It covers a wide array of functions, from data governance and data architecture to data security and storage, providing the structure within which data quality initiatives thrive.</p>
<h3 id="data-process-quality-ensuring-operational-efficacy"><a class="header" href="#data-process-quality-ensuring-operational-efficacy">Data Process Quality: Ensuring Operational Efficacy</a></h3>
<p>Data process quality focuses on the <strong>efficiency, reliability, and effectiveness of the processes that create, manipulate, and utilize data</strong>. It involves optimizing data workflows; ensuring that data processing activities like collection, storage, transformation, and analysis are conducted in a manner that upholds data quality and meets business needs. High data process quality minimizes errors, reduces redundancies, and enhances the overall agility and responsiveness of data operations.</p>
<p>The synergy between data quality, data management, and data process quality is undeniable. Robust data management practices provide the foundation for maintaining high data quality, while the quality of data processes ensures that data management and data quality efforts are effectively implemented and sustained. Together, they form a cohesive system that ensures data is a reliable, strategic asset.</p>
<p>This chapter will explore key models and frameworks that guide organizations in enhancing these areas, including:</p>
<ul>
<li><strong>DAMA DMBOK</strong>: A comprehensive guide to data management best practices.</li>
<li><strong>Aiken's Model</strong>: A framework for assessing and improving data process quality.</li>
<li><strong>Data Management Maturity Model (DMM)</strong>: A model for evaluating and enhancing data management practices.</li>
<li><strong>Gartner's Model</strong>: Gartner's insights and methodologies for data management.</li>
<li><strong>TQDM (Total Quality Data Management)</strong>: A holistic approach to integrating quality principles into data management.</li>
<li><strong>DCAM (Data Capability Assessment Model)</strong>: A framework for assessing data management capabilities and maturity.</li>
<li><strong>MAMD Model</strong>: A model focusing on the maturity assessment of data management disciplines.</li>
</ul>
<h2 id="dama-dmbok"><a class="header" href="#dama-dmbok">DAMA DMBOK</a></h2>
<p>The Data Management Association International (DAMA) Data Management Body of Knowledge (DMBOK) is a comprehensive framework that provides standard industry guidelines and best practices for data management. It serves as a definitive guide for data professionals, outlining the processes, policies, and standards that should be implemented to manage data effectively across an organization. The DMBOK covers a wide range of data management areas, aiming to promote high standards of data quality, integrity, and security.</p>
<p>The DAMA Data Management Framework presents a structured approach to managing an organization's data assets, emphasizing the importance of data as a critical resource for business success. The framework is divided into several knowledge areas, each addressing a specific aspect of data management:</p>
<ul>
<li>
<p><strong>Data Governance</strong>: Establishing the policies, standards, and accountability for data management within an organization.</p>
</li>
<li>
<p><strong>Data Architecture</strong>: Defining the structure, integration, and alignment of data assets with business goals.</p>
</li>
<li>
<p><strong>Data Modeling and Design</strong>: Creating data models that ensure data quality and support business processes.</p>
</li>
<li>
<p><strong>Data Storage and Operations</strong>: Managing the storage, maintenance, and support of data in various forms.</p>
</li>
<li>
<p><strong>Data Security</strong>: Ensuring the confidentiality, integrity, and availability of data.</p>
</li>
<li>
<p><strong>Data Integration and Interoperability</strong>: Enabling the seamless sharing and use of data across different systems and platforms.</p>
</li>
<li>
<p><strong>Document and Content Management</strong>: Managing unstructured data, including documents and multimedia content.</p>
</li>
<li>
<p><strong>Reference and Master Data</strong>: Managing key business entities and ensuring consistency across the enterprise.</p>
</li>
<li>
<p><strong>Data Warehousing and Business Intelligence</strong>: Supporting decision-making through the aggregation, analysis, and presentation of data.</p>
</li>
<li>
<p><strong>Metadata Management</strong>: Managing data about data, ensuring that data assets are easily discoverable and understandable.</p>
</li>
<li>
<p><strong>Data Quality Management</strong>: Ensuring that data is accurate, complete, and reliable for business purposes.
Some examples of how the framework can be applied across different industries:</p>
</li>
<li>
<p><strong>Financial Services</strong>: Implementing the Data Governance and Data Security aspects of the DAMA DMBOK to ensure compliance with financial regulations (e.g., GDPR, CCPA, SOX). This includes establishing data governance policies, data stewardship roles, and security measures to protect sensitive financial information.</p>
</li>
<li>
<p><strong>Healthcare</strong>: Applying the Data Quality Management and Metadata Management components of the framework to ensure the accuracy, completeness, and interoperability of patient data. This involves setting data quality standards, implementing data cleansing processes, and managing metadata to support electronic health records (EHR) systems.</p>
</li>
<li>
<p><strong>Retail and E-commerce</strong>: Utilizing the Reference and Master Data, and Data Warehousing and Business Intelligence knowledge areas to manage product information and customer data across multiple channels. This includes standardizing product data, integrating customer data from various touchpoints, and leveraging BI tools for market analysis and personalized marketing.</p>
</li>
<li>
<p><strong>Manufacturing</strong>: Leveraging the Data Integration and Interoperability and Data Modeling and Design parts of the DAMA DMBOK to streamline supply chain operations. This can involve creating data models that reflect the supply chain structure and implementing data integration solutions to ensure seamless data flow between suppliers, manufacturers, and distributors.</p>
</li>
<li>
<p><strong>Public Sector</strong>: Adopting the Data Architecture and Document and Content Management aspects to manage public records, policy documents, and citizen data. This includes designing a data architecture that supports the accessibility and preservation of public records and implementing content management systems for document storage and retrieval.</p>
</li>
<li>
<p>*<strong>Across All Industries</strong>: Establishing a cross-functional data governance committee to oversee the implementation of the DAMA DMBOK framework across the organization. This committee would be responsible for defining data policies, setting data quality standards, and coordinating efforts to improve data management practices in line with the framework.</p>
</li>
</ul>
<h3 id="maturity-model"><a class="header" href="#maturity-model">Maturity Model</a></h3>
<p>The DAMA DMBOK also introduces a Maturity Model to help organizations assess their current data management capabilities and identify areas for improvement. The model outlines different levels of maturity, from initial/ad-hoc processes to optimized and managed data management practices. Organizations can use this model to benchmark their data management practices against industry standards, set realistic goals for improvement, and develop a roadmap for advancing their data management capabilities.</p>
<p>The model consists of 6 levels:</p>
<h4 id="level-0-non-existent"><a class="header" href="#level-0-non-existent">Level 0: Non-existent</a></h4>
<blockquote>
<p>Data management practices are absent or chaotic. There is no formal recognition of the value of data management, leading to inconsistent, unreliable data handling.</p>
</blockquote>
<p>One example is a small startup with no dedicated data management policies or roles, where data is managed ad-hoc by whoever needs it. To advance to the next level of maturity, the company should recognize the value of structured data management and start developing basic data handling policies and procedures.</p>
<h4 id="level-1-initialad-hoc"><a class="header" href="#level-1-initialad-hoc">Level 1: Initial/Ad Hoc</a></h4>
<blockquote>
<p>Some data management activities occur, but they are informal and inconsistent. There's a lack of standardized processes, leading to inefficiencies and data quality issues.</p>
</blockquote>
<p>One example is a growing business where individual departments manage their data independently, resulting in siloed and inconsistent data practices. To advance to the next maturity level, companies should begin to standardize data management practices across projects or teams and appoint individuals responsible for overseeing data quality and consistency.</p>
<h4 id="level-2-repeatable"><a class="header" href="#level-2-repeatable">Level 2: Repeatable</a></h4>
<blockquote>
<p>The organization has developed and applied data management practices that can be repeated across projects or teams. However, these practices may not yet be uniformly enforced or optimized.</p>
</blockquote>
<p>One example is a medium-sized enterprise where certain departments have established successful data management routines that are recognized and beginning to be adopted by other parts of the organization. To advance to the next maturity level, companies should formalize data management practices into documented policies and procedures, ensuring consistency across the organization.</p>
<h4 id="level-3-defined"><a class="header" href="#level-3-defined">Level 3: Defined</a></h4>
<blockquote>
<p>Data management processes are documented, standardized, and integrated into daily operations across the organization. There's a clear understanding of roles and responsibilities related to data management.</p>
</blockquote>
<p>One example is a large corporation with established data governance frameworks, clear data stewardship roles, and department-wide adherence to data management standards. To advance to the next maturity level, companies should implement metrics to evaluate the effectiveness of data management practices and introduce continuous improvement mechanisms.</p>
<h4 id="level-4-managed"><a class="header" href="#level-4-managed">Level 4: Managed</a></h4>
<blockquote>
<p>The organization monitors and measures compliance with data management standards. There's a focus on continuous improvement based on quantitative performance metrics.</p>
</blockquote>
<p>One example is an enterprise with advanced data governance structures, where data management processes are regularly reviewed for efficiency and effectiveness, and improvements are data-driven. To advance to the next maturity level, companies should foster a culture of innovation in data management, experimenting with new technologies and methodologies to enhance data handling and usage.</p>
<h4 id="level-5-optimizing"><a class="header" href="#level-5-optimizing">Level 5: Optimizing</a></h4>
<blockquote>
<p>At this level, data management practices are continuously optimized through controlled experimentation and innovation. The organization adapts and evolves its data management capabilities to meet future needs and leverage new opportunities.</p>
</blockquote>
<p>One example is a market-leading company that pioneers the use of cutting-edge data technologies and methodologies, setting industry standards for data management and leveraging data as a key competitive advantage. Once in this maturity level, companies should maintain a culture of continuous improvement, staying ahead of industry trends and regularly reassessing and refining data management practices.</p>
<h2 id="aikens-model"><a class="header" href="#aikens-model">Aiken's Model</a></h2>
<blockquote>
<p>Aiken's Model for Data Management Maturity provides a structured approach to assessing and improving an organization's data management capabilities</p>
</blockquote>
<p>While both Aiken's Model and DAMA's DMBOK aim to enhance data management practices, they differ in scope and focus. DAMA's DMBOK provides a comprehensive framework covering a wide range of data management areas, from governance and architecture to data quality and security. Aiken's Model is more narrowly <strong>focused on the maturity progression of data management practices</strong>.</p>
<p>DAMA's DMBOK is broader, offering guidelines and best practices across various knowledge areas. Aiken's Model is specifically concerned with <strong>assessing and advancing the maturity of data management practices through a structured pathway</strong>.</p>
<p>DAMA's DMBOK serves as a reference guide for establishing robust data management practices across the organization. Aiken's Model provides a <strong>roadmap for maturing those practices over time, emphasizing continuous improvement</strong>.</p>
<h3 id="levels-of-measurement-in-aikens-model"><a class="header" href="#levels-of-measurement-in-aikens-model">Levels of Measurement in Aiken's Model</a></h3>
<p>Aiken's Model typically outlines several levels of maturity for data management, from basic, ad-hoc practices to advanced, optimized processes. While the exact levels can vary based on the interpretation of Aiken's principles, a common approach includes:</p>
<h4 id="initialad-hoc"><a class="header" href="#initialad-hoc">Initial/Ad-Hoc</a></h4>
<blockquote>
<p>Data management is unstructured and reactive, with no formal policies or standards.</p>
</blockquote>
<p>To advance to the next level, start by recognizing the importance of structured data management and initiate basic documentation of data processes.</p>
<h4 id="repeatable"><a class="header" href="#repeatable">Repeatable</a></h4>
<blockquote>
<p>Some data management practices are established and can be repeated across projects, but they are not yet standardized or consistently applied.</p>
</blockquote>
<p>To advance to the next level, develop standardized data management policies and ensure they are applied across different teams and projects.</p>
<h4 id="defined"><a class="header" href="#defined">Defined</a></h4>
<blockquote>
<p>Data management processes are formally defined, documented, and integrated into regular business operations.</p>
</blockquote>
<p>To advance to the next level, implement training programs to ensure all team members understand and adhere to established data management practices.</p>
<h4 id="managed"><a class="header" href="#managed">Managed</a></h4>
<blockquote>
<p>The organization regularly measures and evaluates the effectiveness of its data management practices, using metrics to guide improvements.</p>
</blockquote>
<p>To advance to the next level, use insights from data management metrics to identify areas for process optimization and implement targeted improvements.</p>
<h4 id="optimized"><a class="header" href="#optimized">Optimized</a></h4>
<blockquote>
<p>Data management practices are continuously refined and enhanced through feedback loops and the adoption of new technologies and best practices.</p>
</blockquote>
<p>To maintain this level, foster a culture of innovation within the data management team, encouraging experimentation with new tools and methodologies.</p>
<h3 id="implementing-aikens-model"><a class="header" href="#implementing-aikens-model">Implementing Aiken's Model</a></h3>
<p>Implementing Aiken's Model involves a step-by-step approach to maturing an organization's data management practices:</p>
<ul>
<li><strong>Assessment</strong>: Begin with a thorough assessment of current data management practices to identify the current maturity level.</li>
<li><strong>Goal Setting</strong>: Define clear, achievable goals for the next level of maturity, including specific improvements to be made.</li>
<li><strong>Policy Development</strong>: Develop or refine data management policies and standards to support the desired level of maturity.</li>
<li><strong>Training and Communication</strong>: Ensure that all relevant stakeholders are trained on new policies and practices and understand their roles in data management.</li>
<li><strong>Monitoring and Evaluation</strong>: Implement mechanisms to regularly monitor data management practices and measure their effectiveness against defined metrics.</li>
<li><strong>Continuous Improvement</strong>: Use feedback from monitoring and evaluation to continuously improve data management processes.</li>
</ul>
<p>Let's now use three companies as examples: one small tech startup in the Initial phase, a medium-sized retail company in the Repeatable phase, and a multinational corporation in the Managed phase.</p>
<p>The small company, with a few dozen employees, has data scattered across various platforms (e.g., Google Sheets, Dropbox, a simple database). Data management practices are informal, leading to inefficiencies and data quality issues. They plan to advance by implementing the following steps:</p>
<ul>
<li><strong>Assessment</strong>: The startup recognizes the need for structured data management to support growth.</li>
<li><strong>Goal Setting</strong>: Aim to reach the "Repeatable" level by establishing basic data management practices, such as centralized data storage and naming conventions.</li>
<li><strong>Implementation</strong>: The startup decides to consolidate data into a cloud-based platform, providing a single source of truth. They document simple, repeatable processes for data entry, update, and backup.</li>
<li><strong>Advancement</strong>: As these practices become embedded in daily operations, the startup plans to standardize data management policies and provide training to all team members.</li>
</ul>
<p>The medium-sized retail company, with several hundred employees, has basic data management practices in place for customer and inventory data but lacks consistency across departments. Their plan is:</p>
<ul>
<li><strong>Assessment</strong>: The company evaluates its data management practices and identifies inconsistencies in how customer data is handled across sales, marketing, and customer service departments.</li>
<li><strong>Goal Setting</strong>: Aim to reach the "Defined" level by creating a unified customer data management policy and integrating data systems.</li>
<li><strong>Implementation</strong>: The company develops a comprehensive data management policy, standardizing how customer data is collected, stored, and accessed. They implement a CRM system to centralize customer data and provide training to ensure compliance with the new policy.</li>
<li><strong>Advancement</strong>: With standardized data management practices in place, the company focuses on monitoring compliance and effectiveness, setting the stage for further optimization.</li>
</ul>
<p>The multinational corporation, with thousands of employees, has well-established data management practices and uses advanced analytics for strategic decision-making. However, they seek to leverage data more innovatively to maintain a competitive edge. Their plan consists of:</p>
<ul>
<li><strong>Assessment</strong>: The enterprise conducts a thorough review of its data management practices, looking for opportunities to leverage new technologies and methodologies.</li>
<li><strong>Goal Setting</strong>: Aim to reach the "Optimized" level by incorporating AI and machine learning into data processes for predictive analytics and enhanced decision-making.</li>
<li><strong>Implementation</strong>: The enterprise invests in AI and machine learning tools to analyze large datasets for insights. They initiate pilot projects in strategic business areas, applying advanced analytics to improve product development and customer engagement.</li>
<li><strong>Advancement</strong>: The successful integration of AI and machine learning sets a new standard for data management within the enterprise, driving continuous innovation and optimization of data processes.</li>
</ul>
<h2 id="seis-data-management-maturity-model-dmm"><a class="header" href="#seis-data-management-maturity-model-dmm">SEI's Data Management Maturity Model (DMM)</a></h2>
<blockquote>
<p>The DMM model is particularly useful for organizations seeking a structured approach to assessing and improving their data management maturity, with clear categories and maturity levels.</p>
</blockquote>
<p>While the SEI's DMM, DAMA DMBOK, and Aiken's Model all aim to improve data management practices, they have different focuses and structures. SEI's DMM offers a comprehensive and structured assessment model focusing on maturity levels across specific categories of data management. It is particularly useful for organizations looking to benchmark their data management capabilities and develop a roadmap for improvement.</p>
<p>The Data Management Maturity (DMM) Model developed by the Software Engineering Institute (SEI) provides a structured framework for assessing and improving an organization's data management practices. The model is organized into six categories, each focusing on a different aspect of data management:</p>
<ol>
<li><strong>Data Governance</strong>: Focuses on establishing the policies, responsibilities, and processes to ensure effective data management and utilization across the organization.
Example: A financial institution implements a data governance committee to oversee data policies, ensuring compliance with financial regulations and internal data standards.</li>
<li><strong>Data Quality</strong>: Focuses on ensuring the accuracy, completeness, and reliability of data throughout its lifecycle.
Example: An e-commerce company develops automated data quality checks within its product information management system to ensure product descriptions and pricing are accurate and up-to-date.</li>
<li><strong>Data Operations</strong>: Focuses on managing the day-to-day activities involved in data collection, storage, maintenance, and archiving.
Example: A healthcare provider standardizes its patient data entry processes across all clinics to streamline data collection and reduce errors.</li>
<li><strong>Platform and Architecture</strong>: Focuses on establishing the technical infrastructure and architecture to support data management needs.
Example: A technology startup adopts cloud-based data storage solutions and microservices architecture to enhance scalability and data integration capabilities.</li>
<li><strong>Data Management Process</strong>: Focuses on defining and optimizing the processes involved in managing data, from creation to retirement.
Example: A manufacturing company maps out its entire data flow, from raw material procurement data to production and sales data, optimizing each step for efficiency and accuracy.</li>
<li><strong>Supporting Processes</strong>: Focuses on implementing auxiliary processes that support core data management activities, such as security, privacy, and compliance.
Example: An online retailer enhances its data encryption practices and implements stricter access controls to protect customer data and comply with privacy regulations.</li>
</ol>
<h3 id="dmm-model-maturity-levels"><a class="header" href="#dmm-model-maturity-levels">DMM Model Maturity Levels</a></h3>
<p>The DMM Model is structured around specific maturity levels that describe an organization's progression in data management capabilities, focusing on measurable improvements across various categories like Data Governance, Data Quality, and Data Operations. The levels typically range from:</p>
<ol>
<li><strong>Ad Hoc</strong>: Data management practices are unstructured and inconsistent.</li>
<li><strong>Managed</strong>: Basic data management processes are in place but are department-specific.</li>
<li><strong>Standardized</strong>: Organization-wide data management standards and policies are established.</li>
<li><strong>Quantitatively Managed</strong>: Data management processes are measured and controlled.</li>
<li><strong>Optimizing</strong>: Continuous process improvement is embedded in data management practices.</li>
</ol>
<p>DAMA DMBOK does not explicitly define maturity levels in the same structured manner as the DMM Model. Instead, it provides a comprehensive framework covering various knowledge areas essential for effective data management. Aiken's Model outlines a progression through which organizations can develop their data management practices. The comparative analysis for these models is as follows:</p>
<ul>
<li><strong>Structure and Explicitness</strong>: The DMM Model provides a structured and explicit set of maturity levels, making it easier for organizations to benchmark their current state. In contrast, DAMA DMBOK focuses more on the breadth of knowledge areas, leaving maturity assessment more implicit. Aiken's Model offers a clear progression but is more focused on the journey of improving data management practices than on defining specific organizational capabilities at each level.</li>
<li><strong>Focus Areas</strong>: The DMM Model and Aiken's Model both emphasize the evolution of data management practices, but the DMM Model is more granular in its assessment across different data management categories. DAMA DMBOK, while not explicitly structured around maturity levels, covers a broader array of data management disciplines, providing a comprehensive framework that organizations can adapt to their maturity assessment processes.</li>
<li><strong>Application and Goals</strong>: Organizations looking for a detailed roadmap to improve their data management capabilities might lean towards the DMM Model or Aiken's Model for their structured approach to maturity. In contrast, those seeking to ensure comprehensive coverage of all data management areas might use DAMA DMBOK as a guiding framework, supplementing it with maturity concepts from the other models.</li>
</ul>
<p>In practice, organizations might blend elements from each of these frameworks, using DAMA DMBOK's comprehensive knowledge areas as a foundation, Aiken's Model for understanding the staged progression of capabilities, and the DMM Model for specific benchmarks and metrics to gauge and advance their maturity in data management.</p>
<h2 id="gartners-model-for-enterprise-information-management-eim"><a class="header" href="#gartners-model-for-enterprise-information-management-eim">Gartner's Model for Enterprise Information Management (EIM)</a></h2>
<blockquote>
<p>Gartner's EIM model emphasizes the strategic use of information as an asset to drive business value and competitive advantage.</p>
</blockquote>
<p>Gartner's model for Enterprise Information Management (EIM) provides a strategic framework for managing an organization's information assets. Unlike traditional data management models that often focus on the technical aspects of managing data, Gartner's EIM model emphasizes the strategic use of information as an asset to drive business value and competitive advantage. The model integrates data management practices with business strategy, aligning data and information initiatives with broader organizational goals.</p>
<p>Gartner's EIM model distinguishes itself from DAMA's DMBOK and the DMM model by its strong emphasis on aligning information management with business strategy and treating information as a strategic asset. While DAMA's DMBOK provides a comprehensive knowledge framework for data management and the DMM model offers a structured approach to assessing data management maturity, Gartner's EIM model focuses on the strategic integration of information management into business processes and decision-making, aiming to leverage data for competitive advantage.</p>
<p>Gartner's model is more strategic, emphasizing the role of information in achieving business objectives. In contrast, Aiken's model has a more operational focus, concentrating on improving the internal processes and capabilities of data management. Gartner's levels are explicitly aligned with the integration of data management into business strategy, whereas Aiken's stages are more about the maturity and sophistication of data management practices themselves. Gartner's model applies broadly to how an organization manages all its information assets in alignment with business goals, while Aiken's Model is more narrowly focused on the maturity of data management practices.</p>
<h3 id="maturity-levels-in-gartners-eim-model"><a class="header" href="#maturity-levels-in-gartners-eim-model">Maturity Levels in Gartner's EIM Model</a></h3>
<p>Gartner's EIM model outlines several maturity levels, detailing an organization's progression from basic, uncoordinated information management to a mature, optimized, and strategically aligned EIM practice. While Gartner may update its model periodically, a typical progression might include:</p>
<ul>
<li><strong>Awareness</strong>: The organization recognizes the importance of information management but lacks formal strategies and systems. Information is managed in silos, leading to inefficiencies.</li>
<li><strong>Reactive</strong>: The organization begins to address information management in response to specific problems or regulatory requirements. Efforts are project-based and lack cohesion.</li>
<li><strong>Proactive</strong>: There's a shift towards a more proactive approach to information management. The organization has started to implement standardized policies, tools, and governance structures across departments.</li>
<li><strong>Service-Oriented</strong>: Information management is centralized, and services are provided to the entire organization through a shared-service model. There is a focus on efficiency, quality, and supporting business objectives.</li>
<li><strong>Strategic</strong>: Information is fully integrated into business strategy. The organization leverages information as a strategic asset, driving innovation, customer value, and competitive differentiation.</li>
</ul>
<h3 id="metrics-for-assessing-eim-maturity"><a class="header" href="#metrics-for-assessing-eim-maturity">Metrics for Assessing EIM Maturity</a></h3>
<p>To gauge progress and effectiveness at each maturity level, Gartner suggests using a range of metrics that can include, but are not limited to:</p>
<ul>
<li><strong>Data Quality Metrics</strong>: Accuracy, completeness, consistency, and timeliness of data.</li>
<li><strong>Governance Metrics</strong>: Compliance rates with data policies, number of data stewards, and governance initiatives in place.</li>
<li><strong>Usage and Adoption Metrics</strong>: The extent of EIM tool adoption across the organization, user satisfaction scores, and the integration of EIM practices into daily operations.</li>
<li><strong>Business Impact Metrics</strong>: The measurable impact of EIM on business outcomes, such as increased revenue, cost savings, improved customer satisfaction, and reduced risk.</li>
</ul>
<h3 id="advancing-through-the-levels"><a class="header" href="#advancing-through-the-levels">Advancing Through the Levels</a></h3>
<p>Progressing from one maturity level to the next in Gartner's EIM model involves:</p>
<ul>
<li><strong>Strategic Alignment</strong>: Ensuring that information management strategies are aligned with business goals and objectives.</li>
<li><strong>Governance and Leadership</strong>: Establishing strong governance structures and leadership to guide EIM initiatives.</li>
<li><strong>Technology and Tools</strong>: Implementing and integrating the right technologies and tools to support effective information management.</li>
<li><strong>Culture and Collaboration</strong>: Fostering a culture that values information as an asset and promotes collaboration across departments.</li>
<li><strong>Continuous Improvement</strong>: Regularly reviewing and refining EIM practices to adapt to changing business needs and technological advancements.</li>
</ul>
<h2 id="total-quality-data-management-tqdm"><a class="header" href="#total-quality-data-management-tqdm">Total Quality Data Management (TQDM)</a></h2>
<p>Total Quality Data Management (TQDM) is an approach that integrates the principles of Total Quality Management (TQM) into data management practices. TQDM emphasizes continuous improvement, customer (user) satisfaction, and the involvement of all members of an organization in enhancing the quality of data. This approach recognizes data as a critical asset that directly impacts decision-making, operational efficiency, and customer satisfaction.</p>
<p>Compared to traditional data management approaches, TQDM is more holistic and continuous. While traditional data management might focus on specific projects or initiatives to improve data quality, TQDM integrates quality into every aspect of data management, making it an ongoing priority. TQDM's emphasis on user satisfaction, process improvement, and employee involvement also distinguishes it from more technologically focused data management strategies.</p>
<h3 id="key-principles-of-tqdm"><a class="header" href="#key-principles-of-tqdm">Key Principles of TQDM</a></h3>
<p><strong>Customer Focus</strong>: Just as TQM focuses on customer satisfaction, TQDM emphasizes meeting or exceeding the data needs of internal and external users. Understanding and addressing the data requirements of business users, customers, and partners is central to TQDM.</p>
<p><strong>Continuous Improvement</strong>: TQDM adopts the principle of Kaizen, or continuous improvement, applying it to data processes. It involves regularly assessing and enhancing data collection, storage, management, and analysis processes to improve data quality and utility.</p>
<p><strong>Process-Oriented Approach</strong>: Data quality is seen as the result of quality data management processes. TQDM focuses on optimizing these processes to ensure they are efficient, effective, and capable of producing high-quality data.</p>
<p><strong>Employee Involvement</strong>: TQDM encourages the involvement of employees across the organization in data quality initiatives. Data quality is seen as a shared responsibility, with training and empowerment provided to employees to contribute to data management efforts.</p>
<p><strong>Fact-Based Decision Making</strong>: Decisions within a TQDM framework are made based on data and analysis, emphasizing the importance of accurate, reliable data for strategic and operational decision-making.</p>
<h3 id="implementing-tqdm"><a class="header" href="#implementing-tqdm">Implementing TQDM</a></h3>
<p>Implementing TQDM involves several steps, including:</p>
<ul>
<li><strong>Assessing Data Quality Needs</strong>: Identifying the critical data elements and understanding the data quality requirements from the perspective of different data users.</li>
<li><strong>Defining Data Quality Metrics</strong>: Establishing clear, measurable indicators of data quality, such as accuracy, completeness, timeliness, and relevance.</li>
<li><strong>Improving Data Processes</strong>: Analyzing and optimizing data-related processes, from data collection and entry to storage, maintenance, and usage, to enhance quality.</li>
<li><strong>Training and Empowerment</strong>: Providing employees with the knowledge and tools they need to contribute to data quality and making them stakeholders in data management.</li>
<li><strong>Monitoring and Feedback</strong>: Establishing systems for ongoing monitoring of data quality and processes, and creating feedback loops for continuous improvement.</li>
</ul>
<h3 id="benefits-of-tqdm"><a class="header" href="#benefits-of-tqdm">Benefits of TQDM</a></h3>
<ul>
<li><strong>Improved Data Quality</strong>: By focusing on the processes that create and manage data, TQDM helps ensure higher data quality across the organization.</li>
<li><strong>Enhanced Decision Making</strong>: Better data quality leads to more informed decision-making at all levels of the organization.</li>
<li><strong>Increased User Satisfaction</strong>: Addressing the data needs and requirements of users increases satisfaction and trust in the organization's data assets.</li>
<li><strong>Operational Efficiency</strong>: Optimized data processes reduce redundancies and errors, leading to more efficient operations.</li>
</ul>
<h2 id="data-management-capability-assessment-model-dcam"><a class="header" href="#data-management-capability-assessment-model-dcam">Data Management Capability Assessment Model (DCAM)</a></h2>
<p>The Data Management Capability Assessment Model (DCAM) is a comprehensive framework developed by the EDM Council, a global association created to elevate the practice of data management. DCAM provides a structured approach for assessing and improving data management practices, focusing on the capabilities necessary to establish a sustainable data management program. It's designed to help organizations benchmark their data management practices against industry standards and identify areas for improvement.</p>
<p>Compared to models like DAMA DMBOK and TQDM, DCAM provides a more structured approach to assessing data management capabilities, offering a clear maturity model and specific components to guide improvement efforts. While DAMA DMBOK offers a comprehensive knowledge framework for data management, DCAM focuses more on the capability and maturity aspects, providing a benchmarking tool for organizations to measure their progress. TQDM emphasizes quality management principles in data management, whereas DCAM provides a broader assessment model covering all aspects of data management, from governance and quality to technology and analytics.</p>
<h3 id="components-of-dcam"><a class="header" href="#components-of-dcam">Components of DCAM</a></h3>
<p>DCAM is structured around several core components, each addressing critical aspects of data management:</p>
<ul>
<li><strong>Data Management Strategy</strong>: Outlines the overarching approach and objectives for data management within the organization, ensuring alignment with business goals.</li>
<li><strong>Data Governance</strong>: Focuses on the establishment of data governance structures and roles, defining responsibilities and policies for data across the organization.</li>
<li><strong>Data Quality</strong>: Emphasizes the importance of maintaining high data quality through continuous monitoring, measurement, and improvement processes.</li>
<li><strong>Data Operations</strong>: Covers the operational aspects of data management, including data lifecycle management, data security, and data issue resolution.</li>
<li><strong>Data Architecture and Integration</strong>: Addresses the design of data architecture and the integration of data across systems to support accessibility, consistency, and usability.</li>
<li><strong>Business Process and Data Alignment</strong>: Ensures that data management practices are integrated into business processes, supporting operational efficiency and decision-making.</li>
<li><strong>Data Innovation and Analytics</strong>: Encourages the innovative use of data, leveraging analytics and advanced data technologies to drive business value.</li>
<li><strong>Technology and Infrastructure</strong>: Considers the technological foundation required to support effective data management, including data storage, processing, and analytics platforms.</li>
</ul>
<h3 id="dcam-maturity-model"><a class="header" href="#dcam-maturity-model">DCAM Maturity Model</a></h3>
<p>The DCAM framework includes a maturity model that helps organizations assess their level of data management capability across the components mentioned above. The model typically defines several maturity levels, from basic to advanced:</p>
<ul>
<li><strong>Ad Hoc/Undefined</strong>: Data management practices are informal and unstructured, with no clear policies or standards in place.</li>
<li><strong>Performed/Repeatable</strong>: Basic data management practices are being performed, but they may not be consistent or standardized across the organization.</li>
<li><strong>Defined</strong>: Formal data management policies and standards are established and documented, providing a clear framework for data management activities.</li>
<li><strong>Managed and Measurable</strong>: Data management practices are monitored and measured against defined metrics, with active management of data quality and governance.</li>
<li><strong>Optimized</strong>: Continuous improvement processes are in place, with data management practices being regularly refined and optimized based on performance metrics and business needs.</li>
</ul>
<h2 id="model-for-assessing-data-management-mamd"><a class="header" href="#model-for-assessing-data-management-mamd">Model for Assessing Data Management (MAMD)</a></h2>
<p>The Model for Assessing Data Management (MAMD) is a conceptual framework designed to evaluate an organization's data management practices and identify areas for improvement. While not as widely recognized as other models like DAMA DMBOK or DCAM, the principles behind an assessment model like MAMD can provide valuable insights into the maturity and effectiveness of data management within an organization.</p>
<p>Compared to DAMA DMBOK and DCAM, a conceptual model like MAMD would similarly offer a structured approach to assessing and improving data management practices. However, the specific focus areas and maturity levels might vary based on the unique aspects of the MAMD framework. While DAMA DMBOK provides a comprehensive knowledge framework and DCAM offers a capability and maturity assessment model, MAMD would combine evaluation and maturity assessment to guide organizations in enhancing their data management practices systematically.</p>
<h3 id="mamd-evaluation-model"><a class="header" href="#mamd-evaluation-model">MAMD Evaluation Model</a></h3>
<p>The evaluation model within MAMD typically focuses on various dimensions of data management, such as data quality, data governance, data architecture, and data operations, similar to other frameworks. The evaluation process involves:</p>
<ul>
<li><strong>Assessment of Current Practices</strong>: Reviewing current data management practices against best practices and standards to identify gaps and areas of non-compliance.</li>
<li><strong>Stakeholder Engagement</strong>: Involving key stakeholders from across the organization to gather insights into data management challenges and needs.</li>
<li><strong>Data Management Capabilities</strong>: Evaluating the organization's capabilities in managing data across different lifecycle stages, from creation and storage to use and disposal.</li>
<li><strong>Technology and Tools</strong>: Assessing the adequacy of the technology and tools in place to support effective data management.</li>
<li><strong>Compliance and Risk Management</strong>: Evaluating how well data management practices align with regulatory requirements and manage data-related risks.</li>
</ul>
<h3 id="mamd-maturity-model"><a class="header" href="#mamd-maturity-model">MAMD Maturity Model</a></h3>
<p>Like other data management maturity models, the MAMD maturity model would typically categorize an organization's data management practices into several levels, from initial to optimized stages:</p>
<ul>
<li><strong>Initial (Ad-Hoc)</strong>: Data management is unstructured and reactive, with no formal policies or procedures in place.</li>
<li><strong>Developing</strong>: Some data management processes and policies are being developed, but they may not be consistently applied across the organization.</li>
<li><strong>Defined</strong>: Formal data management policies and procedures are documented and implemented, covering key areas of data management.</li>
<li><strong>Managed</strong>: Data management practices are regularly monitored and reviewed, with performance measured against predefined metrics.</li>
<li><strong>Optimized</strong>: Continuous improvement processes are in place for data management, with practices regularly refined based on performance feedback and evolving business needs.</li>
</ul>
<h2 id="conclusion-to-data-process-quality-models"><a class="header" href="#conclusion-to-data-process-quality-models">Conclusion to Data Process Quality Models</a></h2>
<h3 id="comprehensive-data-governance-dama-dcam"><a class="header" href="#comprehensive-data-governance-dama-dcam">Comprehensive Data Governance (DAMA, DCAM)</a></h3>
<p>Models like DAMA DMBOK and DCAM emphasize robust data governance, which is foundational for designing data infrastructures that ensure data quality, security, and compliance. Implementing strong governance frameworks influences how data warehouses, lakes, and marts are structured to enforce policies, standards, and roles effectively.</p>
<h3 id="maturity-and-capability-focus-dmm-mamd"><a class="header" href="#maturity-and-capability-focus-dmm-mamd">Maturity and Capability Focus (DMM, MAMD)</a></h3>
<p>The maturity models provided by DMM and conceptual models like MAMD offer a roadmap for organizations to evolve their data management practices. This progression impacts data infrastructure design by encouraging scalable, flexible architectures that can adapt to growing data management sophistication, from basic data warehousing to advanced analytics in data lakes.</p>
<h3 id="strategic-alignment-gartners-eim"><a class="header" href="#strategic-alignment-gartners-eim">Strategic Alignment (Gartner's EIM)</a></h3>
<p>Gartner's focus on integrating data management with business strategy ensures that data infrastructures are designed not just for operational efficiency but also to drive business value. This approach encourages the alignment of data warehouses, lakes, and marts with strategic business objectives, ensuring they support decision-making and innovation.</p>
<h3 id="quality-driven-processes-tqdm-aikens-model"><a class="header" href="#quality-driven-processes-tqdm-aikens-model">Quality-Driven Processes (TQDM, Aiken's Model)</a></h3>
<p>The emphasis on continuous quality improvement in TQDM and the operational improvement focus of Aiken's Model impact data infrastructure design by promoting architectures that support ongoing data quality initiatives. This includes incorporating data quality tools and processes into data lakes and warehouses and designing data marts that provide high-quality, business-specific insights.</p>
<h3 id="user-centric-design"><a class="header" href="#user-centric-design">User-Centric Design</a></h3>
<p>Across all models, there's an underlying theme of designing data infrastructures that meet the needs of end-users, whether they're business analysts, data scientists, or operational teams. This user-centric approach ensures that data warehouses, lakes, and marts are accessible, understandable, and valuable to all stakeholders, enhancing adoption and driving better business outcomes.</p>
<h3 id="innovation-and-adaptability"><a class="header" href="#innovation-and-adaptability">Innovation and Adaptability</a></h3>
<p>Models like DCAM and Gartner's EIM framework encourage organizations to stay abreast of technological advancements and evolving best practices. This influences data infrastructure design to be adaptable and open to integrating new technologies such as cloud storage, real-time analytics, and machine learning capabilities within data lakes and warehouses.</p>
<h2 id="final-thoughts-on-data-process-quality"><a class="header" href="#final-thoughts-on-data-process-quality">Final Thoughts on Data Process Quality</a></h2>
<p>In conclusion, while each data management model offers distinct methodologies and focuses, collectively, they underscore the importance of strategic, quality-focused, and user-centric approaches to data management. The impact on data infrastructure design is profound, guiding organizations toward building data warehouses, lakes, and marts that are not only efficient and compliant but also agile, scalable, and aligned with business strategies. By adopting principles from these models, organizations can ensure their data infrastructure is well-positioned to support current and future data management needs, driving insights, innovation, and competitive advantage in an increasingly data-driven world.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-quality-models"><a class="header" href="#data-quality-models">Data Quality Models</a></h1>
<blockquote>
<p>Data Quality Models are fundamental frameworks that define, measure, and evaluate the quality of data within an organization. These models are crucial because they provide a structured approach to identifying and quantifying the various aspects of data quality, which are essential for ensuring that data is accurate, consistent, reliable, and fit for its intended use.</p>
</blockquote>
<p>Data Quality Models are particularly important for data teams, data engineers, and data analysts who are responsible for managing the lifecycle of data, from its creation and storage to its processing and analysis. By applying these models, professionals can ensure that the data they work with meets the necessary standards of quality, thereby supporting effective decision-making, optimizing business processes, and enhancing customer satisfaction.</p>
<p>A Data Quality Model is a conceptual framework used to define, understand, and measure the quality of data. It outlines specific criteria and dimensions that are essential for assessing the fitness of data for its intended use. These models serve as a guideline for data teams, including data engineers and data analysts, to systematically evaluate and improve the quality of the data within their systems.</p>
<h2 id="key-criteria-and-dimensions-of-data-quality"><a class="header" href="#key-criteria-and-dimensions-of-data-quality">Key Criteria and Dimensions of Data Quality</a></h2>
<p>Data quality can be assessed through various dimensions, each representing a critical aspect of the data's overall quality. While different models may emphasize different dimensions, the following are widely recognized and form the core of most Data Quality Models:</p>
<ul>
<li><a href="concepts/data-quality/./accuracy_dimension.html"><strong>Accuracy</strong></a>: Refers to the correctness and precision of the data. Data is considered accurate if it correctly represents the real-world values it is intended to model.</li>
<li><a href="concepts/data-quality/./completeness_dimension.html"><strong>Completeness</strong></a>: Measures whether all the required data is present. Incomplete data can lead to gaps in analysis and decision-making.</li>
<li><a href="concepts/data-quality/./consistency_dimension.html"><strong>Consistency</strong></a>: Ensures that the data does not contain conflicting or contradictory information across the dataset or between multiple data sources.</li>
<li><a href="concepts/data-quality/./timeliness_dimension.html"><strong>Timeliness</strong></a>: Pertains to the availability of data when it is needed. Timely data is crucial for decision-making processes that rely on up-to-date information.</li>
<li><a href="concepts/data-quality/./relevance_dimension.html"><strong>Relevance</strong></a>: Assesses whether the data is applicable and helpful for the context in which it is used. Data should meet the needs of its intended purpose.</li>
<li><a href="concepts/data-quality/./reliability_dimension.html"><strong>Reliability</strong></a>: Focuses on the trustworthiness of the data. Reliable data is sourced from credible sources and maintained through dependable processes.</li>
<li><a href="concepts/data-quality/./uniqueness_dimension.html"><strong>Uniqueness</strong></a>: Ensures that entities within the data are represented only once. Duplicate records can skew analysis and lead to inaccurate conclusions.</li>
<li><a href="concepts/data-quality/./validity_dimension.html"><strong>Validity</strong></a>: Measures whether the data conforms to the specific syntax (format, type, range) defined by the data model and business rules.</li>
<li><a href="concepts/data-quality/./accessibility_dimension.html"><strong>Accessibility</strong></a>: Data should be easily retrievable and usable by authorized individuals, ensuring that data consumers can access the data when needed.</li>
<li><a href="concepts/data-quality/./integrity_dimension.html"><strong>Integrity</strong></a>: Refers to the maintenance of data consistency and accuracy over its lifecycle, including relationships within the data that enforce logical rules and constraints.</li>
</ul>
<h2 id="applying-a-data-quality-model"><a class="header" href="#applying-a-data-quality-model">Applying a Data Quality Model</a></h2>
<p>In practice, data teams apply these dimensions by:</p>
<ul>
<li><strong>Setting Benchmarks</strong>: Defining acceptable levels or thresholds for each data quality dimension relevant to their business context.</li>
<li><strong>Data Profiling and Auditing</strong>: Using tools and techniques to assess the current state of data against the defined benchmarks.</li>
<li><strong>Implementing Controls</strong>: Establishing processes and controls to maintain data quality, such as validation checks during data entry or automated cleansing routines.</li>
<li><strong>Continuous Monitoring</strong>: Regularly monitoring data quality metrics to identify areas for improvement and to ensure ongoing compliance with quality standards.</li>
</ul>
<h2 id="impact-on-data-infrastructure"><a class="header" href="#impact-on-data-infrastructure">Impact on Data Infrastructure</a></h2>
<p>The application of a Data Quality Model has a direct impact on the design and architecture of data infrastructure:</p>
<ul>
<li><strong>Data Warehouses and Data Lakes</strong>: Ensuring that data stored in these repositories meets quality standards is crucial for reliable reporting and analytics.</li>
<li><strong>Data Marts</strong>: Tailored for specific business functions, the quality of data in data marts directly affects the accuracy and reliability of business insights derived from them.</li>
<li><strong>ETL Processes</strong>: Extract, Transform, Load (ETL) processes must incorporate data quality checks to cleanse, validate, and standardize data as it moves between systems.</li>
</ul>
<h2 id="scope"><a class="header" href="#scope">Scope</a></h2>
<p>Before delving into the specific dimensions of data quality, it's important to outline the components of the data infrastructure ecosystem that will be under consideration:</p>
<ul>
<li><strong>Data Source (Operational Data)</strong>: This refers to the original data sources that feed into data lakes, data warehouses, and data marts. It's primarily operational data that originates from business activities and transactions.</li>
<li><strong>ELTs (Extract, Load, Transform)</strong>: These are the processes responsible for ingesting Operational Data into the data infrastructure, which could be a database, a data lake, or a data warehouse. Tools like AWS DMS (Database Migration Service), Airbyte, Fivetran, or services connecting to data sources through APIs, ODBC, message queues, etc.</li>
<li><strong>Data Lake</strong>: This component acts as a vast repository for storing a wide array of data types, including Structured, Semi-Structured, and Unstructured data. An example of a data lake is AWS S3 Buckets.</li>
<li><strong>Data Warehouse</strong>: Serving as a centralized repository, a data warehouse enables the analysis of data to support informed decision-making. Some examples include Snowflake, AWS Redshift, and Databricks Data Lakehouse.</li>
<li><strong>Data Marts</strong>: These are focused segments of data warehouses tailored to meet the specific requirements of different business units or departments, facilitating more targeted data analysis.</li>
<li><strong>ETLs (Extract, Transform, Load)</strong>: This process is centered around data transformation. Tools such as dbt, pandas, and Informatica are commonly used for this purpose.</li>
</ul>
<p>Depending on the use case, the presence and significance of these components may vary. Similarly, the dimensions of data quality being assessed might also differ based on the specific requirements and context of each scenario.</p>
<h2 id="data-quality-metricsaudit-database--service"><a class="header" href="#data-quality-metricsaudit-database--service">Data Quality Metrics/Audit Database &amp; Service</a></h2>
<p>Maintaining <a href="concepts/data-quality/./metrics_database.html">Data Quality Metrics/Audit databases and services</a> is foundational to managing modern data ecosystems effectively. They provide the visibility, accountability, and insights necessary to ensure data reliability, optimize operations, maintain compliance, and secure data assets, ultimately supporting the organization's strategic objectives.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="accuracy-dimension-in-data-quality"><a class="header" href="#accuracy-dimension-in-data-quality">Accuracy Dimension in Data Quality</a></h1>
<blockquote>
<p>Accuracy is one of the most critical dimensions of data quality, referring to the closeness of data values to the true values they are intended to represent. Ensuring accuracy is fundamental across all stages of the data infrastructure, from data sources through ELTs (Extract, Load, Transform) processes, data lakes, and data warehouses, to data marts, and ultimately in reports and dashboards.</p>
</blockquote>
<p>When considering accuracy within your data quality framework, it's essential to implement metrics that can capture discrepancies between the data you have and the true, expected values. Here are some accuracy dimension metrics you could implement across different stages of your data infrastructure:</p>
<ol>
<li>
<p><strong>Source-to-Target Data Comparison</strong></p>
<ul>
<li>
<p><strong>Record Count Checks</strong>:
Compare the number of records in the source systems against the number of records loaded into S3 and Redshift to ensure completeness of data transfer.</p>
</li>
<li>
<p><strong>Hash Total Checks</strong>:
Generate and compare hash totals (a checksum of concatenated field values) for datasets in the source and the target to verify that data has been loaded accurately.</p>
</li>
<li>
<p><strong>Field-Level Value Checks</strong>:
Compare sample values for critical fields in source databases with corresponding fields in S3 and Redshift to ensure field values are accurately loaded.</p>
</li>
<li>
<p><strong>Data Type Checks</strong>:
Verify that data types remain consistent when moving from source systems to S3/Redshift, as type mismatches can introduce inaccuracies.</p>
</li>
</ul>
</li>
<li>
<p><strong>Data Transformation Accuracy</strong></p>
<ul>
<li>
<p><strong>Transformation Logic Verification</strong>:
For dbt models creating staging schemas and data marts, perform unit tests to ensure transformation logic preserves data accuracy.</p>
</li>
<li>
<p><strong>Round-Trip Testing</strong>:
Apply transformations to source data and reverse the process to check if the original data is recoverable, ensuring transformations have not introduced inaccuracies.</p>
</li>
</ul>
</li>
<li>
<p><strong>Aggregation and Calculation Consistency</strong></p>
<ul>
<li>
<p><strong>Aggregated Totals Verification</strong>:
Verify that aggregated measures (sums, averages, etc.) in data marts match expected values based on source data.</p>
</li>
<li>
<p><strong>Business Rule Validation</strong>:
Implement rules-based validation to check that calculated fields, such as financial totals or statistical measures, adhere to predefined business rules and logic.</p>
</li>
</ul>
</li>
<li>
<p><strong>Data Quality Scorecards</strong></p>
<ul>
<li><strong>Attribute Accuracy Scores</strong>:
Assign accuracy scores to different attributes or columns based on validation tests, and monitor these scores over time to identify trends and areas needing improvement.</li>
</ul>
</li>
<li>
<p><strong>Anomaly Detection</strong></p>
<ul>
<li>
<p><strong>Statistical Analysis</strong>:
Apply statistical methods to detect outliers or values that deviate significantly from historical patterns or expected ranges.</p>
</li>
<li>
<p><strong>Machine Learning</strong>:
Use machine learning models to predict expected data values and highlight anomalies when actual values diverge.</p>
</li>
</ul>
</li>
<li>
<p><strong>Continuous Monitoring and Alerting</strong></p>
<ul>
<li><strong>Real-Time Alerts</strong>:
Set up real-time monitoring and alerts for data accuracy issues, using tools like DataDog or custom scripts to trigger notifications when data falls outside acceptable accuracy parameters.</li>
</ul>
</li>
<li>
<p><strong>Reporting and Feedback Mechanisms</strong></p>
<ul>
<li>
<p><strong>Accuracy Reporting</strong>:
Create reports and dashboards that track the accuracy of data across different stages and systems, providing visibility to stakeholders.</p>
</li>
<li>
<p><strong>Feedback Loops</strong>:
Establish mechanisms for users to report potential inaccuracies in reports and dashboards, feeding into continuous improvement processes.</p>
</li>
</ul>
</li>
</ol>
<p>Implementing a combination of these metrics and checks will provide a comprehensive approach to ensuring the accuracy of data across your data infrastructure. It's important to tailor these metrics to the specific characteristics of your data and the business context in which it's used. Regular review and adjustment of these metrics will ensure they remain effective and relevant as your data environment evolves.</p>
<h2 id="accuracy-metrics"><a class="header" href="#accuracy-metrics">Accuracy Metrics</a></h2>
<p>To measure accuracy, data teams employ various metrics and techniques, often tailored to the specific type of data and its intended use. Here are some examples of how accuracy can be measured throughout the data infrastructure:</p>
<h3 id="data-sources-operational-data---error-rate"><a class="header" href="#data-sources-operational-data---error-rate">Data Sources (Operational Data) - Error Rate</a></h3>
<p>\[ Error \ Rate = \frac{Number\ of \ Incorrect \ Records}{Total \ Number \ of \ Records} \times 100 \]</p>
<p><strong>Application</strong>: Assess the error rate in operational data by comparing recorded data values against verified true values (from trusted sources or manual verification). Some common uses of this metric are:</p>
<ul>
<li>
<p><strong>Financial Services</strong>:
Banks and financial institutions use the error rate metric to monitor the accuracy of transactional data. High error rates in financial transactions can lead to significant financial loss and regulatory compliance issues.</p>
</li>
<li>
<p><strong>Healthcare</strong>:
In healthcare records management, the error rate is crucial for patient safety. Incorrect records can lead to wrong treatment plans and medication errors. Hence, healthcare providers closely monitor error rates in patient data entries.</p>
</li>
<li>
<p><strong>E-Commerce</strong>:
For e-commerce platforms, error rates in inventory data can result in stock discrepancies, leading to order fulfillment issues. Monitoring error rates helps maintain accurate stock levels and customer satisfaction.</p>
</li>
<li>
<p><strong>Manufacturing</strong>:
In manufacturing, error rate metrics can be used to track the quality of production data. High error rates might indicate issues in the production process, affecting product quality and operational efficiency.</p>
</li>
<li>
<p><strong>Telecommunications</strong>:
Telecom companies may use error rates to evaluate the accuracy of call data records (CDRs), which are vital for billing purposes. Inaccuracies can lead to billing disputes and revenue loss.</p>
</li>
<li>
<p><strong>Retail and Point of Sale (POS) Systems</strong>:
Retailers monitor error rates in sales transactions to ensure accurate sales data, which is essential for inventory management, financial reporting, and customer loyalty programs.</p>
</li>
<li>
<p><strong>Data Migration Projects</strong>:
During data migration or integration projects, the error rate is a critical metric to ensure that data is correctly transferred from legacy systems to new databases without loss or corruption</p>
</li>
<li>
<p><strong>Quality Assurance in Software Development</strong>:
In software testing, error rates can measure the accuracy of data output by new applications or systems under development, ensuring the software meets the required quality standards before release.</p>
</li>
</ul>
<p>In each of these contexts, maintaining a low error rate is important not only for immediate operational success but also for long-term trust in the data systems, customer satisfaction, and compliance with industry standards and regulations. Regular monitoring and efforts to reduce the error rate are key practices in data quality management.</p>
<h3 id="elt-processes---transformation-accuracy-rate"><a class="header" href="#elt-processes---transformation-accuracy-rate">ELT Processes - Transformation Accuracy Rate</a></h3>
<p>\[ Transformation \ Accuracy \ Rate = \frac{Number \ of \ Correctly \ Transformed \ Records}{Total \ Number \ of \ Transformed \ Records} \times 100 \]</p>
<p><strong>Application</strong>: Validate the accuracy of data post-transformation by comparing pre and post-ELT data against expected results based on transformation logic.</p>
<h3 id="data-lakes-and-data-warehouses---data-conformity-rate"><a class="header" href="#data-lakes-and-data-warehouses---data-conformity-rate">Data Lakes and Data Warehouses - Data Conformity Rate</a></h3>
<p>\[ Data \ Conformity \ Rate = \frac{Number \ of \ Records \ Conforming \ to \ Data \ Models}{Total \ Number \ of \ Records} \times 100 \]</p>
<p><strong>Application</strong>: Ensure that data in lakes and warehouses conforms to predefined data models and schemas, indicating accurate structuring and categorization. Some common use cases are:</p>
<ul>
<li>
<p><strong>Data Governance</strong>:
Helps ensure that data governance policies are being followed by measuring how well the data matches the organization's data standards and models.</p>
</li>
<li>
<p><strong>Data Integration</strong>:
During the integration of various data sources into a data lake or warehouse, this metric can indicate the success of harmonizing disparate data formats into a consistent schema.</p>
</li>
</ul>
<h3 id="data-marts---attribute-accuracy"><a class="header" href="#data-marts---attribute-accuracy">Data Marts - Attribute Accuracy</a></h3>
<p>\[ Attribute \ Accuracy = \frac{Number \ of \ Correct \ Attribute \ Values}{Total \ Number \ of \ Attribute \ Values} \times 100 \]</p>
<p><strong>Application</strong>: For each attribute in a data mart, compare the values against a set of true values or rules to assess attribute-level accuracy.</p>
<ul>
<li>
<p><strong>Marketing Analytics</strong>:
Ensuring campaign data attributes like dates, budget figures, and demographic details are correct to inform marketing strategies.</p>
</li>
<li>
<p><strong>Financial Reporting</strong>:
In finance, attribute accuracy for figures such as revenue, cost, and profit margins is critical for regulatory compliance and internal audits.</p>
</li>
</ul>
<h2 id="automating-accuracy-measurement-with-airflow"><a class="header" href="#automating-accuracy-measurement-with-airflow">Automating Accuracy Measurement with Airflow</a></h2>
<p>Airflow provides a robust way to automate and monitor data workflows, and you can extend its capabilities by using sensors and operators to measure the accuracy of your data as it moves through the various stages of your pipeline.</p>
<p>For the examples below, let's imagine a scenario where AWS DMS loads data from multiple databases into Redshift, and dbt models transform the data to create Data Marts. Here are some Sensors and Operators for Accuracy Measurement in Airflow:</p>
<h3 id="dms-task-sensor"><a class="header" href="#dms-task-sensor">DMS Task Sensor</a></h3>
<blockquote>
<p>Monitors the state of an AWS Data Migration Service (DMS) task.</p>
</blockquote>
<p>You can extend this sensor to query the source and target databases after the DMS task is completed, comparing record counts or checksums to ensure data has been transferred correctly. The <em>Accuracy</em> metric could be measured as:</p>
<p>\[ Accuracy = \frac{Number \ of \ Records \ in \ Target}{Total \ Number \ of \ Records \ in \ Source} \times 100 \]</p>
<h3 id="sql-check-operator"><a class="header" href="#sql-check-operator">SQL Check Operator</a></h3>
<blockquote>
<p>Executes an SQL query and checks the result against a predefined condition.</p>
</blockquote>
<p>Run integrity checks such as COUNT(*) on both source and target tables, and use this operator to compare the counts. The <em>Accuracy</em> metric could be measured in this case as:</p>
<p>\[ Accuracy = \frac{Number \ of \ Records \ in \ Target}{Total \ Number \ of \ Records \ in \ Source} \times 100 \]</p>
<h3 id="sql-value-check-operator"><a class="header" href="#sql-value-check-operator">SQL Value Check Operator</a></h3>
<blockquote>
<p>Executes a SQL query and ensures that the returned value meets a certain condition.</p>
</blockquote>
<p>Perform field-level data validation by selecting key fields and comparing them between the source and the target after a DMS task. The <em>Field Accuracy</em> metric could be measured as:</p>
<p>\[ Field \ Accuracy = \frac{Number \ of \ Matching \ Field \ Values}{Total \ Number \ of \ Field \ Values \ Checked} \times 100 \]</p>
<h3 id="dbt-run-operator"><a class="header" href="#dbt-run-operator">dbt Run Operator</a></h3>
<blockquote>
<p>Executes dbt run to run transformation models.</p>
</blockquote>
<p>After the dbt run, use dbt's built-in test functionality to perform accuracy checks on transformed data against source data or expected results. The <em>Transformation Accuracy</em> metric could be measured as:</p>
<p>\[ Transformation \ Accuracy = \frac{Number \ of \ Pass \ Tests}{Total \ Number \ of \ Tests} \times 100 \]</p>
<h3 id="data-quality-operator"><a class="header" href="#data-quality-operator">Data Quality Operator</a></h3>
<blockquote>
<p>A custom operator that you can define to implement data quality checks.</p>
</blockquote>
<p>Incorporate various data quality checks like hash total comparisons, data profiling, anomaly detection, and more complex validations that may not be directly supported by built-in operators. The <em>Accuracy</em> metric could be measured as:</p>
<p>\[ Accuracy = (1 - \frac{Number \ of \ Pass \ Tests}{Total \ Number \ of \ Tests}) \times 100 \]</p>
<h3 id="python-operator"><a class="header" href="#python-operator">Python Operator</a></h3>
<blockquote>
<p>Executes a Python callable (function) to perform custom logic.</p>
</blockquote>
<p>Use this operator to implement custom accuracy metrics, like calculating the percentage of records within an acceptable deviation range from a golden dataset or source of truth. The metrics here will be based on the specific accuracy check implemented in the Python function.</p>
<h3 id="sensors--operators"><a class="header" href="#sensors--operators">Sensors &amp; Operators</a></h3>
<p>In your Airflow DAGs, you would typically sequence these sensors and operators such that the DMS Task Sensor runs first to ensure the DMS task has been completed. Following that, the <em>SQL Check</em> and <em>SQL Value Check Operators</em> can verify the accuracy of the data transfer.</p>
<p>Post-transformation, the <em>dbt Run Operator</em> along with additional data quality checks using the <em>Python Operator</em> or a custom <em>Data Quality Operator</em> can be used to ensure the accuracy of the dbt transformations.</p>
<p>It's important to note that while these checks can provide a good indication of data accuracy, they are most effective when part of a comprehensive data quality framework that includes regular reviews, stakeholder feedback, and iterative improvements to the checks themselves. Moreover, the exact mathematical formulas might need to be adapted to the specific requirements and context of your data and business rules.</p>
<h2 id="ensuring-and-improving-accuracy"><a class="header" href="#ensuring-and-improving-accuracy">Ensuring and Improving Accuracy</a></h2>
<p>Ensuring accuracy across the data infrastructure involves several key practices:</p>
<ul>
<li>
<p><strong>Data Profiling and Cleaning</strong>:
Regularly profile data at source and post-ELT to identify inaccuracies. Implement data cleaning routines to correct identified inaccuracies.</p>
</li>
<li>
<p><strong>Validation Rules</strong>:
Establish comprehensive validation rules that data must meet before entering the system, ensuring only accurate data is processed and stored.</p>
</li>
<li>
<p><strong>Automated Testing and Monitoring</strong>:
Implement automated testing of data transformations and monitoring of data quality metrics to continuously assess and ensure accuracy.</p>
</li>
<li>
<p><strong>Feedback Loops</strong>:
Create mechanisms for users to report inaccuracies in reports and dashboards, feeding back into data cleaning and improvement processes.</p>
</li>
</ul>
<h2 id="accuracy-measurement-example"><a class="header" href="#accuracy-measurement-example">Accuracy Measurement Example</a></h2>
<p>Measuring accuracy in a data infrastructure involves a series of steps and tools that ensure data remains consistent and true to its source throughout its lifecycle. Here's a detailed example incorporating dbt (data build tool), Soda Core, and SQL queries, illustrating how accuracy can be measured from the moment data is loaded into a data lake or warehouse, through transformation processes, and finally when it is ingested into a data mart, in a different process or pipeline, of course. Each pipeline is orchestrated by Apache Airflow.</p>
<h3 id="pipeline-1-validating-operational-data-post-load"><a class="header" href="#pipeline-1-validating-operational-data-post-load">Pipeline 1: Validating Operational Data Post-Load</a></h3>
<ul>
<li>
<p><strong>Scenario</strong>: Once AWS DMS (Database Migration Service) or any ELT tool finishes loading operational data into the data lake or data warehouse, immediate validation is crucial to ensure data accuracy.</p>
</li>
<li>
<p><strong>Implementation</strong>:</p>
<ul>
<li><strong>Soda Core</strong>: Use Soda Core to run validation checks on the newly ingested data. Soda Core can be configured to perform checks such as row counts, null value checks, or even more complex validations against known data quality rules.</li>
<li><strong>SQL Query</strong>: Write an SQL query to validate specific data accuracy metrics, such as comparing sums, counts, or specific field values against expected values or historical data.</li>
</ul>
</li>
<li>
<p><strong>Saving Metrics</strong>: Store the results of these validations in a dedicated metrics or audit database, capturing details like the timestamp of the check, the specific checks performed, and the outcomes.</p>
<ul>
<li><strong>Sample</strong>: 'transactions_yesterday_count' | 1634264 | '2024-02-19T19:12:21.310Z' | 'order_service' | 'orders'</li>
</ul>
</li>
</ul>
<h3 id="pipeline-2-transforming-data-with-dbt"><a class="header" href="#pipeline-2-transforming-data-with-dbt">Pipeline 2: Transforming Data with dbt</a></h3>
<ul>
<li>
<p><strong>Scenario</strong>: Transformations are applied to the ingested data to prepare it for use in data marts, using dbt for data modeling and transformations. After transformations, data is ready to be ingested into data marts for specific business unit analyses.</p>
</li>
<li>
<p><strong>Implementation</strong>:</p>
<ul>
<li>
<p><strong>dbt Tests</strong>: Use dbt's built-in testing capabilities to validate the accuracy of transformed data. This can include unique tests, referential integrity tests, or custom SQL tests that assert data accuracy post-transformation.</p>
</li>
<li>
<p><strong>dbt Metrics</strong>: Define and calculate key data accuracy metrics within dbt, leveraging its ability to capture and model data quality metrics alongside the transformation logic.</p>
</li>
<li>
<p><strong>Metric Comparison</strong>: Before the final ingestion into data marts, compare the dbt-calculated accuracy metrics with the initially captured metrics in the audit database to ensure that the transformation process has not introduced inaccuracies.</p>
</li>
<li>
<p><strong>Automated Alerts</strong>: Implement automated alerts to notify data teams if discrepancies exceed predefined thresholds, indicating potential accuracy issues that require investigation. This can be set in Apache Airflow.</p>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="completeness-dimension-in-data-quality"><a class="header" href="#completeness-dimension-in-data-quality">Completeness Dimension in Data Quality</a></h1>
<blockquote>
<p>Completeness is a crucial dimension of data quality, referring to the extent to which all required data is present within a dataset. It measures the absence of missing values or records in the data and ensures that datasets are fully populated with all necessary information for accurate analysis and decision-making.</p>
</blockquote>
<h2 id="completeness-metrics"><a class="header" href="#completeness-metrics">Completeness Metrics</a></h2>
<p>To assess completeness, data teams utilize various measures and metrics that quantify the presence of data across different stages of the data infrastructure. Here's how completeness can be evaluated throughout the data ecosystem:</p>
<h3 id="data-sources-operational-data---missing-data-ratio"><a class="header" href="#data-sources-operational-data---missing-data-ratio">Data Sources (Operational Data) - Missing Data Ratio</a></h3>
<p>\[ Missing \ Data \ Ratio = \frac{Number\ of \ Missing \ Values}{Total \ Number \ of \ Values} \times 100 \]</p>
<p><strong>Application</strong>: Analyze operational data to identify missing values across critical fields. Use SQL queries or data profiling tools to calculate the missing data ratio for key attributes.</p>
<h3 id="elt-processes---record-completeness-rate"><a class="header" href="#elt-processes---record-completeness-rate">ELT Processes - Record Completeness Rate</a></h3>
<p>\[ Record \ Completeness \ Rate = \frac{Number \ of \ Complete \ Records}{Total \ Number \ of \ Records} \times 100 \]</p>
<p><strong>Application</strong>: After ELT processes, validate the completeness of records by checking for the presence of all expected fields. Automated data quality tools or custom scripts can be used to perform this validation.</p>
<h3 id="data-lakes-and-data-warehouses---dataset-completeness"><a class="header" href="#data-lakes-and-data-warehouses---dataset-completeness">Data Lakes and Data Warehouses - Dataset Completeness</a></h3>
<p><strong>Application</strong>: Ensure that all expected data is loaded into the data lake or warehouse and that datasets are complete. This can involve cross-referencing dataset inventories or metadata against expected data sources. There is no fixed formula, it involves assessing the presence of all expected datasets and their completeness.</p>
<h3 id="data-marts---attribute-completeness"><a class="header" href="#data-marts---attribute-completeness">Data Marts - Attribute Completeness</a></h3>
<p>\[ Attribute \ Completeness = \frac{Number \ of \ Records \ with \ Non-Missing \ Attribute \ Values}{Total \ Number \ of \ Records} \times 100 \]</p>
<p><strong>Application</strong>: For data marts tailored to specific business functions, assess the completeness of critical attributes that support business analysis. SQL queries or data quality tools can automate this assessment.</p>
<h3 id="reports-and-dashboards---information-completeness"><a class="header" href="#reports-and-dashboards---information-completeness">Reports and Dashboards - Information Completeness</a></h3>
<p><strong>Application</strong>: Ensure that reports and dashboards reflect complete information, with no missing data that could lead to incorrect insights. User feedback and manual validation play a key role in this stage. There are no fixed formulas. Qualitative assessment based on user feedback and data validation checks.</p>
<h2 id="completeness-metrics-examples"><a class="header" href="#completeness-metrics-examples">Completeness Metrics Examples</a></h2>
<p>Completeness as a data quality dimension can be quantified through various metrics tailored to different stages in your data pipeline. Here are some metrics you might consider:</p>
<h3 id="record-completeness-by-record"><a class="header" href="#record-completeness-by-record">Record Completeness by Record</a></h3>
<p>\[ Completeness \ Rate \ by \ Record = \frac{Number\ of \ Complete \ Records}{Total \ Number \ of \ Records} \times 100 \]</p>
<p><strong>Application</strong>: Evaluate the proportion of fully populated records in your datasets, where a "complete record" has all fields filled.</p>
<h3 id="field-completeness-rate"><a class="header" href="#field-completeness-rate">Field Completeness Rate</a></h3>
<p>\[ Field \ Completeness \ Rate = \frac{Number\ of \ Non-Null \ Field \ Entries}{Total \ Number \ of \ Field \ Entries} \times 100 \]</p>
<p><strong>Application</strong>: Measure the percentage of non-null entries for a specific field across all records, ensuring critical data attributes are not missing.</p>
<h3 id="source-coverage-rate"><a class="header" href="#source-coverage-rate">Source Coverage Rate</a></h3>
<p>\[ Source \ Coverage \ Rate = \frac{Number \ of \ Fields \ Captured \ by \ Source}{Total \ Number \ of \ Relevant \ Fields \ in \ Source} \times 100 \]</p>
<p><strong>Application</strong>: Monitor the extent to which the full range of relevant fields from the source databases are captured during the ELT process.</p>
<h3 id="historical-data-coverage-rate"><a class="header" href="#historical-data-coverage-rate">Historical Data Coverage Rate</a></h3>
<p>\[ Historical \ Data \ Coverage \ Rate = \frac{Number \ of \ Historical \ Records \ Loaded}{Expected \ Number \ of \ Historical \ Records} \times 100 \]</p>
<p><strong>Application</strong>: Ensure all expected historical data has been loaded into the data lake or warehouse.</p>
<h3 id="incremental-load-completeness-ratio"><a class="header" href="#incremental-load-completeness-ratio">Incremental Load Completeness Ratio</a></h3>
<p>\[ Incremental \ Load \ Completeness \ Ratio = \frac{Number \ of \ Records \ from \ Latest \ Load}{Expected \ Number \ of \ Records \ for \ the \ Period} \times 100 \]</p>
<p><strong>Application</strong>: Confirm that the data loaded during the most recent incremental load matches the expected volume for that load period.</p>
<h3 id="data-mart-coverage-rate"><a class="header" href="#data-mart-coverage-rate">Data Mart Coverage Rate</a></h3>
<p>\[ Data \ Mart \ Coverage \ Rate = \frac{Number \ of \ Fields \ Used \ in \ Data \ Mart}{Total \ Number \ of \ Available \ Fields} \times 100 \]</p>
<p><strong>Application</strong>: Check whether the data marts include all relevant fields from the staging schemas or upstream data sources for analytics and reporting.</p>
<p>For each of these metrics, you can use Airflow to schedule regular data quality checks, and dbt to perform data tests that evaluate completeness. Implementing these metrics will help ensure that your datasets in the data lake, data warehouse, and data marts are fully populated with the necessary information, enhancing the reliability of your data infrastructure for decision-making processes.</p>
<h2 id="ensuring-and-improving-completeness"><a class="header" href="#ensuring-and-improving-completeness">Ensuring and Improving Completeness</a></h2>
<p>To maintain high levels of completeness across the data infrastructure, several best practices can be implemented:</p>
<ul>
<li>
<p><strong>Data Profiling and Auditing</strong>:
Regularly profile and audit data at each stage of the pipeline to identify and address missing values or records.</p>
</li>
<li>
<p><strong>Data Quality Rules</strong>:
Implement data quality rules that enforce the presence of critical data elements during data entry and processing.</p>
</li>
<li>
<p><strong>Data Integration Checks</strong>:
During ELT processes, include checks to ensure all expected data is extracted and loaded, particularly when integrating data from multiple sources.</p>
</li>
<li>
<p><strong>Null Value Handling</strong>:
Develop strategies for handling null values, such as data imputation or default values, where appropriate, to maintain analytical integrity.</p>
</li>
<li>
<p><strong>User Training and Guidelines</strong>:
Educate data producers on the importance of data completeness and provide clear guidelines for data entry and maintenance.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="consistency-dimension-in-data-quality"><a class="header" href="#consistency-dimension-in-data-quality">Consistency Dimension in Data Quality</a></h1>
<blockquote>
<p>Consistency in data quality refers to the absence of discrepancy and contradiction in the data across different datasets, systems, or time periods. It ensures that data remains uniform, coherent, and aligned with predefined rules or formats across the entire data infrastructure, minimizing conflicts and errors that can arise from inconsistent data.</p>
</blockquote>
<h2 id="consistency-metrics"><a class="header" href="#consistency-metrics">Consistency Metrics</a></h2>
<p>To evaluate consistency, data teams apply specific metrics that help identify discrepancies within and across datasets. Here's how consistency can be assessed at various stages of the data infrastructure:</p>
<h3 id="data-sources-operational-data"><a class="header" href="#data-sources-operational-data">Data Sources (Operational Data)</a></h3>
<ul>
<li>
<p><strong>Cross-System Data Validation</strong>:
Compare data values and formats across different operational databases (like Postgres, Oracle, and MariaDB) to ensure they follow the same standards and rules.</p>
</li>
<li>
<p><strong>Reference Data Consistency</strong>:
Ensure that reference data (e.g. country codes, product categories) used across multiple systems is consistent and up-to-date.</p>
</li>
</ul>
<h4 id="example-cross-system-consistency-rate"><a class="header" href="#example-cross-system-consistency-rate">Example: Cross-System Consistency Rate</a></h4>
<p>\[ Consistency \ Ratio = \frac{Number\ of \ Consistent \ Records \ Across \ Systems}{Total \ Number \ of \ Compared \ Records} \times 100 \]</p>
<p><strong>Application</strong>: Compare key data elements (e.g., customer information, and product details) across different operational systems to identify inconsistencies. SQL queries or data comparison tools can facilitate this process.</p>
<h3 id="elt-processes"><a class="header" href="#elt-processes">ELT Processes</a></h3>
<ul>
<li>
<p><strong>Schema Consistency Checks</strong>:
During ELT processes, especially with tools like AWS DMS, validate that the applied schema transformations maintain consistency in data types, formats, and naming conventions across source and target systems.</p>
</li>
<li>
<p><strong>Data Transformation Logic Validation</strong>:
Verify that the transformation logic in ELT does not introduce inconsistencies, especially when aggregating or modifying data.</p>
</li>
</ul>
<h4 id="example-transformation-consistency-check"><a class="header" href="#example-transformation-consistency-check">Example: Transformation Consistency Check</a></h4>
<p><strong>Application</strong>: Consists of implementing automated checks or tests within ELT pipelines to ensure that loaded data maintains data integrity. There is no fixed formula; it involves verifying that data transformations produce consistent results across different batches or datasets.</p>
<h3 id="data-lakes-and-data-warehouses"><a class="header" href="#data-lakes-and-data-warehouses">Data Lakes and Data Warehouses</a></h3>
<ul>
<li>
<p><strong>Historical Data Alignment</strong>:
Check that historical data loaded into data lakes or warehouses remains consistent with current operational data in terms of structure, format, and content.</p>
</li>
<li>
<p><strong>Dimension Table Consistency</strong>:
In data warehousing, ensure that dimension tables (like customer or product dimensions) maintain consistent attribute values over time, even as new data is integrated.</p>
</li>
</ul>
<h4 id="example-historical-data-consistency"><a class="header" href="#example-historical-data-consistency">Example: Historical Data Consistency</a></h4>
<p>\[ Historical \ Consistency \ Rate = \frac{Number\ of \ Records \ Matching \ Historical \ Patterns}{Total \ Number \ of \ Records} \times 100 \]</p>
<p><strong>Application</strong>: Analyze time-series data or historical records within the data lake or warehouse to ensure that data remains consistent over time. This may involve trend analysis or anomaly detection techniques.</p>
<h3 id="data-marts"><a class="header" href="#data-marts">Data Marts</a></h3>
<ul>
<li>
<p><strong>Report Data Consistency</strong>:
Validate that the data used in different data marts for reporting purposes remains consistent, providing a unified view to end-users.</p>
</li>
<li>
<p><strong>Metric Definitions Alignment</strong>:
Ensure that business metrics calculated across various data marts adhere to a single definition to prevent discrepancies in reports.</p>
</li>
</ul>
<h4 id="example-dimensional-consistency"><a class="header" href="#example-dimensional-consistency">Example: Dimensional Consistency</a></h4>
<p>\[ Dimensional \ Consistency \ Rate = \frac{Number\ of \ Consistent \ Dimension \ Records}{Total \ Number \ of \ Dimension \ Records} \times 100 \]</p>
<p><strong>Application</strong>: Assess the consistency of dimension tables (e.g., time dimensions, geographical hierarchies) to ensure they align with business rules and definitions.</p>
<h2 id="ensuring-and-improving-consistency"><a class="header" href="#ensuring-and-improving-consistency">Ensuring and Improving Consistency</a></h2>
<p>Strategies to maintain and enhance data consistency across the data infrastructure include:</p>
<ul>
<li>
<p><strong>Standardization</strong>:
Develop and enforce data standards and conventions across the organization to ensure consistency in data entry, formatting, and processing.</p>
</li>
<li>
<p><strong>Centralized Data Catalogs</strong>:
Maintain centralized data catalogs or dictionaries that define data elements, their acceptable values, and formats to guide consistent data usage.</p>
</li>
<li>
<p><strong>Automated Validation</strong>:
Incorporate automated validation rules and checks in data pipelines to detect and correct inconsistencies as data moves through ELT processes.</p>
</li>
<li>
<p><strong>Master Data Management (MDM)</strong>:
Implement MDM practices to manage key data entities centrally, ensuring consistent reference data across systems.</p>
</li>
<li>
<p><strong>Data Reconciliation</strong>:
Regularly perform data reconciliation exercises to align data across different systems, particularly after significant data migrations or integrations.</p>
</li>
</ul>
<p>Maintaining data consistency is crucial for ensuring that analyses, reports, and business decisions based on the data are accurate and reliable. It reduces confusion, increases trust in data systems, and enhances the overall quality of data available to stakeholders.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="timeliness-dimension-in-data-quality"><a class="header" href="#timeliness-dimension-in-data-quality">Timeliness Dimension in Data Quality</a></h1>
<blockquote>
<p>Timeliness refers to the degree to which data is up-to-date and available when required. It's a critical dimension of data quality that ensures data is current and provided within an acceptable timeframe, making it particularly relevant for time-sensitive decisions and operations.</p>
</blockquote>
<h2 id="timeliness-metrics"><a class="header" href="#timeliness-metrics">Timeliness Metrics</a></h2>
<p>Assessing timeliness involves metrics that quantify the availability and currency of data across the data infrastructure. Here's how timeliness can be evaluated at different stages:</p>
<h3 id="data-sources-operational-data---data-latency"><a class="header" href="#data-sources-operational-data---data-latency">Data Sources (Operational Data) - Data Latency</a></h3>
<p>\[ Data \ Latency = Current \ Time - Data \ Creation \ Time \]</p>
<p><strong>Application</strong>: Measure the time taken for data generated by operational systems to become available for use. Lower latency indicates higher timeliness.</p>
<h3 id="elt-processes---process-duration"><a class="header" href="#elt-processes---process-duration">ELT Processes - Process Duration</a></h3>
<p>\[ Process \ Duration = Process \ End \ Time - Process \ Start \ Time \]</p>
<p><strong>Application</strong>: Track the duration of ELT processes to ensure data is processed and made available within expected timeframes. Monitoring tools or logging within ELT pipelines can facilitate this measurement.</p>
<h3 id="data-lakes-and-data-warehouses---refresh-rate"><a class="header" href="#data-lakes-and-data-warehouses---refresh-rate">Data Lakes and Data Warehouses - Refresh Rate</a></h3>
<p>\[ Refresh \ Rate = \frac{1}{Time \ Between \ Data \ Refreshes} \]</p>
<p><strong>Application</strong>: Assess the frequency at which data in the data lake or warehouse is updated. Higher refresh rates indicate more timely data.</p>
<h3 id="data-marts---data-availability-delay"><a class="header" href="#data-marts---data-availability-delay">Data Marts - Data Availability Delay</a></h3>
<p>\[ Data \ Availability \ Delay = Data \ Mart \ Availability \ Time - Data \ Warehouse \ Availability \ Time \]</p>
<p><strong>Application</strong>: Measure the time lag between data being updated in the data warehouse and its availability in specific data marts. Shorter delays signify better timeliness. In the case of multiple data sources, consider the time of the last available data.</p>
<h2 id="ensuring-and-improving-timeliness"><a class="header" href="#ensuring-and-improving-timeliness">Ensuring and Improving Timeliness</a></h2>
<p>To maintain and boost the timeliness of data across the data infrastructure, consider the following strategies:</p>
<ul>
<li>
<p><strong>Real-Time Data Processing</strong>:
Implement real-time or near-real-time data processing capabilities to minimize latency and ensure data is promptly available for decision-making.</p>
</li>
<li>
<p><strong>Optimize ELT Processes</strong>:
Regularly review and optimize ELT processes to reduce processing time, employing parallel processing, efficient algorithms, and appropriate hardware resources.</p>
</li>
<li>
<p><strong>Incremental Updates</strong>:
Rather than full refreshes, use incremental data updates where possible to reduce the time taken to update data stores.</p>
</li>
<li>
<p><strong>Monitoring and Alerts</strong>:
Establish monitoring systems to track the timeliness of data processes, with alerts set up to notify relevant teams of any delays or issues.</p>
</li>
<li>
<p><strong>Service Level Agreements (SLAs)</strong>:
Define SLAs for data timeliness, clearly outlining expected timeframes for data availability at each stage of the data infrastructure.</p>
</li>
</ul>
<h2 id="timeliness-metrics-examples"><a class="header" href="#timeliness-metrics-examples">Timeliness Metrics Examples</a></h2>
<p>Timeliness in data quality ensures that data is not only current but also available at the right time for decision-making and operational processes. Here are some examples of timeliness metrics that are commonly applied in various business contexts:</p>
<h3 id="data-update-latency"><a class="header" href="#data-update-latency">Data Update Latency</a></h3>
<p><strong>Application</strong>: Measure the time taken from when data is created or captured in source systems to when it becomes available in target systems or databases.</p>
<p><strong>Example</strong>: An e-commerce company might measure the latency from the time an order is placed online to when the order data is available in the analytics database for reporting.</p>
<h3 id="data-refresh-rate"><a class="header" href="#data-refresh-rate">Data Refresh Rate</a></h3>
<p><strong>Application</strong>: Monitor the frequency at which data sets are updated or refreshed to ensure they meet the required cadence for business operations or reporting needs.</p>
<p><strong>Example</strong>: A financial analytics firm may track how frequently market data feeds are refreshed to ensure traders have access to the most current information.</p>
<h3 id="real-time-data-delivery-compliance"><a class="header" href="#real-time-data-delivery-compliance">Real-time Data Delivery Compliance</a></h3>
<p><strong>Application</strong>: Evaluate the percentage of data that is delivered in real-time or near-real-time against the total data that requires immediate availability.</p>
<p><strong>Example</strong>: A logistics company could assess the compliance of real-time tracking data for shipments, ensuring it meets the expected standards for timeliness in delivery tracking.</p>
<h3 id="service-level-agreement-sla-compliance-rate"><a class="header" href="#service-level-agreement-sla-compliance-rate">Service Level Agreement (SLA) Compliance Rate</a></h3>
<blockquote>
<p><strong>Application</strong>: Measure the percentage of data-related operations (like data loading, processing, or delivery) that meet predefined SLA requirements.</p>
</blockquote>
<p><strong>Example</strong>: An IT service provider may monitor its compliance with SLAs for data backup and recovery times, ensuring that services meet contractual timeliness obligations.</p>
<h3 id="average-data-age"><a class="header" href="#average-data-age">Average Data Age</a></h3>
<p><strong>Application</strong>: Calculate the average "age" of data in a system to assess how current the data is. This is particularly relevant for data that loses value over time.</p>
<p><strong>Example</strong>: A news aggregation platform might evaluate the average age of news articles to ensure content is fresh and relevant to its audience.</p>
<h3 id="outdated-records-percentage"><a class="header" href="#outdated-records-percentage">Outdated Records Percentage</a></h3>
<p><strong>Application</strong>: Identify and quantify the proportion of records that are beyond their useful lifespan or haven't been updated within an expected timeframe.</p>
<p><strong>Example</strong>: A healthcare provider may analyze patient records to determine what percentage are outdated, ensuring patient information is current for clinical decisions.</p>
<h3 id="data-access-window-compliance"><a class="header" href="#data-access-window-compliance">Data Access Window Compliance</a></h3>
<p><strong>Application</strong>: Assess whether data is accessible within predefined windows of time, especially for batch-processed or cyclically updated data.</p>
<p><strong>Example</strong>: A retail chain could measure compliance with the data availability window for sales reports, ensuring store managers have access to daily sales data each morning.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="relevance-dimension-in-data-quality"><a class="header" href="#relevance-dimension-in-data-quality">Relevance Dimension in Data Quality</a></h1>
<blockquote>
<p>Relevance in data quality refers to the extent to which data is applicable and useful for the purposes it is intended for. It ensures that the data collected and maintained aligns with the current needs and objectives of the business, supporting effective decision-making and operational processes.</p>
</blockquote>
<h2 id="relevance-metrics"><a class="header" href="#relevance-metrics">Relevance Metrics</a></h2>
<p>Assessing the relevance of data involves evaluating how well the data meets the specific requirements and objectives of various stakeholders, including business units, data analysts, and decision-makers. Here's how relevance can be evaluated across different stages of the data infrastructure:</p>
<h3 id="data-sources-operational-data---data-utilization-rate"><a class="header" href="#data-sources-operational-data---data-utilization-rate">Data Sources (Operational Data) - Data Utilization Rate</a></h3>
<p>\[ Data \ Utilization \ Rate = \frac{Number\ of \ Data \ Elements \ Used \ in \ Decision-Making}{Total \ Number \ of \ Data \ Elements \ Available} \times 100 \]</p>
<p><strong>Application</strong>: Analyze operational data to identify which data elements are actively used in decision-making processes. This can be done through user surveys, data access logs, or analytics on database queries.</p>
<h3 id="data-lakes-and-data-warehouses---data-coverage-ratio"><a class="header" href="#data-lakes-and-data-warehouses---data-coverage-ratio">Data Lakes and Data Warehouses - Data Coverage Ratio</a></h3>
<p>\[ Data \ Coverage \ Ratio = \frac{Number\ of \ Business \ Questions \ Answerable \ with \ Data}{Total \ Number \ of \ Business \ Questions} \times 100 \]</p>
<p><strong>Application</strong>: Evaluate the extent to which data stored in the data lake or warehouse can answer key business questions. This may involve mapping data elements to specific business use cases or analytics requirements.</p>
<h3 id="data-marts---business-alignment-index"><a class="header" href="#data-marts---business-alignment-index">Data Marts - Business Alignment Index</a></h3>
<p>In data marts designed for specific business functions, assess how well the data aligns with the department's KPIs and objectives. This could involve regular reviews with department heads and key users to ensure the data remains relevant to their needs. It is a qualitative assessment based on alignment with departmental objectives and key performance indicators (KPIs).</p>
<h3 id="reports-and-dashboards---user-engagement-score"><a class="header" href="#reports-and-dashboards---user-engagement-score">Reports and Dashboards - User Engagement Score</a></h3>
<p>\[ User \ Engagement \ Score = \frac{Number\ of \ Active \ User \ Interactions \ with \ Reports \ or \ Dashboards}{Total \ Number \ of \ Reports \ or \ Dashboards \ Available} \]</p>
<p><strong>Application</strong>: Monitor user engagement with reports and dashboards to gauge their relevance. High interaction rates may suggest that the information presented is relevant and useful to the users.</p>
<h2 id="ensuring-and-improving-relevance"><a class="header" href="#ensuring-and-improving-relevance">Ensuring and Improving Relevance</a></h2>
<p>Strategies to maintain and enhance the relevance of data across the data infrastructure include:</p>
<ul>
<li>
<p><strong>Regular Needs Assessment</strong>:
Conduct periodic assessments with data users and stakeholders to understand their evolving data needs and ensure that the data infrastructure aligns with these requirements.</p>
</li>
<li>
<p><strong>Agile Data Management</strong>:
Adopt agile data management practices that allow for the flexible and rapid adaptation of data processes and structures in response to changing business needs.</p>
</li>
<li>
<p><strong>Feedback Loops</strong>:
Implement mechanisms for collecting ongoing feedback from data users on the relevance of data and reports, using this feedback to guide data collection, transformation, and presentation efforts.</p>
</li>
<li>
<p><strong>Data Lifecycle Management</strong>:
Establish policies for data archiving and purging, ensuring that only relevant, current data is actively maintained and available for use, reducing clutter, and focusing on valuable data assets.</p>
</li>
</ul>
<h2 id="relevance-metrics-examples"><a class="header" href="#relevance-metrics-examples">Relevance Metrics Examples</a></h2>
<p>Relevance in the context of data quality ensures that the data collected and maintained is applicable, meaningful, and useful for the business purposes it is intended for. Here are some examples of relevance metrics that can be applied in various business scenarios:</p>
<h3 id="data-utilization-rate"><a class="header" href="#data-utilization-rate">Data Utilization Rate</a></h3>
<p><strong>Application</strong>: Measure the percentage of collected data that is actively used in decision-making or operational processes, indicating its relevance to current business needs.</p>
<p><strong>Example</strong>: A marketing department might track the utilization rate of customer data in campaign planning to ensure the data collected is relevant and actively employed in marketing strategies.</p>
<h3 id="data-relevance-score"><a class="header" href="#data-relevance-score">Data Relevance Score</a></h3>
<p><strong>Application</strong>: Assign scores to datasets based on predefined criteria that reflect their importance and applicability to current business objectives or projects.</p>
<p><strong>Example</strong>: A project management office could score project data based on its relevance to strategic initiatives, focusing resources on the most pertinent projects.</p>
<h3 id="data-coverage-adequacy"><a class="header" href="#data-coverage-adequacy">Data Coverage Adequacy</a></h3>
<p><strong>Application</strong>: Assess whether the scope and granularity of collected data cover all necessary aspects of a business process or area, ensuring its relevance and completeness.</p>
<p><strong>Example</strong>: An operations team in a manufacturing firm may evaluate the adequacy of sensor data coverage in monitoring production lines, ensuring critical parameters are tracked for optimal performance.</p>
<h3 id="obsolete-data-percentage"><a class="header" href="#obsolete-data-percentage">Obsolete Data Percentage</a></h3>
<p><strong>Application</strong>: Identify and quantify the proportion of data that is no longer relevant or applicable to current business processes or objectives.</p>
<p><strong>Example</strong>: An IT department might calculate the percentage of obsolete data within its systems to streamline data storage and focus on maintaining relevant data.</p>
<h3 id="user-feedback-score-on-data-relevance"><a class="header" href="#user-feedback-score-on-data-relevance">User Feedback Score on Data Relevance</a></h3>
<p><strong>Application</strong>: Collect and analyze user feedback to gauge the perceived relevance of data sets or reports, using scores or ratings to quantify satisfaction.</p>
<p><strong>Example</strong>: A business intelligence team could gather feedback from end-users on the relevance of dashboards and reports, using this input to tailor data presentations to user needs.</p>
<h3 id="data-strategy-alignment-index"><a class="header" href="#data-strategy-alignment-index">Data-Strategy Alignment Index</a></h3>
<p><strong>Application</strong>: Evaluate how well data assets align with strategic business objectives, ensuring that data collection and management efforts are directed towards relevant business goals.</p>
<p><strong>Example</strong>: A strategic planning department might use an alignment index to assess how well data initiatives support overarching business strategies, ensuring efforts are not misdirected.</p>
<h3 id="decision-impact-analysis"><a class="header" href="#decision-impact-analysis">Decision Impact Analysis</a></h3>
<p><strong>Application</strong>: Analyze the impact of data on key business decisions to determine its relevance and effectiveness in supporting those decisions.</p>
<p><strong>Example</strong>: A financial analytics team could retrospectively analyze how data-driven recommendations impacted investment decisions, assessing the relevance of the data used.</p>
<p>Implementing these relevance metrics helps organizations ensure that their data assets remain aligned with current business needs, objectives, and processes. By regularly assessing the relevance of their data, businesses can make informed decisions about data collection, retention, and utilization strategies, ensuring that resources are allocated efficiently and effectively to maintain data that offers real value and supports the organization's goals.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reliability-dimension-in-data-quality"><a class="header" href="#reliability-dimension-in-data-quality">Reliability Dimension in Data Quality</a></h1>
<blockquote>
<p>Reliability in the context of data quality refers to the degree of trustworthiness and dependability of the data, ensuring it consistently produces the same results under similar conditions and over time. Reliable data is crucial for maintaining the integrity of analyses, reports, and business decisions derived from that data.</p>
</blockquote>
<h2 id="reliability-metrics"><a class="header" href="#reliability-metrics">Reliability Metrics</a></h2>
<p>To evaluate the reliability of data, it's essential to consider various aspects such as source credibility, data collection consistency, and the stability of data values over time. Here's how reliability can be assessed across different stages of the data infrastructure:</p>
<h3 id="data-sources-operational-data--source-credibility-score"><a class="header" href="#data-sources-operational-data--source-credibility-score">Data Sources (Operational Data)- Source Credibility Score</a></h3>
<p><strong>Application</strong>: Evaluate each data source's reliability by considering its track record, reputation, and any third-party certifications or audits. This could involve a review of source documentation and user feedback. It is a qualitative assessment based on the source's historical accuracy, authority, and trustworthiness.</p>
<h3 id="elt-processes--process-stability-index"><a class="header" href="#elt-processes--process-stability-index">ELT Processes- Process Stability Index</a></h3>
<p>\[ Process \ Stability \ Index = \frac{Number \ of \ Successful \ ELT \ Runs}{Total \ Number \ of \ ELT \ Runs} \times 100 \]</p>
<p><strong>Application</strong>: Monitor the stability and consistency of ELT processes by tracking the success rate of data extraction, loading, and transformation jobs. High stability indicates reliable data processing.</p>
<h3 id="data-lakes-and-data-warehouses---data-variation-coefficient"><a class="header" href="#data-lakes-and-data-warehouses---data-variation-coefficient">Data Lakes and Data Warehouses - Data Variation Coefficient</a></h3>
<p>\[ Data \ Variation \ Coefficient = \frac{Standard \ Deviation \ of \ Data \ Values}{Mean \ of \ Data \ Values} \]</p>
<p><strong>Application</strong>: Analyze the variation in data values stored in the data lake or warehouse, especially for key metrics, to assess the stability and reliability of the data over time.</p>
<h3 id="data-marts---data-consensus-ratio"><a class="header" href="#data-marts---data-consensus-ratio">Data Marts - Data Consensus Ratio</a></h3>
<p>\[ Data \ Consensus \ Ratio = \frac{Number \ of \ Data \ Points \ in \ Agreement \ with \ Consensus \ Value}{Total \ Number \ of \ Data \ Points} \times 100 \]</p>
<p>For data marts serving specific business functions, evaluate the consistency of data with established benchmarks or consensus values, ensuring that the data reliably reflects business realities.</p>
<h3 id="reports-and-dashboards---user-trust-index"><a class="header" href="#reports-and-dashboards---user-trust-index">Reports and Dashboards - User Trust Index</a></h3>
<p><strong>Application</strong>: Gauge the level of trust users have in reports and dashboards by collecting feedback on their experiences and perceptions of data accuracy, consistency, and reliability. It is a qualitative assessment based on user surveys and feedback regarding their trust in the data presented.</p>
<h2 id="ensuring-and-improving-reliability"><a class="header" href="#ensuring-and-improving-reliability">Ensuring and Improving Reliability</a></h2>
<p>Strategies to maintain and enhance data reliability across the data infrastructure include:</p>
<ul>
<li>
<p><strong>Data Source Validation</strong>:
Regularly validate and audit data sources to ensure they continue to meet quality and reliability standards.</p>
</li>
<li>
<p><strong>Robust Data Processing</strong>:
Design ELT processes with error handling, logging, and recovery mechanisms to maintain consistency and reliability in data processing.</p>
</li>
<li>
<p><strong>Historical Data Tracking</strong>:
Maintain historical data records and change logs to track data stability and reliability over time, facilitating audits and reliability assessments.</p>
</li>
<li>
<p><strong>User Education and Communication</strong>:
Educate users about the sources, processes, and controls in place to ensure data reliability, building user trust and confidence in the data.</p>
</li>
</ul>
<h2 id="reliability-metrics-examples"><a class="header" href="#reliability-metrics-examples">Reliability Metrics Examples</a></h2>
<p>Reliability in data quality is fundamental for ensuring that data can be trusted and relied upon for consistent decision-making and analysis. Here are some examples of reliability metrics that are often applied in real-world business contexts:</p>
<h3 id="data-source-reliability-score"><a class="header" href="#data-source-reliability-score">Data Source Reliability Score</a></h3>
<p><strong>Application</strong>: Assess and rate the reliability of different data sources based on criteria such as source stability, historical accuracy, and frequency of updates.</p>
<p><strong>Example</strong>: A data governance team might evaluate and score the reliability of external data providers to determine which sources are most dependable for financial market data.</p>
<h3 id="data-error-rate"><a class="header" href="#data-error-rate">Data Error Rate</a></h3>
<p><strong>Application</strong>: Measure the frequency of errors in data collection, entry, or processing within a given time period, indicating the reliability of data handling processes.</p>
<p><strong>Example</strong>: An e-commerce platform may track the error rate in customer transaction data to ensure the reliability of sales and inventory data.</p>
<h3 id="data-reproducibility-index"><a class="header" href="#data-reproducibility-index">Data Reproducibility Index</a></h3>
<p><strong>Application</strong>: Evaluate the extent to which data analyses or reports can be consistently reproduced using the same data and methodologies, indicating the reliability of the data and analytical processes.</p>
<p><strong>Example</strong>: A research department might use a reproducibility index to ensure that analytical results can be consistently replicated, confirming the reliability of their data and analyses.</p>
<h3 id="data-recovery-success-rate"><a class="header" href="#data-recovery-success-rate">Data Recovery Success Rate</a></h3>
<p><strong>Application</strong>: Measure the effectiveness of data backup and recovery processes by quantifying the rate of successful data restorations after incidents.</p>
<p><strong>Example</strong>: An IT operations team could track the success rate of data recovery drills to ensure that critical business data can be reliably restored in the event of a system failure.</p>
<h3 id="data-validation-pass-rate"><a class="header" href="#data-validation-pass-rate">Data Validation Pass Rate</a></h3>
<p><strong>Application</strong>: Quantify the proportion of data that passes predefined validation checks, reflecting the reliability of the data in meeting quality standards.</p>
<p><strong>Example</strong>: A data ingestion pipeline might monitor the pass rate of incoming data against validation rules to ensure the reliability of data being stored in a data warehouse.</p>
<h3 id="data-consistency-rate-across-sources"><a class="header" href="#data-consistency-rate-across-sources">Data Consistency Rate Across Sources</a></h3>
<p><strong>Application</strong>: Measure the degree of consistency in data across various sources or systems, indicating the reliability of data integration processes.</p>
<p><strong>Example</strong>: A multinational corporation may assess the consistency rate of customer data across regional databases to ensure reliable, unified customer views.</p>
<h3 id="system-uptime-and-availability"><a class="header" href="#system-uptime-and-availability">System Uptime and Availability</a></h3>
<p><strong>Application</strong>: Track the uptime and availability of critical data systems and platforms, as system reliability directly impacts data reliability.</p>
<p><strong>Example</strong>: A cloud services provider might monitor the uptime of data storage services to guarantee reliable access to data for their clients.</p>
<p>By implementing these reliability metrics, businesses can monitor and improve the trustworthiness and dependability of their data. Reliable data is essential for ensuring that analyses, reports, and decisions are based on accurate and consistent information, thereby supporting effective business operations and strategic initiatives.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="uniqueness-dimension-in-data-quality"><a class="header" href="#uniqueness-dimension-in-data-quality">Uniqueness Dimension in Data Quality</a></h1>
<blockquote>
<p>Uniqueness is a critical dimension of data quality that ensures each data item or entity is represented only once within a dataset or across integrated systems. It aims to prevent duplicates, which can lead to inaccuracies in analysis, reporting, and decision-making processes. Ensuring uniqueness is particularly important in databases, data warehouses, and customer relationship management (CRM) systems where the integrity of data like customer records, product information, and transaction details is important.</p>
</blockquote>
<h2 id="uniqueness-metrics"><a class="header" href="#uniqueness-metrics">Uniqueness Metrics</a></h2>
<p>To assess the uniqueness of data, data teams utilize specific metrics that help identify and quantify duplicate entries within their datasets. Here's how uniqueness can be evaluated across different stages of the data infrastructure:</p>
<h3 id="data-sources-operational-data---duplication-rate"><a class="header" href="#data-sources-operational-data---duplication-rate">Data Sources (Operational Data) - Duplication Rate</a></h3>
<p>\[ Duplication \ Rate = \frac{Number\ of \ Duplicate \ Records}{Total \ Number \ of \ Records} \times 100 \]</p>
<p><strong>Application</strong>: Analyze operational data for duplicate entries by comparing key identifiers (e.g., customer IDs, product codes) within the source system. SQL queries or data profiling tools can facilitate this process.</p>
<h3 id="data-lakes-and-data-warehouses---entity-uniqueness-score"><a class="header" href="#data-lakes-and-data-warehouses---entity-uniqueness-score">Data Lakes and Data Warehouses - Entity Uniqueness Score</a></h3>
<p>\[ Entity \ Uniqueness \ Score = \frac{Number \ of \ Unique \ Entity \ Records}{Total \ Number \ of \ Entity \ Records} \times 100 \]</p>
<p><strong>Application</strong>: In data lakes and warehouses, assess the uniqueness of entities across datasets by comparing key attributes. Data quality tools can automate the identification of duplicates across disparate datasets.</p>
<h3 id="data-marts---dimensional-key-uniqueness"><a class="header" href="#data-marts---dimensional-key-uniqueness">Data Marts - Dimensional Key Uniqueness</a></h3>
<p>\[ Dimensional \ Key \ Uniqueness = \frac{Number \ of \ Unique \ Dimension \ Keys}{Total \ Number \ of \ Dimension \ Records} \times 100 \]</p>
<p><strong>Application</strong>: For data marts, ensure that dimensional keys (e.g., time dimensions, product dimensions) are unique to maintain data integrity and accurate reporting.</p>
<h3 id="reports-and-dashboards---report-data-redundancy-check"><a class="header" href="#reports-and-dashboards---report-data-redundancy-check">Reports and Dashboards - Report Data Redundancy Check</a></h3>
<p><strong>Application</strong>: Validate that reports and dashboards do not present redundant information, which could mislead decision-making. This involves both user feedback and automated data validation techniques. It is a qualitative assessment based on user validation and automated data checks.</p>
<h3 id="ensuring-and-improving-uniqueness"><a class="header" href="#ensuring-and-improving-uniqueness">Ensuring and Improving Uniqueness</a></h3>
<p>To maintain high levels of uniqueness across the data infrastructure, several best practices can be implemented:</p>
<ul>
<li>
<p><strong>De-duplication Processes</strong>:
Establish automated de-duplication routines within ELT processes to identify and resolve duplicates before they enter the data warehouse or data marts.</p>
</li>
<li>
<p><strong>Master Data Management (MDM)</strong>:
Implement MDM practices to manage key entities centrally, ensuring a single source of truth and preventing duplicates across systems.</p>
</li>
<li>
<p><strong>Key and Index Management</strong>:
Use primary keys and unique indexes in database design to enforce uniqueness at the data storage level.</p>
</li>
<li>
<p><strong>Regular Data Audits</strong>:
Conduct periodic audits of data to identify and rectify duplication issues, ensuring ongoing data quality.</p>
</li>
<li>
<p><strong>User Training and Guidelines</strong>:
Educate data entry personnel on the importance of data uniqueness and provide clear guidelines for maintaining it during data collection and entry.</p>
</li>
</ul>
<h2 id="uniqueness-metrics-examples"><a class="header" href="#uniqueness-metrics-examples">Uniqueness Metrics Examples</a></h2>
<p>Uniqueness in data quality plays a crucial role in maintaining the integrity and usefulness of data, especially in environments where the accuracy of records is paramount. Here are examples of metrics that can be applied to measure and ensure the uniqueness dimension in various data environments:</p>
<h3 id="duplicate-record-rate"><a class="header" href="#duplicate-record-rate">Duplicate Record Rate</a></h3>
<p><strong>Application</strong>: Calculate the percentage of duplicate records within a dataset to identify the extent of redundancy in data storage.</p>
<p><strong>Example</strong>: In a CRM system, this metric can help identify duplicate customer profiles, ensuring each customer is represented only once.</p>
<h3 id="unique-entity-ratio"><a class="header" href="#unique-entity-ratio">Unique Entity Ratio</a></h3>
<p><strong>Application</strong>: Measure the ratio of unique entities (such as customers, products, or transactions) to the total number of records, highlighting the effectiveness of deduplication efforts.</p>
<p><strong>Example</strong>: An e-commerce platform might use this metric to ensure that each product listing is unique and not duplicated across different categories.</p>
<h3 id="key-integrity-index"><a class="header" href="#key-integrity-index">Key Integrity Index</a></h3>
<p><strong>Application</strong>: Assess the integrity of primary and foreign keys in relational databases, ensuring that each key uniquely identifies a record without overlaps.</p>
<p><strong>Example</strong>: In a data warehouse, maintaining a high key integrity index is crucial to ensure that joins and relationships between tables accurately reflect unique entities.</p>
<h3 id="cross-system-uniqueness-verification"><a class="header" href="#cross-system-uniqueness-verification">Cross-System Uniqueness Verification</a></h3>
<p><strong>Application</strong>: Verify that entities are unique not just within a single system but across interconnected systems, essential for integrated data environments.</p>
<p><strong>Example</strong>: A business might check that employee IDs are unique not only within the HR system but also across access control, payroll, and other internal systems.</p>
<h3 id="incremental-load-uniqueness-check"><a class="header" href="#incremental-load-uniqueness-check">Incremental Load Uniqueness Check</a></h3>
<p><strong>Application</strong>: During data ETL (Extract, Transform, Load) processes, ensure that each incrementally loaded record is unique and does not duplicate existing data.</p>
<p><strong>Example</strong>: When loading daily sales transactions into a data warehouse, this metric ensures each transaction is recorded once, even across multiple loads.</p>
<h3 id="uniqueness-trend-over-time"><a class="header" href="#uniqueness-trend-over-time">Uniqueness Trend Over Time</a></h3>
<p><strong>Application</strong>: Monitor the trend of unique records over time to identify patterns or changes in data capture processes that may affect data uniqueness.</p>
<p><strong>Example</strong>: An organization might track the uniqueness trend of contact information in its marketing database to ensure that data collection methods continue to produce unique entries.</p>
<h3 id="match-and-merge-effectiveness"><a class="header" href="#match-and-merge-effectiveness">Match and Merge Effectiveness</a></h3>
<p><strong>Application</strong>: In systems employing match-and-merge techniques for deduplication, measure the effectiveness of these operations in consolidating duplicate records into unique entities.</p>
<p><strong>Example</strong>: In healthcare databases, this metric can ensure patient records are uniquely merged from various sources without losing critical information.</p>
<p>By monitoring these uniqueness metrics, organizations can detect and address issues related to duplicate data, thereby enhancing the quality and reliability of their information assets. Ensuring data uniqueness is essential for accurate analytics, efficient operations, and effective decision-making, particularly in contexts where the precision of each data entity is critical.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="validity-dimension-in-data-quality"><a class="header" href="#validity-dimension-in-data-quality">Validity Dimension in Data Quality</a></h1>
<blockquote>
<p>Validity in data quality refers to the degree to which data conforms to specific syntax (format, type, range) and semantic (meaningful and appropriate content) rules defined by the data model and business requirements. Valid data adheres to predefined formats, standards, and constraints, ensuring that it is both structurally sound and contextually meaningful for its intended use.</p>
</blockquote>
<h2 id="validity-metrics"><a class="header" href="#validity-metrics">Validity Metrics</a></h2>
<p>Assessing validity involves checking data against established rules and constraints to ensure it meets the required standards for format, type, range, and content. Here's how validity can be evaluated across different stages of the data infrastructure:</p>
<h3 id="data-sources-operational-data---format-conformance-rate"><a class="header" href="#data-sources-operational-data---format-conformance-rate">Data Sources (Operational Data) - Format Conformance Rate</a></h3>
<p>\[ Format \ Conformance \ Rate = \frac{Number\ of \ Records \ Meeting \ Format \ Specifications}{Total \ Number \ of \ Records} \times 100 \]</p>
<p><strong>Application</strong>: Analyze operational data to ensure that it conforms to expected formats (e.g., date formats, postal codes). This can be done using SQL queries or data profiling tools to check data formats against predefined patterns.</p>
<h3 id="data-lakes-and-data-warehouses---data-type-integrity-score"><a class="header" href="#data-lakes-and-data-warehouses---data-type-integrity-score">Data Lakes and Data Warehouses - Data Type Integrity Score</a></h3>
<p>\[ Data \ Type \ Integrity \ Rate = \frac{Number \ of \ Records \ with \ Correct \ Data \ Types}{Total \ Number \ of \ Records} \times 100 \]</p>
<p><strong>Application</strong>: In data lakes and warehouses, assess the integrity of data types to ensure that data is stored in the correct format (e.g., numeric fields are stored as numbers). Automated data quality tools can scan datasets to identify type mismatches.</p>
<h3 id="data-marts---business-rule-compliance-rate"><a class="header" href="#data-marts---business-rule-compliance-rate">Data Marts - Business Rule Compliance Rate</a></h3>
<p>\[ Business \ Rule \ Compliance \ Rate = \frac{Number \ of \ Records \ Complying \ with \ Business \ Rules}{Total \ Number \ of \ Records} \times 100 \]</p>
<p><strong>Application</strong>: For data marts, ensure that data complies with specific business rules relevant to the department or function. This involves setting up rule-based validation checks that can be run on the data mart contents.</p>
<h2 id="ensuring-and-improving-validity"><a class="header" href="#ensuring-and-improving-validity">Ensuring and Improving Validity</a></h2>
<p>Strategies to maintain and enhance data validity across the data infrastructure include:</p>
<ul>
<li>
<p><strong>Validation Rules and Constraints</strong>:
Implement comprehensive validation rules and constraints at the point of data entry and throughout data processing pipelines to ensure data validity.</p>
</li>
<li>
<p><strong>Data Quality Tools</strong>:
Utilize data quality tools that offer automated validation capabilities, allowing for the continuous checking of data against validity rules.</p>
</li>
<li>
<p><strong>Data Cleansing</strong>:
Engage in regular data cleansing activities to correct invalid data, using scripts or data quality platforms to identify and rectify issues.</p>
</li>
<li>
<p><strong>Metadata Management</strong>:
Maintain detailed metadata that specifies the valid format, type, and constraints for each data element, guiding data handling and validation processes.</p>
</li>
<li>
<p><strong>User Education and Guidelines</strong>:
Educate users involved in data entry and management about the importance of data validity and provide clear guidelines and training on maintaining it.</p>
</li>
</ul>
<h2 id="validity-metrics-examples"><a class="header" href="#validity-metrics-examples">Validity Metrics Examples</a></h2>
<p>For the validity dimension in data quality, ensuring that data adheres to both structural and contextual rules is crucial. Here are some examples of validity metrics that can be applied in various business contexts:</p>
<h3 id="format-compliance-rate"><a class="header" href="#format-compliance-rate">Format Compliance Rate</a></h3>
<p><strong>Application</strong>: Measure the percentage of data entries that adhere to predefined format rules (e.g., date formats, phone numbers).</p>
<p><strong>Example</strong>: A customer service database might track the format compliance rate for customer phone numbers to ensure they are stored in a uniform and usable format.</p>
<h3 id="data-type-integrity-rate"><a class="header" href="#data-type-integrity-rate">Data Type Integrity Rate</a></h3>
<p><strong>Application</strong>: Quantify the proportion of data that matches the expected data types defined in the data model (e.g., integers, strings).</p>
<p><strong>Example</strong>: A financial system may monitor the data type integrity rate for transaction amounts to ensure they are recorded as numeric values, not strings.</p>
<h3 id="range-and-boundary-adherence-rate"><a class="header" href="#range-and-boundary-adherence-rate">Range and Boundary Adherence Rate</a></h3>
<p><strong>Application</strong>: Evaluate the percentage of data entries that fall within acceptable range limits or boundaries (e.g., age, salary caps).</p>
<p><strong>Example</strong>: An HR system could track the adherence rate of employee salaries to ensure they fall within the defined salary bands for their roles.</p>
<h3 id="referential-integrity-compliance"><a class="header" href="#referential-integrity-compliance">Referential Integrity Compliance</a></h3>
<p><strong>Application</strong>: Assess the extent to which foreign key values in a database table correctly reference existing primary keys in another table, ensuring relational integrity.</p>
<p><strong>Example</strong>: An e-commerce platform might measure referential integrity compliance to ensure that all order records correctly reference existing customer records.</p>
<h3 id="mandatory-fields-completion-rate"><a class="header" href="#mandatory-fields-completion-rate">Mandatory Fields Completion Rate</a></h3>
<p><strong>Application</strong>: Measure the percentage of records that have all mandatory fields filled, ensuring completeness and validity.</p>
<p><strong>Example</strong>: A lead generation form might track the completion rate of mandatory fields to ensure that leads are captured with all necessary information.</p>
<h3 id="logical-consistency-check-rate"><a class="header" href="#logical-consistency-check-rate">Logical Consistency Check Rate</a></h3>
<p><strong>Application</strong>: Quantify the proportion of data that passes logical consistency checks (e.g., a child's birth date being after the parent's birth date).</p>
<p><strong>Example</strong>: A healthcare application may monitor the logical consistency check rate for patient and family records to ensure logical relationships are maintained.</p>
<h3 id="pattern-matching-success-rate"><a class="header" href="#pattern-matching-success-rate">Pattern Matching Success Rate</a></h3>
<p><strong>Application</strong>: Evaluate the success rate at which data entries match predefined patterns (e.g., email address patterns, product codes).</p>
<p><strong>Example</strong>: An online registration system could track the pattern-matching success rate for email addresses to ensure they follow a valid email format.</p>
<p>By implementing these validity metrics, organizations can ensure that their data is not only structurally sound but also contextually appropriate for its intended use. Ensuring data validity is essential for maintaining the integrity of data systems and for supporting accurate, reliable decision-making processes.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="accessibility-dimension-in-data-quality"><a class="header" href="#accessibility-dimension-in-data-quality">Accessibility Dimension in Data Quality</a></h1>
<blockquote>
<p>Accessibility in data quality refers to the ease with which data can be retrieved and used by authorized individuals or systems. It ensures that data is available when needed, through appropriate channels, and in usable formats, while also maintaining necessary security and privacy controls. Accessibility is crucial for efficient decision-making, operational processes, and ensuring that data serves its intended purpose effectively.</p>
</blockquote>
<h2 id="accessibility-metrics"><a class="header" href="#accessibility-metrics">Accessibility Metrics</a></h2>
<p>Evaluating accessibility involves assessing the systems, protocols, and permissions in place that enable or restrict access to data. Here’s how accessibility can be gauged across different stages of the data infrastructure:</p>
<h3 id="data-sources-operational-data---data-access-success-rate"><a class="header" href="#data-sources-operational-data---data-access-success-rate">Data Sources (Operational Data) - Data Access Success Rate</a></h3>
<p>\[ Data \ Access \ Success \ Rate = \frac{Number\ of \ Successful \ Data \ Retrieval \ Attempts}{Total \ Number \ of \ Data \ Retrieval \ Attempts} \times 100 \]</p>
<p><strong>Application</strong>: Monitor and log access attempts to operational databases or systems to identify and address any access issues, ensuring that data can be successfully retrieved when needed.</p>
<h3 id="data-lakes-and-data-warehouses---query-performance-index"><a class="header" href="#data-lakes-and-data-warehouses---query-performance-index">Data Lakes and Data Warehouses - Query Performance Index</a></h3>
<p>\[ Query \ Performance \ Index = Average \ Response \ Time \ for \ Data \ Retrieval \ Queries \]</p>
<p><strong>Application</strong>: Measure the performance of data retrieval queries in data lakes and warehouses to assess how quickly and efficiently data can be accessed, considering factors like indexing and query optimization.</p>
<h3 id="data-marts---user-access-rate"><a class="header" href="#data-marts---user-access-rate">Data Marts - User Access Rate</a></h3>
<p>\[ User \ Access \ Rate = \frac{Number \ of \ Unique \ Users \ Accessing \ the \ Data \ Mart}{Total \ Number \ of \ Authorized \ Users} \times 100 \]</p>
<p><strong>Application</strong>: Track the usage of data marts by authorized users to ensure that they can access the data they need for analysis and reporting.</p>
<h2 id="ensuring-and-improving-accessibility"><a class="header" href="#ensuring-and-improving-accessibility">Ensuring and Improving Accessibility</a></h2>
<p>To maintain and enhance data accessibility across the data infrastructure, consider the following strategies:</p>
<ul>
<li>
<p><strong>Robust Data Architecture</strong>:
Design data systems and architectures that support efficient data retrieval and query performance, incorporating features like indexing, caching, and data partitioning.</p>
</li>
<li>
<p><strong>Access Control Policies</strong>:
Implement comprehensive access control policies that define who can access what data, ensuring that data is accessible to authorized users while maintaining security and privacy.</p>
</li>
<li>
<p><strong>User-Centric Design</strong>:
Ensure that data repositories, reports, and dashboards are designed with the end-user in mind, focusing on usability, intuitive navigation, and user-friendly interfaces.</p>
</li>
<li>
<p><strong>Monitoring and Alerts</strong>:
Set up monitoring systems to track data system performance and accessibility, with alerts for any issues that might impede access, allowing for prompt resolution.</p>
</li>
<li>
<p><strong>Training and Support</strong>:
Provide training and support to users on how to access and use data systems, tools, and platforms effectively, enhancing their ability to retrieve and utilize data.</p>
</li>
</ul>
<h2 id="accessibility-metrics-examples"><a class="header" href="#accessibility-metrics-examples">Accessibility Metrics Examples</a></h2>
<p>Here are some examples of accessibility metrics that can be applied in various business contexts:</p>
<h3 id="average-time-to-retrieve-data"><a class="header" href="#average-time-to-retrieve-data">Average Time to Retrieve Data</a></h3>
<p><strong>Application</strong>: Measures the average time taken to access and retrieve data from databases, data lakes, or data warehouses, indicating system performance and efficiency.</p>
<h3 id="data-system-availability-rate"><a class="header" href="#data-system-availability-rate">Data System Availability Rate</a></h3>
<p><strong>Application</strong>: Quantifies the percentage of time a data system is operational and accessible, reflecting system reliability and uptime.</p>
<h3 id="data-access-error-rate"><a class="header" href="#data-access-error-rate">Data Access Error Rate</a></h3>
<p><strong>Application</strong>: Tracks the frequency of errors encountered during data access attempts, indicating potential issues in data retrieval processes or system stability.</p>
<h3 id="data-access-permission-compliance-rate"><a class="header" href="#data-access-permission-compliance-rate">Data Access Permission Compliance Rate</a></h3>
<p><strong>Application</strong>: Assesses how well data access controls and permissions are enforced, ensuring only authorized users or systems can access sensitive or restricted data.</p>
<h3 id="data-format-compatibility-rate"><a class="header" href="#data-format-compatibility-rate">Data Format Compatibility Rate</a></h3>
<p><strong>Application</strong>: Evaluates the proportion of data requests that are fulfilled with data in formats compatible with users' or systems' requirements, facilitating ease of use.</p>
<p>These metrics can be integrated into data quality monitoring systems and can be tracked over time to ensure that data remains accessible, secure, and usable for all authorized users and applications. Setting thresholds for these metrics can help in triggering alerts or actions when data accessibility is compromised, ensuring prompt resolution of issues.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="integrity-dimension-in-data-quality"><a class="header" href="#integrity-dimension-in-data-quality">Integrity Dimension in Data Quality</a></h1>
<blockquote>
<p>Integrity in data quality refers to the consistency, accuracy, and trustworthiness of data across its lifecycle. It involves maintaining data's completeness, coherence, and credibility, ensuring that it remains unaltered from its source through various transformations and usage. Data integrity is crucial for ensuring that the information used for decision-making, reporting, and analysis is reliable and reflects the true state of affairs.</p>
</blockquote>
<h2 id="integrity-metrics"><a class="header" href="#integrity-metrics">Integrity Metrics</a></h2>
<p>Evaluating data integrity involves assessing the processes, controls, and systems in place to prevent unauthorized data alteration and to ensure data remains consistent and accurate. Here’s how integrity can be assessed across different stages of the data infrastructure:</p>
<h3 id="data-sources-operational-data---source-to-target-consistency-rate"><a class="header" href="#data-sources-operational-data---source-to-target-consistency-rate">Data Sources (Operational Data) - Source-to-Target Consistency Rate</a></h3>
<p>\[ Source-to-Target \ Consistency \ Rate = \frac{Number\ of \ Consistent \ Records \ Between \ Source \ and \ Target}{Total \ Number \ of \ Records \ Reviewed} \times 100 \]</p>
<p><strong>Application</strong>: Compare data records in the operational systems (source) with those in the data warehouse or lake (target) to ensure data has been transferred accurately and remains unaltered.</p>
<h3 id="data-lakes-and-data-warehouses---referential-integrity-score"><a class="header" href="#data-lakes-and-data-warehouses---referential-integrity-score">Data Lakes and Data Warehouses - Referential Integrity Score</a></h3>
<p>\[ Referential \ Integrity \ Score = \frac{Number \ of \ Records \ with \ Valid \ References}{Total \ Number \ of \ Records} \times 100 \]</p>
<p><strong>Application</strong>: Validate referential integrity within the data lake or warehouse, ensuring that all foreign key relationships are consistent and that related records are present.</p>
<h3 id="data-marts---dimensional-integrity-index"><a class="header" href="#data-marts---dimensional-integrity-index">Data Marts - Dimensional Integrity Index</a></h3>
<p>\[ Dimensional \ Integrity \ Index = \frac{Number \ of \ Dimension \ Records \ with \ Consistent \ Attributes}{Total \ Number \ of \ Dimension \ Records} \times 100 \]</p>
<p><strong>Application</strong>: Check the integrity of dimension tables in data marts, ensuring that attributes like time dimensions, geographical hierarchies, or product categories remain consistent and accurate.</p>
<h3 id="reports-and-dashboards---data-traceability-index"><a class="header" href="#reports-and-dashboards---data-traceability-index">Reports and Dashboards - Data Traceability Index</a></h3>
<p><strong>Application</strong>: Ensure that data presented in reports and dashboards can be traced back to its original source or the transformation logic applied, maintaining a clear lineage for auditability and verification. It is a qualitative assessment based on the ability to trace data back to its source.</p>
<h2 id="ensuring-and-improving-integrity"><a class="header" href="#ensuring-and-improving-integrity">Ensuring and Improving Integrity</a></h2>
<p>To maintain and enhance data integrity across the data infrastructure, consider implementing the following strategies:</p>
<ul>
<li>
<p><strong>Data Validation Rules</strong>:
Establish validation rules that check data for integrity at every stage of its movement and transformation within the system.</p>
</li>
<li>
<p><strong>Audit Trails and Data Lineage</strong>:
Maintain comprehensive audit trails and clear data lineage documentation, enabling the tracking of data from its source through all transformations to its final form.</p>
</li>
<li>
<p><strong>Access Controls and Security Measures</strong>:
Implement robust access controls and security measures to prevent unauthorized data access or alteration, protecting data integrity.</p>
</li>
<li>
<p><strong>Regular Data Audits</strong>:
Conduct periodic audits of data and data management processes to identify and rectify any integrity issues, ensuring ongoing compliance with data integrity standards.</p>
</li>
<li>
<p><strong>Error Handling and Correction Procedures</strong>:
Develop standardized procedures for handling data errors and anomalies detected during processing, ensuring that integrity issues are promptly and effectively addressed.</p>
</li>
</ul>
<h2 id="integrity-metrics-examples"><a class="header" href="#integrity-metrics-examples">Integrity Metrics Examples</a></h2>
<p>Here are some examples of integrity metrics that can be applied in various business contexts:</p>
<h3 id="data-lineage-traceability-score"><a class="header" href="#data-lineage-traceability-score">Data Lineage Traceability Score</a></h3>
<p><strong>Application</strong>: Measure the percentage of data elements within a dataset for which complete lineage (origin, transformations, and current state) can be accurately traced, ensuring transparency and accountability in data handling.</p>
<h3 id="cross-system-data-consistency-rate"><a class="header" href="#cross-system-data-consistency-rate">Cross-System Data Consistency Rate</a></h3>
<p><strong>Application</strong>: Evaluate the level of consistency for the same data elements stored across different systems or databases, ensuring data remains unaltered and reliable across platforms.</p>
<h3 id="data-transformation-integrity-score"><a class="header" href="#data-transformation-integrity-score">Data Transformation Integrity Score</a></h3>
<p><strong>Application</strong>: Assess the accuracy and correctness of data transformations applied during ETL processes, maintaining the integrity of data as it is processed and stored.</p>
<h3 id="referential-integrity-compliance-rate"><a class="header" href="#referential-integrity-compliance-rate">Referential Integrity Compliance Rate</a></h3>
<p>Formula:
<strong>Application</strong>: Measure the degree to which databases maintain referential integrity by ensuring that all foreign key values have a corresponding primary key value in the related table, preserving data relationships and coherence.</p>
<h3 id="audit-trail-coverage-rate"><a class="header" href="#audit-trail-coverage-rate">Audit Trail Coverage Rate</a></h3>
<p><strong>Application</strong>: Quantify the proportion of data transactions or modifications that have a complete, unbroken audit trail, allowing for full accountability and traceability of data changes.</p>
<p>By monitoring these metrics, organizations can ensure that their data maintains high integrity throughout its lifecycle, from creation and storage to transformation and usage. This is crucial for relying on data for critical business decisions, regulatory compliance, and maintaining trust with stakeholders. Setting up alerts for deviations in these metrics can help in quickly identifying and addressing issues that may compromise data integrity.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-quality-metricsaudit-database--service-1"><a class="header" href="#data-quality-metricsaudit-database--service-1">Data Quality Metrics/Audit Database &amp; Service</a></h1>
<p>Maintaining Metrics/Audit databases and services is important for several reasons, particularly in complex data environments where ensuring data integrity, compliance, and operational efficiency is required:</p>
<h3 id="data-integrity-and-quality-assurance"><a class="header" href="#data-integrity-and-quality-assurance">Data Integrity and Quality Assurance</a></h3>
<p>Metrics and audit databases provide a systematic way to track and measure data quality, performance, and integrity over time. By maintaining these databases, organizations can identify trends, pinpoint anomalies, and take corrective actions to uphold data standards, ensuring that stakeholders can trust and rely on the data for decision-making.</p>
<h3 id="compliance-and-regulatory-requirements"><a class="header" href="#compliance-and-regulatory-requirements">Compliance and Regulatory Requirements</a></h3>
<p>Many industries are subject to strict regulatory requirements regarding data management, privacy, and security. Audit databases help in logging access, changes, and operations performed on data, which is essential for demonstrating compliance with regulations such as GDPR, HIPAA, SOX, and others. They provide an immutable record that can be reviewed during audits or inspections.</p>
<h3 id="operational-efficiency-and-optimization"><a class="header" href="#operational-efficiency-and-optimization">Operational Efficiency and Optimization</a></h3>
<p>By analyzing metrics related to system performance, query times, resource utilization, and more, organizations can identify bottlenecks and inefficiencies within their data pipelines and infrastructure. This insight allows for targeted optimization efforts, improving overall operational efficiency and reducing costs.</p>
<h3 id="security-and-anomaly-detection"><a class="header" href="#security-and-anomaly-detection">Security and Anomaly Detection</a></h3>
<p>Metrics and audit logs play a critical role in security by providing detailed records of data access and system interactions. Analyzing these records helps in detecting unauthorized access, data breaches, and other security threats, enabling timely response and mitigation.</p>
<h3 id="change-management-and-troubleshooting"><a class="header" href="#change-management-and-troubleshooting">Change Management and Troubleshooting</a></h3>
<p>In dynamic environments where changes are frequent, maintaining a detailed record of system states, data modifications, and operational metrics is invaluable for troubleshooting issues. Audit trails and metrics allow teams to understand the impact of changes, diagnose problems, and restore system functionality more quickly.</p>
<h3 id="knowledge-sharing-and-collaboration"><a class="header" href="#knowledge-sharing-and-collaboration">Knowledge Sharing and Collaboration</a></h3>
<p>Metrics/Audit databases serve as a knowledge base, documenting the operational history and performance characteristics of data systems. This information can be shared across teams, improving collaboration, and enabling more informed decision-making.</p>
<h3 id="service-level-agreements-slas-monitoring"><a class="header" href="#service-level-agreements-slas-monitoring">Service Level Agreements (SLAs) Monitoring</a></h3>
<p>For organizations that rely on data services (either internal or external), metrics databases are essential for monitoring adherence to SLAs. They help in tracking availability, performance, and response times, ensuring that service providers meet their contractual obligations.</p>
<h2 id="data-quality-metricsaudit-database"><a class="header" href="#data-quality-metricsaudit-database">Data Quality Metrics/Audit Database</a></h2>
<p>Below is a conceptual example of how metrics records might be structured within a metrics database:</p>
<pre><code class="language-sql">-- Table structure for 'data_quality_metric_records'
CREATE TABLE data_quality_metric_records (
    id SERIAL PRIMARY KEY,
    metric_type VARCHAR(255) NOT NULL,
    metric_name VARCHAR(255) NOT NULL,
    metric_formula TEXT NOT NULL,
    metric_value NUMERIC(5,2) NOT NULL,
    source_system VARCHAR(255) NOT NULL,
    target_system VARCHAR(255) NOT NULL,
    data_domain VARCHAR(255) NOT NULL,
    measurement_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    notes TEXT
);

-- Sample entries for metrics
INSERT INTO data_quality_metric_records (metric_type, metric_name, metric_formula, metric_value, source_system, target_system, data_domain, notes)
VALUES
('Completeness', 'Record Completeness', '(Number of Complete Records / Total Number of Records) * 100', 97.50, 'Postgres', 'S3', 'Sales', 'Monthly sales data completeness.'),
('Completeness', 'Field Completeness', '(Number of Fields without NULLs / Total Number of Fields) * 100', 99.30, 'Oracle', 'Redshift', 'Customer', 'Customer data fields completeness.'),
('Completeness', 'Data Mart Completeness', '(Number of Complete Data Mart Records / Total Expected Records) * 100', 98.75, 'MariaDB', 'Data Mart', 'Inventory', 'Inventory data mart completeness after dbt transformation.'),
('Completeness', 'ELT Completeness', '(Number of Records Loaded by DMS / Number of Records in Source) * 100', 99.80, 'All Sources', 'Data Lake (S3)', 'All Domains', 'Completeness of the ELT process monitored by DMS tasks.');

-- Query to retrieve the latest metrics for the 'Sales' data domain
SELECT * FROM data_quality_metric_records
WHERE data_domain = 'Sales'
ORDER BY measurement_time DESC
LIMIT 1;
</code></pre>
<p>In this example:</p>
<ul>
<li><code>id</code> is a unique identifier for each metric record.</li>
<li><code>metric_type</code> describes the metric dimension (Accuracy, Completeness, etc.) being measured.</li>
<li><code>metric_name</code> describes the type of metric being measured.</li>
<li><code>metric_formula</code> provides the formula used to calculate the metric.</li>
<li><code>metric_value</code> stores the actual metric value, in this case, a percentage.</li>
<li><code>source_system</code> and <code>target_system</code> indicate where the data is coming from and where it is being loaded to.</li>
<li><code>data_domain</code> specifies the domain or category of the data being measured (e.g., sales, customer, inventory).</li>
<li><code>measurement_time</code> records the timestamp when the measurement was taken.</li>
<li><code>notes</code> is an optional field for any additional information or context about the metric.</li>
</ul>
<h2 id="data-quality-service"><a class="header" href="#data-quality-service">Data Quality Service</a></h2>
<p>In a practical data environment, it's crucial to organize data quality metrics and measurement tasks into separate, well-defined tables to maintain clarity and facilitate easy data management. Here's what the structure might look like:</p>
<h3 id="data_quality_metrics-tables"><a class="header" href="#data_quality_metrics-tables"><code>data_quality_metrics</code> Tables</a></h3>
<p>This table would act as a reference for all defined metrics, capturing their names, formulas, and other relevant details. As a <a href="concepts/data-quality/../data-architecture/slowly_changing_dimensions.html"><strong>Type 4 Slowly Changing Dimension (SCD) table</strong></a>, it would maintain in one table (<code>data_quality_metrics_history</code>), a complete history of each metric (Type 2 SCD), including when they were created or if they were ever retired (<code>deleted_at</code>), and in the main table (<code>data_quality_metrics</code>), only the current metrics (Type 1 SCD).</p>
<h3 id="data_quality_measurement_tasks-tables"><a class="header" href="#data_quality_measurement_tasks-tables"><code>data_quality_measurement_tasks</code> Tables</a></h3>
<p>This table would contain information about the measurement tasks themselves, including the system used for measurement and the specific source and target systems involved. Like the metrics table, this would also be a type 4 SCD, preserving a historical record of measurement tasks' lifecycles (<code>data_quality_measurement_tasks_history</code>), and the current tasks (<code>data_quality_measurement_tasks</code>).</p>
<h3 id="data_quality_metric_records-table"><a class="header" href="#data_quality_metric_records-table"><code>data_quality_metric_records</code> Table</a></h3>
<p>Serving as the transaction table, <code>data_quality_metric_records</code> would hold the actual records of measurements. Each record would reference the relevant metric (<code>data_quality_metrics.id</code>) and measurement task (<code>data_quality_measurement_tasks</code>), along with the unique identifier for the run (<code>run_id</code>), and a URL pointing to the relevant logs for that run (<code>run_url</code>).</p>
<h3 id="data-quality-service-1"><a class="header" href="#data-quality-service-1">Data Quality Service</a></h3>
<p>The setup would be supported by a dedicated service, tentatively named <code>data-quality-service</code>, which would facilitate the recording of measurement data, potentially through an API. The management of <code>data_quality_metrics</code> and <code>data_quality_measurement_tasks</code> through their APIs, while not detailed in this example, would be a critical part of the overall data quality infrastructure.</p>
<p>By segregating metric definitions, measurement tasks, and actual measurement records into distinct tables and managing them through a dedicated service, organizations can ensure that data quality tracking is both efficient and scalable. This approach allows for the precise pinpointing of data quality issues and facilitates a structured way to track improvements and changes over time.</p>
<h2 id="taking-action"><a class="header" href="#taking-action">Taking Action</a></h2>
<p>In a practical setup, it's crucial to not only collect data quality metrics but also to analyze, monitor, and act upon them effectively. Integrating observability tools, automating ticketing systems, utilizing data visualization platforms, leveraging communication systems, and disseminating reports are key to maintaining high data quality standards:</p>
<h3 id="observability-tools-eg-datadog"><a class="header" href="#observability-tools-eg-datadog">Observability Tools (e.g., DataDog)</a></h3>
<p>Configure DataDog to monitor <code>data_quality_metric_records</code> for significant deviations or trends in data quality metrics.</p>
<p>Set up alerts in DataDog for when metrics fall below predefined thresholds, indicating potential data quality issues.</p>
<h3 id="ticket-automation-eg-jira"><a class="header" href="#ticket-automation-eg-jira">Ticket Automation (e.g., Jira)</a></h3>
<p>Automate the creation of Jira tickets through API integration when DataDog alerts trigger, ensuring immediate action on data quality issues.</p>
<p>Include relevant details in the ticket, such as <code>metric_name</code>, <code>metric_value</code>, <code>run_url</code>, and a brief description of the potential issue for quicker resolution.</p>
<h3 id="data-visualization-dashboards-eg-tableau-powerbi"><a class="header" href="#data-visualization-dashboards-eg-tableau-powerbi">Data Visualization Dashboards (e.g., Tableau, PowerBI)</a></h3>
<p>Develop dashboards in Tableau or PowerBI that visualize key data quality metrics over time, providing a clear view of data quality trends and anomalies.</p>
<p>Enable dashboard filters by <code>source_system</code>, <code>target_system</code>, and <code>data_domain</code> for targeted analysis by different teams.</p>
<h3 id="communication-systems-eg-slack-teams"><a class="header" href="#communication-systems-eg-slack-teams">Communication Systems (e.g., Slack, Teams)</a></h3>
<p>Set up integrations with Slack or Teams to send automated notifications about critical data quality alerts, ensuring broad awareness among relevant stakeholders.</p>
<p>Create dedicated channels for data quality discussions, facilitating collaborative problem-solving and updates on issue resolution.</p>
<h3 id="reports-eg-sharepoint"><a class="header" href="#reports-eg-sharepoint">Reports (e.g., SharePoint)</a></h3>
<p>Regularly generate comprehensive data quality reports that summarize the state of data quality across different domains and systems, making them accessible on SharePoint for wider organizational visibility.</p>
<p>Include insights, trend analyses, and recommendations for improvements in the reports to guide strategic data quality initiatives.</p>
<p>By employing this multifaceted approach, organizations can ensure that data quality metrics are not only tracked but also analyzed and acted upon promptly. This proactive stance on data quality management enables quicker identification and resolution of issues, maintains trust in data systems, and supports informed decision-making across the organization.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="final-thoughts-on-data-quality-dimensions"><a class="header" href="#final-thoughts-on-data-quality-dimensions">Final Thoughts on Data Quality Dimensions</a></h1>
<p>In this chapter, we explored several critical dimensions of data quality, including <a href="concepts/data-quality/./accuracy_dimension.html">Accuracy</a>, <a href="concepts/data-quality/./completeness_dimension.html">Completeness</a>, <a href="concepts/data-quality/./consistency_dimension.html">Consistency</a>, <a href="concepts/data-quality/./relevance_dimension.html">Relevance</a>, <a href="concepts/data-quality/./reliability_dimension.html">Reliability</a>, <a href="concepts/data-quality/./uniqueness_dimension.html">Uniqueness</a>, <a href="concepts/data-quality/./validity_dimension.html">Validity</a>, <a href="concepts/data-quality/./accessibility_dimension.html">Accessibility</a>, and <a href="concepts/data-quality/./integrity_dimension.html">Integrity</a>. Each of these dimensions plays a vital role in ensuring that data serves its intended purpose effectively, supporting decision-making, operational efficiency, and strategic initiatives.</p>
<p>However, it's important to recognize that not every use case will require an exhaustive focus on all these dimensions. The relevance and priority of each dimension can vary significantly depending on factors such as industry norms, organizational size, team composition, and the maturity of the data infrastructure in place. For instance:</p>
<ul>
<li>
<p>A financial institution might prioritize Accuracy and Integrity due to the regulatory and fiduciary responsibilities inherent in the industry.</p>
</li>
<li>
<p>A retail business may focus more on Completeness and Relevance to ensure customer data supports effective marketing and sales strategies.</p>
</li>
<li>
<p>A startup with a lean data team might concentrate on Accessibility and Validity to quickly derive value from limited data resources.</p>
</li>
</ul>
<p>Moreover, the metrics presented for measuring each dimension, while broadly applicable, may not be entirely relevant or sufficient for every context. Organizations may find that industry-specific metrics, company-size considerations, team capabilities, or the particularities of their data infrastructure necessitate the development of custom metrics tailored to their unique use cases.</p>
<p>For example:</p>
<ul>
<li>
<p>A large enterprise with a complex data ecosystem might develop sophisticated metrics to measure data lineage and impact analysis, ensuring Integrity and Consistency across multiple systems.</p>
</li>
<li>
<p>A small team within a mid-sized company might adopt more straightforward, manually checked metrics focused on the immediate usability of data, emphasizing Validity and Relevance.</p>
</li>
</ul>
<p>Additionally, as data environments evolve and new technologies emerge, new dimensions of data quality may become relevant, and existing dimensions may need to be reinterpreted or expanded. Continuous learning, adaptation, and innovation in data quality practices are essential for organizations to keep pace with these changes.</p>
<p>In conclusion, while the dimensions of data quality outlined in this chapter provide a comprehensive framework for understanding and improving data quality, their application must be adapted to fit the specific needs and constraints of each organization. By carefully selecting which dimensions to focus on and customizing metrics to their unique contexts, data teams can effectively enhance the quality of their data, driving more accurate insights, efficient operations, and strategic growth.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-quality--data-reliability"><a class="header" href="#data-quality--data-reliability">Data Quality &amp; Data Reliability</a></h1>
<p>As we conclude our exploration of data quality dimensions and their critical role within the broader context of data reliability engineering, it's essential to recognize that data quality is not just a set of standards to be met. Instead, it's a basic building block that supports the reliability, trustworthiness, and overall value of data in driving business decisions, insights, and strategies.</p>
<h3 id="the-role-of-data-quality-in-data-reliability"><a class="header" href="#the-role-of-data-quality-in-data-reliability">The Role of Data Quality in Data Reliability</a></h3>
<p>Data reliability depends on the consistent delivery of accurate, complete, and timely data. The dimensions of data quality, such as accuracy, completeness, consistency, timeliness, and others discussed in this chapter, serve as pillars that uphold the reliability of data. Ensuring high standards across these dimensions means that data can be trusted as a reliable asset for operational and analytical purposes.</p>
<h3 id="data-anomalies-and-their-impact-on-reliability"><a class="header" href="#data-anomalies-and-their-impact-on-reliability">Data Anomalies and Their Impact on Reliability</a></h3>
<p>Data anomalies, which may arise from inconsistencies, inaccuracies, or incomplete data, can significantly undermine data reliability. They can lead to faulty analyses, misguided business decisions, and diminished trust in data systems. Proactive measures to detect and rectify anomalies are crucial in maintaining the integrity and reliability of data.</p>
<h3 id="data-quality-in-data-integration-and-migration"><a class="header" href="#data-quality-in-data-integration-and-migration">Data Quality in Data Integration and Migration</a></h3>
<p>The integration and migration of data present critical moments where data quality must be rigorously managed to preserve data reliability. Ensuring that data remains valid, unique, and consistent across systems is super important, especially when consolidating data from disparate sources into a unified data lake, data warehouse, or data mart.</p>
<h3 id="the-influence-of-data-architecture-on-data-quality"><a class="header" href="#the-influence-of-data-architecture-on-data-quality">The Influence of Data Architecture on Data Quality</a></h3>
<p>The underlying data architecture plays a huge role in facilitating data quality. A well-designed architecture that supports robust data management practices, including effective data governance and metadata management, sets the foundation for high-quality, reliable data.</p>
<h3 id="role-of-metadata-in-data-quality-and-reliability"><a class="header" href="#role-of-metadata-in-data-quality-and-reliability">Role of Metadata in Data Quality and Reliability</a></h3>
<p>Metadata provides essential context that enhances the quality and reliability of data by offering insights into its origin, structure, and usage. Effective metadata management ensures that data is accurately described, classified, and easily discoverable, contributing to its overall quality and reliability.</p>
<h3 id="addressing-data-quality-at-the-source"><a class="header" href="#addressing-data-quality-at-the-source">Addressing Data Quality at the Source</a></h3>
<p>Proactive strategies that address data quality issues at the source are among the most effective. Implementing strict data entry checks, validation rules, and early anomaly detection can significantly reduce the downstream impact of data quality issues, enhancing data reliability.</p>
<h3 id="data-reliability-engineering--data-quality"><a class="header" href="#data-reliability-engineering--data-quality">Data Reliability Engineering &amp; Data Quality</a></h3>
<p>In this chapter, we mostly explored how data quality impacts data reliability engineering, but the opposite is also true, the stability and dependability of technical systems and processes are critical for maintaining high data quality. If these technical aspects are not reliable, they can introduce errors and delays, directly affecting the accuracy, completeness, and timeliness of the data. This makes ensuring the smooth operation of data infrastructure essential for preserving the quality of data, highlighting the interconnectedness between technical reliability and data quality in supporting effective data management and utilization.</p>
<h3 id="final-thoughts-1"><a class="header" href="#final-thoughts-1">Final Thoughts</a></h3>
<p>In the diverse landscape of industries, company sizes, and data infrastructures, the relevance and applicability of specific data quality dimensions and metrics can vary widely. Each organization must tailor its approach to data quality, considering its unique context, requirements, and challenges. Not all dimensions may be equally relevant, and additional, industry-specific metrics may be necessary to fully capture the nuances of data quality within a particular domain.</p>
<p>Embracing a holistic view of data quality, one that integrates seamlessly with the principles of data reliability engineering enables organizations to not only address data quality reactively but to embed quality and reliability into the very fabric of their data management practices. This proactive stance on data quality ensures that data remains a true, reliable asset that can support the organization's goals, drive innovation, and deliver lasting value in an increasingly data-driven world.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="confiabilidad-de-datos"><a class="header" href="#confiabilidad-de-datos">Confiabilidad de Datos</a></h1>
<blockquote>
<p>Tomándose por base los fundamentos y metodologías desarrolladas por los <strong>Site Reliability Engineers</strong>, conocidos como los “firefighters” del mundo de la ingeniería de sistemas, los cuales construyen sistemas automatizados para optimizar la disponibilidad de las aplicaciones (reducir downtime), definiremos <strong>Data Reliability</strong> cómo la capacidad del equipo de data en entregar alta disponibilidad de la data durante todo el ciclo de vida de la misma. En resumen, garantizar los periodos de tiempo que la data no presenta inacurácia, no es faltante ni errónea.</p>
</blockquote>
<p>...</p>
<h2 id="consecuencias-de-la-confiabilidad"><a class="header" href="#consecuencias-de-la-confiabilidad">Consecuencias de la confiabilidad</a></h2>
<p>Consecuencias (en administrar el downtime de la data):</p>
<ul>
<li>Los equipos de data reducen de manera muy importante el tiempo perdido en “apagar incendios”, escalaciones y troubleshooting de la data. Utilizan ese tiempo para enfocarse en la construcción de una buena infraestructura, y en agregar valor a la data.</li>
<li>Los equipos de data son más rápidos en actualizar y modificar la infraestructura de la data, ya que tienen claro la confiabilidad del sistema.</li>
<li>Los equipos de data ganan el respeto y la confianza de los stakeholders, ya que entregan datos confiables de manera consistente.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="chaos-engineering-tools"><a class="header" href="#chaos-engineering-tools">Chaos Engineering Tools</a></h1>
<p>Chaos engineering tools, such as Gremlin or Chaos Mesh, introduce controlled disruptions into data systems (like network latency, server failures, or resource exhaustion) to test and improve their resilience. By proactively identifying and addressing potential points of failure, data systems become more robust and reliable.</p>
<ul>
<li>Chaos Mesh</li>
<li>Chaos Monkey</li>
<li>Gremlin</li>
<li>Harness Chaos Engineering Powered by Litmus</li>
<li>LitmusChaos</li>
<li>Harness.io</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="control-systems-high-availability"><a class="header" href="#control-systems-high-availability">Control Systems High Availability</a></h1>
<blockquote>
<p>Control Systems High Availability refers to the design and implementation of control systems in a way that ensures they are <strong>consistently available and operational</strong>, minimizing downtime and maintaining continuous service. <em>High availability in control systems is achieved through redundancy, fault tolerance, failover strategies, and robust system monitoring</em>.</p>
</blockquote>
<p>Adapting the principles of High Availability from control systems to data reliability engineering involves ensuring that data systems and services are designed to be resilient, with minimal disruptions, and can recover quickly from failures. This can be achieved through several strategies:</p>
<ul>
<li><strong>Redundancy</strong>: Implementing redundant data storage and processing systems so that if one system fails, another can take over without loss of service.</li>
<li><strong>Fault Tolerance</strong>: Designing data systems to continue operating even when components fail. This might involve using distributed systems that can handle the failure of individual nodes without affecting the overall system performance.</li>
<li><strong>Failover Mechanisms</strong>: Establishing automated processes that detect system failures and seamlessly switch operations to backup systems to maintain service continuity.</li>
<li><strong>Load Balancing</strong>: Distributing data processing and queries across multiple servers to prevent any single point of failure and to manage load efficiently, ensuring consistent performance.</li>
<li><strong>Regular Data Backups</strong>: Maintaining frequent and reliable data backups to enable quick data restoration in the event of data loss or corruption.</li>
<li><strong>Monitoring and Alerts</strong>: Implementing comprehensive monitoring of data systems to detect issues proactively, with alerting mechanisms that notify relevant personnel to take immediate action.</li>
<li><strong>Disaster Recovery Planning</strong>: Developing and regularly testing disaster recovery plans that outline clear steps for restoring data services in the event of significant system failures or catastrophic events.</li>
</ul>
<p>By incorporating these high availability strategies into data systems design and management, data reliability engineers can ensure that data services are robust, resilient, and capable of maintaining high levels of service availability, even in the face of system failures or unexpected disruptions.</p>
<p>Here's an example of how principles of Control Systems High Availability can be adapted to data reliability engineering:</p>
<h3 id="scenario"><a class="header" href="#scenario">Scenario</a></h3>
<p>A company relies heavily on its customer data platform (CDP) to deliver personalized marketing campaigns. The CDP integrates data from various sources, including e-commerce transactions, customer service interactions, and social media engagement. High availability of this platform is crucial to ensure continuous marketing operations and customer engagement.</p>
<h3 id="implementation-of-high-availability-strategies"><a class="header" href="#implementation-of-high-availability-strategies">Implementation of High Availability Strategies</a></h3>
<h4 id="redundancy-1"><a class="header" href="#redundancy-1">Redundancy</a></h4>
<p>The CDP is hosted on a cloud platform that automatically replicates data across multiple geographic regions. In case of a regional outage, the system can quickly failover to another region without losing access to critical customer data.</p>
<h4 id="fault-tolerance-1"><a class="header" href="#fault-tolerance-1">Fault Tolerance</a></h4>
<p>The CDP is built on a microservices architecture, where each service operates independently. If one service fails (e.g., the recommendation engine), other services (like customer segmentation) continue to function, ensuring the platform remains partially operational while the issue is addressed.</p>
<h4 id="failover-mechanisms"><a class="header" href="#failover-mechanisms">Failover Mechanisms</a></h4>
<p>The system is equipped with a failover mechanism that automatically detects service disruptions. For example, if the primary database becomes unavailable, the system seamlessly switches to a standby database, minimizing downtime.</p>
<h4 id="load-balancing"><a class="header" href="#load-balancing">Load Balancing</a></h4>
<p>Incoming data processing requests are distributed among multiple servers using a load balancer. This not only prevents any single server from being overwhelmed but also ensures that if one server goes down, the others can handle the extra load.</p>
<h4 id="regular-data-backups"><a class="header" href="#regular-data-backups">Regular Data Backups</a></h4>
<p>The system performs nightly backups of the entire CDP, including all customer data and interaction histories. These backups are stored in a secure, offsite location and can be used to restore the system in case of significant data loss.</p>
<h4 id="monitoring-and-alerts"><a class="header" href="#monitoring-and-alerts">Monitoring and Alerts</a></h4>
<p>A monitoring system tracks the health and performance of the CDP in real-time. If anomalies or performance issues are detected (e.g., a sudden drop in data ingestion rates), alerts are sent to the data reliability engineering team for immediate investigation.</p>
<h4 id="disaster-recovery-planning"><a class="header" href="#disaster-recovery-planning">Disaster Recovery Planning</a></h4>
<p>The company has a documented disaster recovery plan specifically for the CDP. This plan includes detailed procedures for restoring services in various failure scenarios, and it's regularly tested through drills to ensure the team is prepared to respond effectively to actual incidents.</p>
<p>By integrating these high availability strategies, the company ensures its customer data platform remains reliable and accessible, supporting uninterrupted marketing activities and customer interactions, even in the face of system failures or external disruptions.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="antifragility"><a class="header" href="#antifragility">Antifragility</a></h1>
<blockquote>
<p>Inspired by Nassim Nicholas Taleb's book <em>Antifragile: Things That Gain from Disorder</em>, antifragility differs from resilience or robustness concepts, where systems seek to maintain their reliability level. Instead, from their design, systems increase their reliability concerning the system's inputs.</p>
</blockquote>
<p>Antifragility proposes a system design change, which are commonly designed to be fragile, meaning they will fail if operated outside their requirements. Antifragility suggests the opposite, designing systems that improve when exposed to loads outside of the requirements. In this sense, systems are not only designed to respond to the expected or anticipated but interact with their environment in real-time and adapt to it.</p>
<p>Examples of antifragile systems:</p>
<ul>
<li>Self-healing</li>
<li>Real time sensoring, monitoring</li>
<li>Live FRACAS</li>
<li>System Health Management</li>
<li>Automatic Repair</li>
</ul>
<p>Methods such as <strong>Real-Time Anomaly Detection and Adaptation</strong> and <strong>Adaptive Load Balancing</strong> might interest data teams, but they are not covered in this book. Adaptive Load Balancing, in particular, might be a interesting topic for Data Platform or Data DevOps teams.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bulkhead-pattern"><a class="header" href="#bulkhead-pattern">Bulkhead Pattern</a></h1>
<blockquote>
<p>In the nautical world, we find bulkheads, wooden plates found in ships, designed to prevent the ship from sinking when a portion of the hull is compromised. The Bulkhead Pattern adapts exactly this idea, that a failure in one portion of the system should not compromise the entire system.</p>
</blockquote>
<p>This design pattern is commonly applied in software development, consisting of not overloading a service with more calls than it can handle at a given time, an example of this is Netflix's Hystrix system.</p>
<p>In the context of data engineering, the Bulkhead Pattern involves segmenting data processing tasks, resources, and services into isolated units so that a failure in one area does not cascade and disrupt the entire system. Here's how it could be used:</p>
<h3 id="segmenting-data-pipelines"><a class="header" href="#segmenting-data-pipelines">Segmenting Data Pipelines</a></h3>
<p>Data pipelines can be divided into independent segments or modules, each handling a specific part of the data processing workflow. If one segment encounters an issue, such as an unexpected data format or a processing error, it can be addressed or bypassed without halting the entire pipeline. This approach ensures that other data processing activities continue unaffected, maintaining overall system availability and reliability.</p>
<h3 id="isolating-services-and-resources"><a class="header" href="#isolating-services-and-resources">Isolating Services and Resources</a></h3>
<p>In a microservices architecture, each data service (e.g., data ingestion, transformation, and storage services) can be isolated, ensuring that issues in one service don't impact others. Similarly, resources like databases and compute instances can be dedicated to specific tasks or services. If one service or resource fails or becomes overloaded, it won't drag down the others, helping maintain the stability of the broader data platform.</p>
<h3 id="rate-limiting-and-throttling"><a class="header" href="#rate-limiting-and-throttling">Rate Limiting and Throttling</a></h3>
<p>Applying rate limiting to APIs and data ingestion endpoints can prevent any single user or service from consuming too many resources, which could lead to system-wide failures. By throttling the number of requests or the amount of data processed within a given timeframe, the system can remain stable even under high load, protecting against cascading failures.</p>
<h3 id="implementing-circuit-breakers"><a class="header" href="#implementing-circuit-breakers">Implementing Circuit Breakers</a></h3>
<p>Circuit breakers can temporarily halt the flow of data or requests to a service or component when a failure is detected, similar to how a bulkhead would seal off a damaged section of a ship. Once the issue is resolved, or after a certain timeout, the circuit breaker can reset, allowing the normal operation to resume. This prevents repeated failures and gives the system time to recover.</p>
<h3 id="use-of-containers-and-virtualization"><a class="header" href="#use-of-containers-and-virtualization">Use of Containers and Virtualization</a></h3>
<p>Deploying data services and applications in containers or virtualized environments can provide natural isolation, acting as bulkheads. If one containerized component fails, it can be restarted or replaced without affecting others, ensuring that the overall system remains operational.</p>
<p>By employing the Bulkhead Pattern in data engineering, organizations can build more resilient data systems that are capable of withstanding localized issues without widespread impact, ensuring continuous data processing and availability.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cold-standby"><a class="header" href="#cold-standby">Cold Standby</a></h1>
<p>Cold Standby is a redundancy technique used in data reliability engineering and system design to ensure high availability and continuity of service in the event of system failure. Unlike hot standby or warm standby, where backup systems or components are kept running or at a near-ready state, in cold standby, the backup systems are kept fully offline and are <em>only activated when the primary system fails or during maintenance periods</em>. Here’s a deeper look into cold standby:</p>
<ul>
<li><strong>Fully Offline</strong>: The standby system is not running during normal operations; it's fully powered down or in a dormant state.</li>
<li><strong>Manual Activation</strong>: Switching to the cold standby system often requires manual intervention to bring the system online, configure it, and start the services.</li>
<li><strong>Data Synchronization</strong>: Data is not continuously synchronized between the primary and cold standby systems. Instead, data is periodically backed up and would need to be restored on the cold standby system upon activation.</li>
<li><strong>Cost-Effective</strong>: Because the standby system is not running, it doesn't incur costs for power or compute resources during normal operations, making it a cost-effective solution for non-critical applications or where downtime can be tolerated for longer periods.</li>
</ul>
<p>Cold standby systems are typically used in scenarios where high availability is not critically required, or the cost of maintaining a hot or warm standby system cannot be justified. Examples include non-critical batch processing systems, archival systems, or in environments where budget constraints do not allow for more sophisticated redundancy setups.</p>
<p>Implementation considerations:</p>
<ul>
<li><strong>Recovery Time</strong>: The time to recover services using a cold standby can be significant since the system needs to be powered up, configured, and data may need to be restored from backups. This recovery time should be considered in the system's SLA (Service Level Agreement).</li>
<li><strong>Regular Testing</strong>: Regular drills or tests should be conducted to ensure that the cold standby system can be brought online effectively and within the expected time frame.</li>
<li><strong>Data Loss Risk</strong>: Given that data synchronization is not continuous, there is a risk of data loss for transactions or data changes that occurred after the last backup. This risk needs to be assessed and mitigated through frequent backups or other means.</li>
<li><strong>Manual Processes</strong>: The need for manual intervention to activate cold standby systems requires well-documented procedures and trained personnel to ensure a smooth transition during a failure event.</li>
</ul>
<p>Cold Standby is a fundamental concept in designing resilient and reliable systems, especially when balancing the need for availability with cost constraints. It provides a basic level of redundancy that can be suitable for certain applications and scenarios in data reliability engineering.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="single-point-of-failure-spof"><a class="header" href="#single-point-of-failure-spof">Single Point of Failure (SPOF)</a></h1>
<p>Eliminating Single Point of Failure (SPOF) is a critical strategy in data reliability engineering aimed at enhancing the resilience and availability of data systems. A Single Point of Failure refers to <em>any component, system, or aspect of the infrastructure whose failure would lead to the failure of the entire system</em>. This could be a database, a network component, a server, or even a piece of software that is critical to data processing or storage.</p>
<p>The goal of eliminating SPOFs is to ensure that no single failure can disrupt the entire service or data flow. This is achieved through redundancy, fault tolerance, and careful system design. Here’s how it relates to data reliability:</p>
<h3 id="redundancy-2"><a class="header" href="#redundancy-2">Redundancy</a></h3>
<p>Introducing redundancy involves duplicating critical components or services so that if one fails, the other can take over without interruption. For example, having multiple data servers, redundant network paths, or replicated databases can prevent downtime caused by the failure of any single component.</p>
<h3 id="fault-tolerance-2"><a class="header" href="#fault-tolerance-2">Fault Tolerance</a></h3>
<p>Building systems to be fault-tolerant means they can continue operating correctly even if some components fail. This might involve implementing software that can reroute data flows away from failed components or hardware that can automatically switch to backup systems.</p>
<h3 id="distributed-architectures"><a class="header" href="#distributed-architectures">Distributed Architectures</a></h3>
<p>Designing systems with distributed architectures can spread out the risk, so no single component's failure can affect the entire system. For example, using cloud services that distribute data and processing across multiple geographical locations can safeguard against regional outages.</p>
<h3 id="regular-testing"><a class="header" href="#regular-testing">Regular Testing</a></h3>
<p>Regularly testing the failover and recovery processes is essential to ensure that redundancy measures work as expected when a real failure occurs. This can include disaster recovery drills and using chaos engineering principles to intentionally introduce failures.</p>
<h3 id="continuous-monitoring-and-alerting"><a class="header" href="#continuous-monitoring-and-alerting">Continuous Monitoring and Alerting</a></h3>
<p>Implementing continuous monitoring and alerting systems helps in the early detection of potential SPOFs before they cause system-wide failures. Monitoring can identify over-utilized resources, impending hardware failures, or software errors that could become SPOFs if not addressed.</p>
<p>By eliminating Single Points of Failure, data engineering teams can create more robust and reliable systems that can withstand individual component failures without significant impact on the overall system performance or data availability. This approach is fundamental to maintaining high levels of service and ensuring that data-driven operations can proceed without interruption.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="general-reliability-development-hazard-logs-grdhl"><a class="header" href="#general-reliability-development-hazard-logs-grdhl">General Reliability Development Hazard Logs (GRDHL)</a></h1>
<blockquote>
<p>General Reliability Development Hazard Logs (GRDHL) are comprehensive records used in various engineering disciplines to <strong>identify, document, and manage potential hazards</strong> throughout the development and lifecycle of a system or product. These logs typically include details about identified hazards, their potential impact, the likelihood of occurrence, mitigation strategies, and the status of the hazard (e.g., resolved, pending review).</p>
</blockquote>
<p>In the context of data reliability engineering, adapting General Reliability Development Hazard Logs could involve creating detailed logs that specifically focus on identifying and managing risks associated with data systems and processes. This could include:</p>
<ul>
<li><strong>Data Integrity Hazards</strong>: Issues that could lead to data corruption, loss, or unauthorized alteration.</li>
<li><strong>System Availability Risks</strong>: Potential system failures or downtimes that could make critical data inaccessible when needed.</li>
<li><strong>Data Quality Issues</strong>: Risks associated with inaccuracies, incompleteness, or inconsistencies in data that could compromise decision-making or operational efficiency.</li>
<li><strong>Security Vulnerabilities</strong>: Hazards related to data breaches, unauthorized access, or data leaks.</li>
<li><strong>Compliance and Privacy Risks</strong>: Potential hazards related to failing to meet regulatory compliance standards or protect sensitive information.</li>
</ul>
<p>For each identified hazard, the log would document the potential impact on data reliability, measures to mitigate the risk, responsible parties for addressing the hazard, and a timeline for resolution. Regularly reviewing and updating the hazard log would be a key practice in data reliability engineering, ensuring that emerging risks are promptly identified and managed to maintain the integrity, availability, and quality of data systems.</p>
<p>Examples:</p>
<table>
    <thead>
        <tr>
            <th>Hazard ID</th>
            <th>Description</th>
            <th>Impact Level</th>
            <th>Likelihood</th>
            <th>Mitigation Strategy</th>
            <th>Responsible</th>
            <th>Status</th>
            <th>Due Date</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>HZ001</td>
            <td>Database corruption due to system crash</td>
            <td>High</td>
            <td>Medium</td>
            <td>Implement regular database backups and failover systems</td>
            <td>Data Ops Team</td>
            <td>In Progress</td>
            <td>2023-03-15</td>
        </tr>
        <tr>
            <td>HZ002</td>
            <td>Unauthorized data access</td>
            <td>Critical</td>
            <td>Low</td>
            <td>Enhance authentication protocols and access controls</td>
            <td>Security Team</td>
            <td>Open</td>
            <td>2023-04-01</td>
        </tr>
        <tr>
            <td>HZ003</td>
            <td>Inaccurate sales data due to input errors</td>
            <td>Medium</td>
            <td>High</td>
            <td>Deploy data validation checks at entry points</td>
            <td>Data Quality Team</td>
            <td>Resolved</td>
            <td>2023-02-28</td>
        </tr>
        <tr>
            <td>HZ004</td>
            <td>Non-compliance with GDPR</td>
            <td>Critical</td>
            <td>Medium</td>
            <td>Conduct a GDPR audit and update data handling policies</td>
            <td>Legal Team</td>
            <td>In Progress</td>
            <td>2023-05-10</td>
        </tr>
        <tr>
            <td>HZ005</td>
            <td>Data lake performance degradation</td>
            <td>Medium</td>
            <td>Medium</td>
            <td>Optimize data storage and query indexing</td>
            <td>Data Engineering Team</td>
            <td>Open</td>
            <td>2023-04-15</td>
        </tr>
    </tbody>
</table>
<p>This table illustrates how potential hazards to data reliability are systematically identified, evaluated, and managed within an organization. Each entry includes a unique identifier for the hazard, a brief description, an assessment of the potential impact and likelihood of the hazard occurring, proposed strategies for mitigating the risk, the team responsible for addressing the hazard, the current status of mitigation efforts, and a target date for resolution. Regular updates and reviews of the hazard log ensure that the organization proactively addresses risks to maintain the reliability and integrity of its data systems.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="spare-parts-stocking-strategy"><a class="header" href="#spare-parts-stocking-strategy">Spare Parts Stocking Strategy</a></h1>
<blockquote>
<p>Ideally, clean data sources with complex transformations and cleanings, which save time and processing and can be used in multiple stages of multiple processes, will always be available. However, they may temporarily be unavailable or fail. Once such sources are identified and found to be critical to a system or process, it is prudent to have minimal cleaning and transformation tasks that work on raw data or sources of the source. These may not result in final data with the same level of detail but will be good enough.</p>
</blockquote>
<p>These tasks are not designed to be part of the normal process flow but are "spare parts" available for use when maintenance times are too long. The use of such tasks should be for the shortest time possible while the team has time to resolve failures in the original task or design its replacement.</p>
<p>In data engineering, a Spare Parts Stocking Strategy can be metaphorically applied to maintain high availability and reliability of data pipelines and systems. While in traditional contexts, this strategy involves keeping physical spare parts for machinery or equipment, in data engineering, it translates to having backup processes, data sources, and systems in place to ensure continuity in data operations. Here’s how it could be used:</p>
<h3 id="backup-data-processes"><a class="header" href="#backup-data-processes">Backup Data Processes</a></h3>
<p>Just as spare parts can replace failed components in machinery, backup data processes can take over when primary data processes fail. For example, if a primary ETL (Extract, Transform, Load) process fails due to an issue with a data source or transformation logic, a backup ETL process can be initiated. This backup process might use a different data source or a simplified transformation logic to ensure that essential data flows continue, albeit possibly at a reduced fidelity or completeness.</p>
<h3 id="redundant-data-sources"><a class="header" href="#redundant-data-sources">Redundant Data Sources</a></h3>
<p>Having alternate data sources is akin to having spare parts for critical components. If a primary data source becomes unavailable (e.g., due to an API outage or data corruption), the data engineering process can switch to a redundant data source to minimize downtime. This ensures that data pipelines are not entirely dependent on a single source and can continue operating even when one source fails.</p>
<h3 id="pre-processed-data-reservoirs"><a class="header" href="#pre-processed-data-reservoirs">Pre-Processed Data Reservoirs</a></h3>
<p>Maintaining pre-processed versions of critical datasets can be seen as having spare parts ready to be used immediately. In case of a processing failure in real-time data pipelines, these pre-processed datasets can be quickly utilized to ensure continuity in data availability for reporting, analytics, or other downstream processes.</p>
<h3 id="simplified-or-degraded-processing-modes"><a class="header" href="#simplified-or-degraded-processing-modes">Simplified or Degraded Processing Modes</a></h3>
<p>In situations where complex data processing cannot be performed due to system failures, having a simplified or degraded mode of operation can serve as a "spare part." This approach involves having predefined, less resource-intensive processes that can provide essential functionality or data outputs until the primary systems are restored.</p>
<h3 id="automated-failover-mechanisms"><a class="header" href="#automated-failover-mechanisms">Automated Failover Mechanisms</a></h3>
<p>Automated systems that can detect failures and switch to backup processes or systems without manual intervention can be seen as having an automated spare parts deployment system. These mechanisms ensure minimal disruption to data services by quickly responding to failures.</p>
<h3 id="documentation-and-testing"><a class="header" href="#documentation-and-testing">Documentation and Testing</a></h3>
<p>Just as spare parts need to be compatible and tested for specific machinery, backup data processes and sources need to be well-documented and regularly tested to ensure they can effectively replace primary processes when needed. Regular drills or simulations of failures can help ensure that the spare processes are ready to be deployed at a moment's notice.</p>
<p>By adopting a Spare Parts Stocking Strategy in data engineering, organizations can enhance the resilience of their data infrastructure, ensuring that data processing and availability are maintained even in the face of system failures or disruptions. This strategy is crucial for businesses where data availability directly impacts decision-making, operations, and customer satisfaction.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="availability-controls"><a class="header" href="#availability-controls">Availability Controls</a></h1>
<blockquote>
<p>Availability failures can occur for numerous reasons (from hardware to bugs), and some systems or processes are significant enough that availability controls should be implemented to ensure that certain services or data remain available when such failures occur.</p>
</blockquote>
<p>Availability controls range from using periodic data backups, snapshots, time travel, redundant processes, backup systems in local or cloud servers, etc.</p>
<p>Availability Controls in data engineering are mechanisms and strategies implemented to ensure that data and data processing capabilities are available when needed, particularly in the face of failures, maintenance, or unexpected demand spikes. These controls are crucial for maintaining the reliability and performance of data systems. Here's how they can be used in data engineering:</p>
<h3 id="data-backups"><a class="header" href="#data-backups">Data Backups</a></h3>
<p>Regular data backups are a fundamental availability control. By maintaining copies of critical datasets, data engineers can ensure that data can be restored in the event of corruption, accidental deletion, or data storage failures. Backups can be scheduled at regular intervals and stored in secure, geographically distributed locations to safeguard against site-specific disasters.</p>
<h3 id="redundant-data-storage"><a class="header" href="#redundant-data-storage">Redundant Data Storage</a></h3>
<p>Using redundant data storage solutions, such as RAID configurations in hardware or distributed file systems in cloud environments, can enhance data availability. These systems store copies of data across multiple disks or nodes, ensuring that the failure of a single component does not result in data loss and that data remains accessible even during partial system outages.</p>
<h3 id="high-availability-architectures"><a class="header" href="#high-availability-architectures">High Availability Architectures</a></h3>
<p>Designing data systems with high availability in mind involves deploying critical components in a redundant manner across multiple servers or clusters. This can include setting up active-active or active-passive configurations for databases, ensuring that if one instance fails, another can immediately take over without disrupting data access.</p>
<h3 id="disaster-recovery-plans"><a class="header" href="#disaster-recovery-plans">Disaster Recovery Plans</a></h3>
<p>Disaster recovery planning involves defining strategies and procedures for recovering from major incidents, such as natural disasters, cyber-attacks, or significant hardware failures. This includes not only data restoration from backups but also the rapid provisioning of replacement computing resources and network infrastructure.</p>
<h3 id="load-balancing-and-scaling"><a class="header" href="#load-balancing-and-scaling">Load Balancing and Scaling</a></h3>
<p>Load balancers distribute incoming data requests across multiple servers or services, preventing any single point from becoming overwhelmed, which could lead to failures and data unavailability. Similarly, implementing auto-scaling for data processing and storage resources can ensure that the system can handle varying loads, maintaining availability during peak demand periods.</p>
<h3 id="data-quality-gates"><a class="header" href="#data-quality-gates">Data Quality Gates</a></h3>
<p>Data quality gates are checkpoints in data pipelines where data is validated against predefined quality criteria. By ensuring that only accurate and complete data moves through the system, these gates help prevent errors and inconsistencies that could lead to processing failures and data unavailability.</p>
<h3 id="monitoring-and-alerting"><a class="header" href="#monitoring-and-alerting">Monitoring and Alerting</a></h3>
<p>Continuous monitoring of data systems and pipelines allows for the early detection of issues that could impact availability. Coupled with an alerting system, monitoring ensures that data engineers can quickly respond to and address potential failures, often before they impact end-users.</p>
<h3 id="versioning-and-data-immutability"><a class="header" href="#versioning-and-data-immutability">Versioning and Data Immutability</a></h3>
<p>Implementing data versioning and immutability can prevent data loss and ensure availability in the face of changes or updates. By keeping immutable historical versions of data, systems can revert to previous states if a new data version causes issues.</p>
<p>By employing these Availability Controls, data engineers can create resilient systems that ensure continuous access to data and data processing capabilities, critical for businesses that rely on timely and reliable data for operational decision-making and customer services.</p>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="backlog"><a class="header" href="#backlog">Backlog</a></h1>
<h2 id="chapters-section-i"><a class="header" href="#chapters-section-i">Chapters (Section I)</a></h2>
<ul>
<li>Data Engineering</li>
<li>Modern Data Architecture</li>
<li>DataOps</li>
<li>DevOps in Data Engineering</li>
<li>Data Platform Engineering</li>
<li>Observability</li>
<li>Data Governance &amp; Compliance</li>
<li>Data Security &amp; Privacy</li>
</ul>
<h2 id="topics"><a class="header" href="#topics">Topics</a></h2>
<p><strong>Fault Tolerant Systems</strong>:</p>
<ul>
<li>Technical documentation: Manuals, specifications, and guidelines that describe how systems or software operate. (Worth exploring further)</li>
<li>Safety cases: Documented arguments that a system is safe for a given application in a given environment.</li>
<li>Change Control: The process of managing changes to system or software specifications, ensuring that no unnecessary changes are made and that all changes are documented. (Worth exploring further)</li>
<li>Defensive Design: Strategies implemented in the design phase to anticipate and mitigate potential system failures or misuse. (Worth exploring further)</li>
<li>Derating: Using components below their maximum capacity to reduce the risk of failure, thereby increasing reliability.</li>
<li>Design Debt: The future cost incurred as a result of taking shortcuts or employing suboptimal design solutions in the short term.</li>
<li>Design Life: The expected lifetime for which a product is designed to last without major degradation or failure.</li>
<li>Design Thinking: A problem-solving approach that involves empathizing with users, defining problems, ideating solutions, prototyping, and testing.</li>
<li>Durability: The ability of a system or component to withstand wear and tear over time without significant degradation.</li>
<li>Edge Case: Rare or extreme conditions that occur at the boundary of operating parameters, which may not have been fully considered in the design. (Worth exploring further)</li>
<li>Entropy: In the context of systems, a measure of disorder or randomness that can lead to system degradation over time.</li>
<li>Error Tolerance: The capacity of a system to continue operating correctly in the presence of errors.</li>
<li>Fault Tolerance: The ability of a system to continue functioning in the event of a failure of some of its components. (Worth exploring further)</li>
<li>Fail Well: Designing systems to handle failures gracefully, minimizing the impact on users and the system.</li>
<li>Fail-Safe: Designing systems to default to a safe condition in the event of a failure. (Worth exploring further)</li>
<li>Graceful Degradation: The ability of a system to continue providing functionality when some subsystems or features fail.</li>
<li>Mistake Proofing &amp; Poka Yoke Technique: Methods used to avoid simple human errors in processes or systems, making it impossible or difficult to make mistakes.</li>
<li>No Fault Found: A situation where reported problems cannot be replicated or identified during testing or inspection.</li>
<li>Resilience: The ability of a system to recover quickly from difficulties or changes. (Worth exploring further)</li>
<li>Safety by Design: Incorporating safety considerations into the design process to minimize risks to users and the environment.</li>
<li>Self-Healing: The capability of a system to detect and fix problems automatically. (Worth exploring further)</li>
<li>Service Life: The period during which a system or component remains functional and meets performance requirements.</li>
<li>Systems Thinking: An approach to problem-solving that views problems as parts of an overall system, rather than isolated issues.</li>
<li>Testbed: An environment or platform used for testing new technologies or systems under controlled conditions.</li>
<li>Wear and Tear: The damage that naturally and inevitably occurs as a result of normal wear or aging.</li>
<li>Deconstructability: The ease with which a system can be disassembled, typically for maintenance, repair, or recycling.</li>
<li>Refinement: The process of making incremental improvements to a system or design to enhance functionality or performance.</li>
<li>Defense in Depth: A multi-layered strategy in system security that uses several components to protect operations and information. (Worth exploring further)</li>
<li>FMEA Design and Process: Failure Mode and Effects Analysis; a systematic method for evaluating processes to identify where and how they might fail and assessing the relative impact of different failures. (Worth exploring further)</li>
<li>Physics of Failure (PoF): An approach to reliability engineering that focuses on understanding the root causes of failure at the material, component, or system levels.</li>
<li>Built-in Self-test: A mechanism that allows a system to test its operation or functionality without the need for external equipment.</li>
<li>Eliminating Single Point of Failure (SPOF): Designing systems to remove any one component whose failure would cause the entire system to fail. (Worth exploring further)</li>
</ul>
<p><strong>Analysis</strong>:</p>
<ul>
<li>Root Cause Analysis (RCA): Identifying the primary cause of a fault or problem to prevent recurrence. (Worth exploring further)</li>
<li>Fault Tree Analysis (FTA): A top-down approach to identify various causes of system failures. (Worth exploring further)</li>
<li>Failure Mode and Effects Analysis (FMEA): Assessing potential failures in products or processes and their effects. (Worth exploring further)</li>
<li>Failure Mode, Effects, and Criticality Analysis (FMECA): An extension of FMEA that includes a criticality analysis to prioritize failure modes. (Worth exploring further)</li>
<li>Reliability, Availability, and Maintainability Study (RAMS): Evaluating a system's reliability, its availability for use, and ease of maintenance. (Worth exploring further)</li>
<li>Mission Readiness Analysis: Assessing whether a system is ready and capable of performing its intended mission.</li>
<li>Functional System Failure Analysis: Investigating failures based on the system's functional requirements.</li>
<li>Inherent Design Reliability Analysis: Analyzing reliability that's built into the design of a system.</li>
<li>Use/Load Analysis and Wear Calculations: Evaluating how use and load contribute to system wear and potential failure.</li>
<li>Fatigue and Creep Analysis: Examining how materials deteriorate under repeated stress (fatigue) and long-term stress (creep).</li>
<li>Component Stress Analysis: Analyzing stress on individual components to predict potential failure points.</li>
<li>Field Failure Monitoring &amp; Data Analysis: Collecting and analyzing data from systems in use to monitor for failures.</li>
<li>Chaos Engineering: Intentionally introducing disturbances to systems to test their reliability. (Worth exploring further)</li>
<li>Reliability Risk Assessments &amp; Hazard Analysis: Evaluating the risks and hazards that could impact system reliability.</li>
<li>Manufacturing Defect Analysis &amp; Residual Risk Analysis (RCA): Identifying defects from manufacturing and analyzing risks not mitigated by controls.</li>
<li>Weibull Analysis: A statistical method used in reliability engineering for analyzing life data, failure rates, and reliability. (Worth exploring further)</li>
<li>Accelerated Life Testing (ALT Analysis): Testing that uses elevated stresses to induce failures and predict a product's life under normal conditions.</li>
<li>Material Strength Analysis: Assessing the strength and durability of materials used in system components.</li>
<li>Quality of Service, Quality Control, Defect Rate, Failure Rate: Measures and controls to ensure system quality and reliability. (Worth exploring further)</li>
<li>Mean Time Between Failures (MTBF), Mean Time to Repair (MTTR), Mean Corrective Maintenance Time (MCMT), Mean Preventive Maintenance Time (MPMT), Mean Maintenance Hours per Repair (MMH/Repair), Maximum Corrective Maintenance Time (MaxCMT): Metrics that provide insights into system reliability, maintenance efficiency, and repair times. (Worth exploring further)</li>
</ul>
<p><strong>Data Quality</strong>:</p>
<ul>
<li>Data Integrity: The accuracy and consistency of data stored in a database or a data warehouse over its entire lifecycle. (Worth exploring further)</li>
<li>Data Cleansing: The process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database. (Worth exploring further)</li>
<li>Data Corruption: The occurrence of unintended changes to data in storage or transit, leading to a loss of data accuracy and integrity.</li>
<li>Data Degradation: The gradual corruption or loss of data quality and integrity over time, often due to factors like hardware failures, software issues, or data entry errors.</li>
<li>Data Artifact: Any undesired alteration in data introduced through a technical process; can include errors, anomalies, or peculiarities in data sets.</li>
<li>Data Rot: The gradual decay of storage media that leads to the loss or corruption of the data stored on them.</li>
<li>Conformance Quality: The degree to which a product or service meets specified standards or requirements; in data quality, it can relate to adherence to data models and validation rules.</li>
<li>Credence Quality: Aspects of a product or service that are difficult to observe or assess even after consumption; in data, this might relate to the inherent trustworthiness or credibility of data sources.</li>
<li>Quality Assurance: Activities and processes aimed at ensuring that the products or services meet the required quality standards before they reach the customer. (Worth exploring further)</li>
<li>Quality Control: The operational techniques and activities used to fulfill requirements for quality, including measuring, examining, testing, or gauging one or more characteristics of a product or service.</li>
<li>Service Quality: The comparison of perceived expectations with actual service performance; in data services, this could relate to the reliability, availability, and performance of data systems.</li>
<li>Experience Quality: Relates to the subjective assessment of the interaction with a product or service; in data systems, this might involve user satisfaction with data tools and platforms.</li>
<li>Referential Integrity: A concept in databases where table relationships are correctly maintained, ensuring that foreign keys match primary keys. (Worth exploring further)</li>
<li>Reusability: The ease with which parts of a system can be used in other systems; in data management, this might relate to the ability to repurpose data models, schemas, or ETL processes.</li>
</ul>
<p><strong>Maintenance</strong>:</p>
<ul>
<li>Maintenance Requirement Allocation: Assigning specific maintenance tasks and intervals based on reliability data and requirements to ensure optimal system performance. (Worth exploring)</li>
<li>Predictive and Preventive Maintenance: Techniques for anticipating maintenance needs to prevent failures and for performing regular maintenance to avoid unexpected breakdowns, respectively. (Worth exploring)</li>
<li>Reliability Centered Maintenance (RCM): A process to determine the most effective maintenance approach, including predictive, preventive, and corrective actions, to ensure system reliability. (Worth exploring)</li>
</ul>
<p><strong>Failures</strong>:</p>
<ul>
<li>Manufacturing-induced Failures: Failures that originate from defects or issues introduced during the manufacturing process.</li>
<li>Assembly-induced Failures: Failures caused by mistakes or issues during the assembly of components or systems.</li>
<li>Transport-induced Failures: Failures that occur as a result of damage or stresses during transportation.</li>
<li>Storage-induced Failures: Failures or degradation that happen while equipment or components are stored, often due to environmental conditions.</li>
<li>Systematic Failures: Failures that are predictable and consistent, often due to inherent design issues or flaws in the system.</li>
</ul>
<p><strong>Tests</strong>:</p>
<ul>
<li>System Diagnostics Design: Creating systems or procedures for diagnosing issues or failures within a system, crucial for effective maintenance and troubleshooting.</li>
<li>Failure/Reliability Testing: Conducting tests to evaluate a system's susceptibility to failure or to assess its reliability under various conditions. (Worth exploring)</li>
</ul>
<p><strong>Human Factors</strong>:</p>
<ul>
<li>Human Factors: The study of how humans interact with systems and environments, aiming to improve efficiency, safety, and usability.</li>
<li>Human Interaction: The ways in which people engage with and influence systems, and how these interactions can be optimized for better performance and safety.</li>
<li>Human Errors: Mistakes made by humans that can lead to system failures or reduced performance, often analyzed to improve system design and training.</li>
<li>Latent Human Error: Hidden errors in system design or operation that may not immediately result in failure but can lead to issues when combined with other factors.</li>
</ul>
<p><strong>DataOps</strong>:</p>
<p><strong>Business Process Management</strong>:</p>
<ul>
<li>BPM</li>
<li>BPI</li>
<li>BPE</li>
<li>BPA</li>
<li>BPR</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="test-images"><a class="header" href="#test-images">Test Images</a></h1>
<h2 id="original-svg"><a class="header" href="#original-svg">Original SVG</a></h2>
<p align="center">
  <img src="test/../assets/example/example.svg" alt="Example - Original">
</p>
<h2 id="instad-blue-pen-svg"><a class="header" href="#instad-blue-pen-svg">Instad (Blue-Pen) SVG</a></h2>
<p align="center">
  <img src="test/../assets/example/example-instad.svg" alt="Example - Original">
</p><div style="break-before: page; page-break-before: always;"></div><h1 id="test-mdbook-admonish"><a class="header" href="#test-mdbook-admonish">Test mdbook-admonish</a></h1>
<!-- toc -->
<!-- markdown-link-check-disable -->
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p><a href="https://crates.io/crates/mdbook-admonish"><img src="https://img.shields.io/crates/v/mdbook-admonish.svg" alt="Latest version" /></a>
<a href="https://docs.rs/mdbook-admonish"><img src="https://img.shields.io/docsrs/mdbook-admonish" alt="docs.rs" /></a></p>
<p>A preprocessor for <a href="https://github.com/rust-lang/mdBook">mdbook</a> to add <a href="https://material.io/design">Material Design</a> admonishments, based on the <a href="https://squidfunk.github.io/mkdocs-material/reference/admonitions/">mkdocs-material</a> implementation.</p>
<p>It turns this:</p>
<pre><code>```admonish info
A beautifully styled message.
```
</code></pre>
<p>into this:</p>
<div id="admonition-info" class="admonition admonish-info">
<div class="admonition-title">
<p>Info</p>
<p><a class="admonition-anchor-link" href="test/admonish.html#admonition-info"></a></p>
</div>
<div>
<p>A beautifully styled message.</p>
</div>
</div>
<h2 id="usage"><a class="header" href="#usage">Usage</a></h2>
<h3 id="a-basic-admonish-block"><a class="header" href="#a-basic-admonish-block">A basic <code>admonish</code> block</a></h3>
<p>Use any <a href="https://spec.commonmark.org/0.30/#fenced-code-blocks">fenced code-block</a> as you normally would, but annotate it with <code>admonish &lt;admonition type&gt;</code>:</p>
<pre><code>```admonish example
My example is the best!
```
</code></pre>
<div id="admonition-example" class="admonition admonish-example">
<div class="admonition-title">
<p>Example</p>
<p><a class="admonition-anchor-link" href="test/admonish.html#admonition-example"></a></p>
</div>
<div>
<p>My example is the best!</p>
</div>
</div>
<p>See the <a href="test/%5B./reference.md#directives%5D(https://github.com/tommilligan/mdbook-admonish/blob/main/book/src/reference.html">list of directives</a> for a full list of supported admonitions. You'll find:</p>
<ul>
<li><code>info</code></li>
<li><code>warning</code></li>
<li><code>danger</code></li>
<li><code>example</code></li>
</ul>
<p>and quite a few more!</p>
<p>You can also leave out the admonition type altogether, in which case it will default to <code>note</code>:</p>
<pre><code>```admonish
A plain note.
```
</code></pre>
<div id="admonition-note" class="admonition admonish-note">
<div class="admonition-title">
<p>Note</p>
<p><a class="admonition-anchor-link" href="test/admonish.html#admonition-note"></a></p>
</div>
<div>
<p>A plain note.</p>
</div>
</div>
<h3 id="invalid-blocks"><a class="header" href="#invalid-blocks">Invalid blocks</a></h3>
<p>By default, if an <code>admonish</code> block cannot be parsed, an error will be rendered in the output:</p>
<pre><code>```admonish title="\j"
This block will error
```
</code></pre>
<div id="admonition-error-rendering-admonishment" class="admonition admonish-bug">
<div class="admonition-title">
<p>Error rendering admonishment</p>
<p><a class="admonition-anchor-link" href="test/admonish.html#admonition-error-rendering-admonishment"></a></p>
</div>
<div>
<p>Failed with:</p>
<pre><code class="language-log">TOML parsing error: TOML parse error at line 1, column 10
  |
1 | title="\j"
  |          ^
invalid escape sequence
expected `b`, `f`, `n`, `r`, `t`, `u`, `U`, `\`, `"`

</code></pre>
<p>Original markdown input:</p>
<pre><code class="language-markdown">```admonish title="\j"
This block will error
```
</code></pre>
</div>
</div>
<p>You can also configure the build to fail loudly, by setting <code>on_failure = "bail"</code> in <code>book.toml</code>. See the <a href="test/%5B./reference.md#booktoml-configuration%5D(https://github.com/tommilligan/mdbook-admonish/blob/main/book/src/reference.html">configuration reference</a> for more details.</p>
<h3 id="additional-options"><a class="header" href="#additional-options">Additional Options</a></h3>
<p>You can pass additional options to each block. The options are structured as TOML key-value pairs.</p>
<p>Note that some options can be passed globally, through the <code>default</code> section in <code>book.toml</code>. See the <a href="test/%5B./reference.md#booktoml-configuration%5D(https://github.com/tommilligan/mdbook-admonish/blob/main/book/src/reference.html">configuration reference</a> for more details.</p>
<h4 id="custom-title"><a class="header" href="#custom-title">Custom title</a></h4>
<p>A custom title can be provided, contained in a double quoted TOML string.
Note that TOML escapes must be escaped again - for instance, write <code>\"</code> as <code>\\"</code>.</p>
<pre><code>```admonish warning title="Data loss"
The following steps can lead to irrecoverable data corruption.
```
</code></pre>
<div id="admonition-data-loss" class="admonition admonish-warning">
<div class="admonition-title">
<p>Data loss</p>
<p><a class="admonition-anchor-link" href="test/admonish.html#admonition-data-loss"></a></p>
</div>
<div>
<p>The following steps can lead to irrecoverable data corruption.</p>
</div>
</div>
<p>You can also remove the title bar entirely, by specifying the empty string:</p>
<pre><code>```admonish success title=""
This will take a while, go and grab a drink of water.
```
</code></pre>
<div id="admonition-default" class="admonition admonish-success">
<div>
<p>This will take a while, go and grab a drink of water.</p>
</div>
</div>
<h4 id="nested-markdownhtml"><a class="header" href="#nested-markdownhtml">Nested Markdown/HTML</a></h4>
<p>Markdown and HTML can be used in the inner content, as you'd expect:</p>
<pre><code>```admonish tip title="_Referencing_ and &lt;i&gt;dereferencing&lt;/i&gt;"
The opposite of *referencing* by using `&amp;` is *dereferencing*, which is
accomplished with the &lt;span style="color: hotpink"&gt;dereference operator&lt;/span&gt;, `*`.
```
</code></pre>
<div id="admonition-_referencing_-and-dereferencing" class="admonition admonish-tip">
<div class="admonition-title">
<p><em>Referencing</em> and <i>dereferencing</i></p>
<p><a class="admonition-anchor-link" href="test/admonish.html#admonition-_referencing_-and-dereferencing"></a></p>
</div>
<div>
<p>The opposite of <em>referencing</em> by using <code>&amp;</code> is <em>dereferencing</em>, which is
accomplished with the <span style="color: hotpink">dereference operator</span>, <code>*</code>.</p>
</div>
</div>
<p>If you have code blocks you want to include in the content, use <a href="https://spec.commonmark.org/0.30/#fenced-code-blocks">tildes for the outer code fence</a>:</p>
<pre><code>~~~admonish bug
This syntax won't work in Python 3:
```python
print "Hello, world!"
```
~~~
</code></pre>
<div id="admonition-bug" class="admonition admonish-bug">
<div class="admonition-title">
<p>Bug</p>
<p><a class="admonition-anchor-link" href="test/admonish.html#admonition-bug"></a></p>
</div>
<div>
<p>This syntax won't work in Python 3:</p>
<pre><code class="language-python">print "Hello, world!"
</code></pre>
</div>
</div>
<h4 id="custom-styling"><a class="header" href="#custom-styling">Custom styling</a></h4>
<p>If you want to provide custom styling to a specific admonition, you can attach one or more custom classnames:</p>
<pre><code>```admonish note class="custom-0 custom-1"
Styled with my custom CSS class.
```
</code></pre>
<p>Will yield something like the following HTML, which you can then apply styles to:</p>
<pre><code class="language-html">&lt;div class="admonition note custom-0 custom-1"
    ...
&lt;/div&gt;
</code></pre>
<h4 id="custom-css-id"><a class="header" href="#custom-css-id">Custom CSS ID</a></h4>
<p>If you want to customize the CSS <code>id</code> field, set <code>id="custom-id"</code>.
This will ignore <a href="test/reference.html#default"><code>default.css_id_prefix</code></a>.</p>
<p>The default id is a normalized version of the admonishment's title,
prefixed with the <code>default.css_id_prefix</code>,
with an appended number if multiple blocks would have the same id.</p>
<p>Setting the <code>id</code> field will <em>ignore</em> all other ids and the duplicate counter.</p>
<pre><code>```admonish info title="My Info" id="my-special-info"
Link to this block with `#my-special-info` instead of the default `#admonition-my-info`.
```
</code></pre>
<h4 id="collapsible"><a class="header" href="#collapsible">Collapsible</a></h4>
<p>For a block to be initially collapsible, and then be openable, set <code>collapsible=true</code>:</p>
<pre><code>```admonish collapsible=true
Content will be hidden initially.
```
</code></pre>
<p>Will yield something like the following HTML, which you can then apply styles to:</p>
<details id="admonition-note-1" class="admonition admonish-note">
<summary class="admonition-title">
<p>Note</p>
<p><a class="admonition-anchor-link" href="test/admonish.html#admonition-note-1"></a></p>
</summary>
<div>
<p>Content will be hidden initially.</p>
</div>
</details>
<h3 id="custom-blocks"><a class="header" href="#custom-blocks">Custom blocks</a></h3>
<p>You can add new block types via the <code>book.toml</code> config:</p>
<pre><code class="language-toml"># book.toml

[[preprocessor.admonish.custom]]
directive = "expensive"
icon = "./money-bag.svg"
color = "#24ab38"
aliases = ["money", "cash", "budget"]
</code></pre>
<p>You must then generate the relevant CSS file, and reference it in the <code>output.html</code> section.
<code>mdbook-admonish</code> has a helper to quickly do this for you:</p>
<pre><code class="language-bash"># Generates a file at ./mdbook-admonish-custom.css with your styles in
$ mdbook-admonish generate-custom ./mdbook-admonish-custom.css
</code></pre>
<pre><code class="language-toml"># book.toml

[output.html]
# Reference the new file, so it's bundled in with book styles
additional-css = ["./mdbook-admonish.css", "./mdbook-admonish-custom.css"]
</code></pre>
<p>You can then reference the new directive (or alias) like usual in your blocks.</p>
<pre><code>```admonish expensive
Remember, this operation costs money!
```
</code></pre>
<div id="admonition-note-2" class="admonition admonish-note">
<div class="admonition-title">
<p>Note</p>
<p><a class="admonition-anchor-link" href="test/admonish.html#admonition-note-2"></a></p>
</div>
<div>
<p>Remember, this operation costs money!</p>
</div>
</div>
<p>You can also set a default <code>title</code>. See the <a href="https://github.com/tommilligan/mdbook-admonish/blob/main/book/src/reference.md">Reference</a> page for more details.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="test-diagrams"><a class="header" href="#test-diagrams">Test Diagrams</a></h1>
<h2 id="mermaid"><a class="header" href="#mermaid">Mermaid</a></h2>
<pre class="mermaid">graph TD;
    A--&gt;B;
    A--&gt;C;
    B--&gt;D;
    C--&gt;D;
</pre>
<h2 id="plantuml"><a class="header" href="#plantuml">PlantUML</a></h2>
<p><img src="test/../mdbook-plantuml-img/099ffab5f5e292799ab82f248bfb3d1623fc5500.svg" alt="" /></p>
<h2 id="graphviz"><a class="header" href="#graphviz">Graphviz</a></h2>
<pre><code class="language-dot process">digraph {
    "processed" -&gt; "graph"
}
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="theme/mermaid.min.js"></script>
        <script src="theme/mermaid-init.js"></script>

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>