<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Data Reliability Engineering</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/catppuccin.css">
        <link rel="stylesheet" href="theme/catppuccin-highlight.css">

    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="COVER.html">Cover</a></li><li class="chapter-item expanded affix "><a href="TITLE.html">Title</a></li><li class="chapter-item expanded affix "><a href="SUMMARY.html">Summary</a></li><li class="chapter-item expanded affix "><a href="DEDICATION.html">Dedication</a></li><li class="chapter-item expanded affix "><a href="FOREWORD.html">Foreword</a></li><li class="chapter-item expanded affix "><a href="PREFACE.html">Preface</a></li><li class="chapter-item expanded affix "><a href="AUTHOR.html">Author</a></li><li class="chapter-item expanded affix "><a href="OBJECTIVES.html">Objectives</a></li><li class="chapter-item expanded affix "><a href="STRUCTURE.html">Structure</a></li><li class="chapter-item expanded "><a href="CONCEPTS.html"><strong aria-hidden="true">1.</strong> I - Concepts</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/systems_intro.html"><strong aria-hidden="true">1.1.</strong> Introduction to Systems</a></li><li class="chapter-item expanded "><a href="concepts/systems_reliability.html"><strong aria-hidden="true">1.2.</strong> Systems Reliability</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/impediments.html"><strong aria-hidden="true">1.2.1.</strong> Impediments</a></li><li class="chapter-item expanded "><a href="concepts/attributes.html"><strong aria-hidden="true">1.2.2.</strong> Attributes</a></li><li class="chapter-item expanded "><a href="concepts/mechanisms.html"><strong aria-hidden="true">1.2.3.</strong> Mechanisms</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/fault_prevention_avoidance.html"><strong aria-hidden="true">1.2.3.1.</strong> Fault Prevention: Avoidance</a></li><li class="chapter-item expanded "><a href="concepts/fault_tolerance.html"><strong aria-hidden="true">1.2.3.2.</strong> Fault Tolerance</a></li><li class="chapter-item expanded "><a href="concepts/fault_prevention_elimination.html"><strong aria-hidden="true">1.2.3.3.</strong> Fault Prevention: Elimination</a></li><li class="chapter-item expanded "><a href="concepts/fault_prediction.html"><strong aria-hidden="true">1.2.3.4.</strong> Fault Prediction</a></li><li class="chapter-item expanded "><a href="concepts/reliability_tools.html"><strong aria-hidden="true">1.2.3.5.</strong> Reliability Tools</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="concepts/data_quality.html"><strong aria-hidden="true">1.3.</strong> Data Quality</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data_quality_foundations.html"><strong aria-hidden="true">1.3.1.</strong> Foundations of Data Quality</a></li><li class="chapter-item expanded "><a href="concepts/data_quality_master_data.html"><strong aria-hidden="true">1.3.2.</strong> Master Data</a></li><li class="chapter-item expanded "><a href="concepts/data_quality_management.html"><strong aria-hidden="true">1.3.3.</strong> Data Management</a></li><li class="chapter-item expanded "><a href="concepts/data_quality_models.html"><strong aria-hidden="true">1.3.4.</strong> Data Quality Models</a></li><li class="chapter-item expanded "><a href="concepts/data_quality_conclusions.html"><strong aria-hidden="true">1.3.5.</strong> Final Thoughts on Data Quality</a></li></ol></li><li class="chapter-item expanded "><a href="concepts/data_relibility.html"><strong aria-hidden="true">1.4.</strong> Data Reliability</a></li><li class="chapter-item expanded "><a href="concepts/processes.html"><strong aria-hidden="true">1.5.</strong> Processes</a></li><li class="chapter-item expanded "><a href="concepts/operations.html"><strong aria-hidden="true">1.6.</strong> Operations</a></li><li class="chapter-item expanded "><a href="concepts/data_architecture.html"><strong aria-hidden="true">1.7.</strong> Data Architecture</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data_sources.html"><strong aria-hidden="true">1.7.1.</strong> Data Sources</a></li><li class="chapter-item expanded "><a href="concepts/data_tier.html"><strong aria-hidden="true">1.7.2.</strong> Data Tier</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data_lake.html"><strong aria-hidden="true">1.7.2.1.</strong> Data Lake</a></li><li class="chapter-item expanded "><a href="concepts/data_warehouse.html"><strong aria-hidden="true">1.7.2.2.</strong> Data Warehouse</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data_warehouse_tier_architecture.html"><strong aria-hidden="true">1.7.2.2.1.</strong> Two-Tier vs Three-Tier Architecture</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data_warehouse_application_tier.html"><strong aria-hidden="true">1.7.2.2.1.1.</strong> Application Tier</a></li></ol></li><li class="chapter-item expanded "><a href="concepts/data_modelling.html"><strong aria-hidden="true">1.7.2.2.2.</strong> Data Modelling</a></li></ol></li><li class="chapter-item expanded "><a href="concepts/data_lakehouse.html"><strong aria-hidden="true">1.7.2.3.</strong> Data Lakehouse</a></li><li class="chapter-item expanded "><a href="concepts/data_marts.html"><strong aria-hidden="true">1.7.2.4.</strong> Data Marts</a></li></ol></li><li class="chapter-item expanded "><a href="concepts/application_tier.html"><strong aria-hidden="true">1.7.3.</strong> Application Tier</a></li><li class="chapter-item expanded "><a href="concepts/presentation_tier.html"><strong aria-hidden="true">1.7.4.</strong> Presentation Tier</a></li><li class="chapter-item expanded "><a href="concepts/metadata_management_tools.html"><strong aria-hidden="true">1.7.5.</strong> Metadata Management Tools</a></li></ol></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.</strong> II - Use Cases</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.</strong> A - Aranduka Inc.</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.</strong> Data Architecture</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.1.</strong> Operational System and Internal Data Sources</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.2.</strong> Integrating Data Partners</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.3.</strong> Designing the Data Lake</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.3.1.</strong> Anonymized Data</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.3.2.</strong> Distilled Data</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.4.</strong> Designing the Data Warehouse</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.4.1.</strong> Core Data</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.5.</strong> Designing the Data Marts</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.2.</strong> BICC & BI</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.2.1.</strong> Building Reliable Pipelines</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.2.2.</strong> Data Quality Assurance & Monitoring</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.2.3.</strong> Continuous Service</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.3.</strong> Growth, Marketing & Attribution Models</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.4.</strong> Multidimensional Analysis: Geo vs Verticals</div></li></ol></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.</strong> III - Incorporating Data Reliability Engineering</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">3.1.</strong> Solutions Architects</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">3.1.1.</strong> Data Architect</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.2.</strong> Data Engineers</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.3.</strong> Backend Engineers</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.4.</strong> BI Engineers</div></li></ol></li><li class="chapter-item expanded "><a href="EPILOGUE.html">Epilogue</a></li><li class="chapter-item expanded affix "><a href="DICTIONARY.html">Dictionary</a></li><li class="chapter-item expanded affix "><a href="REFERENCES.html">References</a></li><li class="chapter-item expanded affix "><a href="NEXT.html">Next</a></li><li class="chapter-item expanded affix "><a href="BACK_COVER.html">Back Cover</a></li><li class="chapter-item expanded affix "><a href="backlog.html">Backlog</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="latte">Latte</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="frappe">Frappé</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="macchiato">Macchiato</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mocha">Mocha</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Data Reliability Engineering</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div style="break-before: page; page-break-before: always;"></div><p>Data Reliability Engineering</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="summary"><a class="header" href="#summary">Summary</a></h1>
<p><a href="./COVER.html">Cover</a>
<a href="./TITLE.html">Title</a>
<a href="./SUMMARY.html">Summary</a>
<a href="./DEDICATION.html">Dedication</a>
<a href="./FOREWORD.html">Foreword</a>
<a href="./PREFACE.html">Preface</a>
<a href="./AUTHOR.html">Author</a>
<a href="./OBJECTIVES.html">Objectives</a>
<a href="./STRUCTURE.html">Structure</a></p>
<ul>
<li><a href="./CONCEPTS.html">I - Concepts</a>
<ul>
<li><a href="./concepts/systems_intro.html">Introduction to Systems</a></li>
<li><a href="./concepts/systems_reliability.html">Systems Reliability</a>
<ul>
<li><a href="./concepts/impediments.html">Impediments</a></li>
<li><a href="./concepts/attributes.html">Attributes</a></li>
<li><a href="./concepts/mechanisms.html">Mechanisms</a>
<ul>
<li><a href="./concepts/fault_prevention_avoidance.html">Fault Prevention: Avoidance</a></li>
<li><a href="./concepts/fault_tolerance.html">Fault Tolerance</a></li>
<li><a href="./concepts/fault_prevention_elimination.html">Fault Prevention: Elimination</a></li>
<li><a href="./concepts/fault_prediction.html">Fault Prediction</a></li>
<li><a href="./concepts/reliability_tools.html">Reliability Tools</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="./concepts/data_quality.html">Data Quality</a>
<ul>
<li><a href="./concepts/data_quality_foundations.html">Foundations of Data Quality</a></li>
<li><a href="./concepts/data_quality_master_data.html">Master Data</a></li>
<li><a href="./concepts/data_quality_management.html">Data Management</a></li>
<li><a href="./concepts/data_quality_models.html">Data Quality Models</a></li>
<li><a href="./concepts/data_quality_conclusions.html">Final Thoughts on Data Quality</a></li>
</ul>
</li>
<li><a href="./concepts/data_relibility.html">Data Reliability</a></li>
<li><a href="./concepts/processes.html">Processes</a></li>
<li><a href="./concepts/operations.html">Operations</a></li>
<li><a href="./concepts/data_architecture.html">Data Architecture</a>
<ul>
<li><a href="./concepts/data_sources.html">Data Sources</a></li>
<li><a href="./concepts/data_tier.html">Data Tier</a>
<ul>
<li><a href="./concepts/data_lake.html">Data Lake</a></li>
<li><a href="./concepts/data_warehouse.html">Data Warehouse</a>
<ul>
<li><a href="./concepts/data_warehouse_tier_architecture.html">Two-Tier vs Three-Tier Architecture</a>
<ul>
<li><a href="./concepts/data_warehouse_application_tier.html">Application Tier</a></li>
</ul>
</li>
<li><a href="./concepts/data_modelling.html">Data Modelling</a></li>
</ul>
</li>
<li><a href="./concepts/data_lakehouse.html">Data Lakehouse</a></li>
<li><a href="./concepts/data_marts.html">Data Marts</a></li>
</ul>
</li>
<li><a href="./concepts/application_tier.html">Application Tier</a></li>
<li><a href="./concepts/presentation_tier.html">Presentation Tier</a></li>
<li><a href="./concepts/metadata_management_tools.html">Metadata Management Tools</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="">II - Use Cases</a>
<ul>
<li><a href="">A - Aranduka Inc.</a>
<ul>
<li><a href="">Data Architecture</a>
<ul>
<li><a href="">Operational System and Internal Data Sources</a></li>
<li><a href="">Integrating Data Partners</a></li>
<li><a href="">Designing the Data Lake</a>
<ul>
<li><a href="">Anonymized Data</a></li>
<li><a href="">Distilled Data</a></li>
</ul>
</li>
<li><a href="">Designing the Data Warehouse</a>
<ul>
<li><a href="">Core Data</a></li>
</ul>
</li>
<li><a href="">Designing the Data Marts</a></li>
</ul>
</li>
<li><a href="">BICC &amp; BI</a>
<ul>
<li><a href="">Building Reliable Pipelines</a></li>
<li><a href="">Data Quality Assurance &amp; Monitoring</a></li>
<li><a href="">Continuous Service</a></li>
</ul>
</li>
<li><a href="">Growth, Marketing &amp; Attribution Models</a></li>
<li><a href="">Multidimensional Analysis: Geo vs Verticals</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="">III - Incorporating Data Reliability Engineering</a>
<ul>
<li><a href="">Solutions Architects</a>
<ul>
<li><a href="">Data Architect</a></li>
</ul>
</li>
<li><a href="">Data Engineers</a></li>
<li><a href="">Backend Engineers</a></li>
<li><a href="">BI Engineers</a></li>
</ul>
</li>
</ul>
<p><a href="./EPILOGUE.html">Epilogue</a>
<a href="./DICTIONARY.html">Dictionary</a>
<a href="./REFERENCES.html">References</a>
<a href="./NEXT.html">Next</a>
<a href="./BACK_COVER.html">Back Cover</a></p>
<p><a href="./backlog.html">Backlog</a></p>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="concepts"><a class="header" href="#concepts">Concepts</a></h1>
<blockquote>
<p>The first part of this book exposes the different concepts around the data reliability engineering subject. It's intended to be heavilly technical, in contrast with the subsequent parts, intended to explore practical use cases.</p>
</blockquote>
<h2 id="introduction-to-systems-and-systems-reliability"><a class="header" href="#introduction-to-systems-and-systems-reliability"><a href="./concepts/systems_intro.html">Introduction to Systems</a> and <a href="./concepts/systems_reliability.html">Systems Reliability</a></a></h2>
<blockquote>
<p>These chapters explore what are systems, what is reliability, and how to understand systems reliability, specially its impediments, its attributes, and mechanisms to design and maintain reliable systems. All this for general systems, data systems, and data products.</p>
</blockquote>
<h2 id="data-quality"><a class="header" href="#data-quality"><a href="./concepts/data_quality.html">Data Quality</a></a></h2>
<blockquote>
<p>This chapter explores what is data, what is quality, and what is data quality, to finally explore what is data reliability.
The goal is to understand these concepts in all aspects of the data: life cycle, design, modelling, governance, management, access, security, uses, legal frameworks, best practices, maturity, standards, etc.</p>
</blockquote>
<h2 id="processes"><a class="header" href="#processes"><a href="./concepts/processes.html">Processes</a></a></h2>
<blockquote>
<p>This chapter explores, for a given system, the concept of data processes, data and information flow, workflows, orchestration, pipelines, ETL, and ELT.</p>
</blockquote>
<h2 id="operations"><a class="header" href="#operations"><a href="./concepts/operations.html">Operations</a></a></h2>
<blockquote>
<p>This chapter explores the concept of SRE, DataOps, DevOps, Agile methodologies, CI/CD, and other methodologies to assure reliable data operations.</p>
</blockquote>
<h2 id="data-architecture"><a class="header" href="#data-architecture"><a href="./concepts/data_architecture.html">Data Architecture</a></a></h2>
<blockquote>
<p>This chapter explores what is data architecture, including its sources, its storage (Data Lake, Data Warehouses, Data Marts), its application (OLAP servers, processing engines), and its presentation (dashboards, reports).</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction-to-systems"><a class="header" href="#introduction-to-systems">Introduction to Systems</a></h1>
<blockquote>
<p>In the context of data reliability, a system can be defined as a collection of interrelated components working together towards a common goal, often to process, store, and manage data. These components can include hardware, software, databases, procedures, and people, all interacting in a structured way to achieve efficient and reliable data handling.</p>
</blockquote>
<p>For people interested in studying data reliability, it's important to understand a system from both a <strong>technical</strong> and <strong>operational perspective</strong>. Technically, a system would include the architecture, technology, and protocols that ensure data integrity, availability, and consistency. Operationally, it involves the procedures and practices that maintain the system's performance and reliability over time.</p>
<p>In essence, when talking about data reliability, a system can be thought of as the entire ecosystem that supports the lifecycle of data, from its creation and storage to its retrieval and usage. This includes considerations of redundancy, fault tolerance, backup procedures, security measures, and regular maintenance practices, all of which contribute to the system's overall reliability and the trustworthiness of the service it provides.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="systems-reliability"><a class="header" href="#systems-reliability">Systems Reliability</a></h1>
<blockquote>
<p>The reliability of a system is the property that allows the system's service to be justifiably qualified as reliable.</p>
</blockquote>
<p>The goal of this chapter is to introduce the concepts of reliability and safety explored by Alan Burns and Andy Wellings in their book "Real-Time Systems and Programming Languages", concepts developed by different industries mainly between the 60s and 90s, and the concepts of Site Reliability Engineering (SRE) developed from the 2000s onwards, in addition to complementing it with reliability concepts worked on in other engineering fields (mechanical, industrial, etc.), as well as contextualizing it with concepts currently worked on in the software and computer systems industry.</p>
<p>This chapter is divided in three parts, each exploring one of these concepts:</p>
<p><a href="concepts/./impediments.html"><strong>Impediments</strong></a></p>
<blockquote>
<p>Impediments prevent a system from functioning perfectly, or are a consequence of it. This subsection will address the detection of different types of impediments, which include <strong>Failures</strong>, <strong>Errors</strong>, and <strong>Defects</strong>.</p>
</blockquote>
<p><a href="concepts/./attributes.html"><strong>Attributes</strong></a></p>
<blockquote>
<p>The way and measures by which the <strong>quality of a reliable service can be estimated</strong>.</p>
</blockquote>
<p><a href="concepts/./mechanisms.html"><strong>Mechanisms</strong></a></p>
<blockquote>
<p>The mechanisms through which system reliability is addressed, whether by internalization and adoption of best practices, or by the application of specific methodologies, architectures, or tools. This subsection aims to create a <strong>framework</strong> that engineers can adopt for system reliability from the design phase itself.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="impediments"><a class="header" href="#impediments">Impediments</a></h1>
<h2 id="failures-errors-and-faults"><a class="header" href="#failures-errors-and-faults">Failures, Errors, and Faults</a></h2>
<blockquote>
<p><strong>Failures</strong> are the result of unexpected internal problems that a system eventually exhibits in its external behavior. These problems are called <strong>errors</strong>, and their mechanical or algorithmic causes are referred to as <strong>faults</strong>.
When a system's behavior deviates from what is specified for it, it is said to have a <strong>failure</strong>, or the system have <strong>failed</strong>.</p>
</blockquote>
<p>Systems are composed of <strong>components</strong>, each of which can be considered a system in itself. Thus, a failure in one system can induce a fault in another, which may result in an error and a potential failure of this system. This can continue and produce an effect on any related system, and so on.</p>
<p>A faulty component of a system is one that will produce an error under a specific set of circumstances during the system's lifetime. Seen in terms of state transitions, <em>a system can be considered as a number of external and internal states</em>.</p>
<p>An external state not specified in the system's behavior will be considered a system failure. The system itself consists of a number of components (each with its own states), all contributing to the system's external behavior. The combination of these components' states is called the system's internal state. <em>An unspecified internal state is considered an error, and the component that produced the illegal state transition is said to be faulty</em>.</p>
<p>The three types of failures:</p>
<ul>
<li><strong>Transient failures</strong>: begin at a specific point in time, remain in the system for some period, and then disappear.</li>
<li><strong>Permanent failures</strong>: begin at a certain point and remain in the system until they are repaired.</li>
<li><strong>Intermittent failures</strong>: are transient failures that occur sporadically.</li>
</ul>
<h2 id="failure-modes"><a class="header" href="#failure-modes">Failure Modes</a></h2>
<blockquote>
<p>A system can fail in many ways. A designer may design the system assuming a finite number of failure modes, however, the system may fail in ways that were not anticipated.</p>
</blockquote>
<p>We can classify the failure modes of the services that a system provides, which are:</p>
<ul>
<li><strong>Value failures</strong>: the value associated with the service is incorrect.</li>
<li><strong>Timing failure</strong>: the service is completed at the wrong time.</li>
<li><strong>Arbitrary failure</strong>: a combination of value and timing failures.</li>
</ul>
<p>Value failure modes are called <strong>value domain</strong>, and are classified into <strong>boundary error</strong>, and <strong>wrong value</strong>, where the value is outside the stipulated range.</p>
<p>Failures in the time domain can cause the service to be delivered:</p>
<ul>
<li><strong>Too early</strong> (premature): the service is delivered before it is required.</li>
<li><strong>Too late</strong> (delayed or performance error): the service is delivered after it is required.</li>
<li><strong>Infinitely late</strong> (omission failure): the service is never delivered.</li>
<li><strong>Unexpected</strong> (commission failure or improvisation): the service is delivered without being expected.</li>
</ul>
<p>In general, we can assume the modes in which a system can fail:</p>
<ul>
<li><strong>Uncontrolled failure</strong>: a system that produces arbitrary errors, both in the value domain and in the time domain (including improvisation errors).</li>
<li><strong>Delay failure</strong>: a system that produces correct services in the value domain but suffers from timing delays.</li>
<li><strong>Silent failure</strong>: a system that produces correct services in both the value and time domains, until it fails. The only possible failure is omission, and when it occurs, all subsequent services will also suffer from omission failures.</li>
<li><strong>Crash failure</strong>: a system that has all the properties of a silent failure but allows other systems to detect that it has entered the state of silent failure.</li>
<li><strong>Controlled failure</strong>: a system that fails in a specified and controlled manner.</li>
<li><strong>Failure-free</strong>: a system that always produces the correct services.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="attributes"><a class="header" href="#attributes">Attributes</a></h1>
<h2 id="reliability"><a class="header" href="#reliability">Reliability</a></h2>
<blockquote>
<p>It is the probability <em>R(t)</em> that the system will <strong>continue functioning at the end of the process</strong>.</p>
</blockquote>
<p>The time <em>t</em> is measured in continuous working hours between diagnostics. The constant failure rate λ is measured in <em>failures/hour</em>. The useful life of a system component is the constant region (on a logarithmic scale) of the curve between the component's age and its failure rate. The region of the graph before equilibrium is the Burn-In Phase, and the region where the failure rate starts to increase is the End of Life Phase. Thus, we have <em>R(t) = exp(-λt)</em>.</p>
<h2 id="availability"><a class="header" href="#availability">Availability</a></h2>
<blockquote>
<p>It is the measure of the <strong>frequency of incorrect service periods</strong>.</p>
</blockquote>
<h2 id="dependability"><a class="header" href="#dependability">Dependability</a></h2>
<blockquote>
<p>Continuity of service delivery.</p>
</blockquote>
<p>It is a measure (probability) of the <strong>success with which the system conforms to the definitive specification of its behavior</strong>.</p>
<h2 id="safety"><a class="header" href="#safety">Safety</a></h2>
<blockquote>
<p>It is the absence of conditions that can cause damage and the propagation of <strong>catastrophic damage</strong> in production.</p>
</blockquote>
<p>However, as this definition can classify virtually any process as unsafe, we often consider the term <strong>mishap</strong>.</p>
<blockquote>
<p>A mishap is an <strong>unplanned event</strong> or sequence of events that can produce catastrophic damage.</p>
</blockquote>
<p>Despite its similarity to the definition of <strong>dependability</strong>, the difference in emphasis should be noted. Dependability is the measure of success with which the system conforms to the specification of its behavior, typically in terms of <strong>probability</strong>. Safety, however, is the <strong>improbability of conditions leading to a mishap occurring, regardless of whether the intended function is performed</strong>.</p>
<h2 id="integrity"><a class="header" href="#integrity">Integrity</a></h2>
<blockquote>
<p>It is the absence of conditions that can lead to inappropriate alterations of data in production. It is the <strong>improbability of conditions occurring that alter inappropriate data in production, regardless of whether the intended function is performed</strong>.</p>
</blockquote>
<h2 id="maintainability"><a class="header" href="#maintainability">Maintainability</a></h2>
<blockquote>
<p>Ability to undergo repairs and evolve.</p>
</blockquote>
<h2 id="scalability"><a class="header" href="#scalability">Scalability</a></h2>
<blockquote>
<p>Ability to adapt to business needs.</p>
</blockquote>
<h2 id="deficiencies"><a class="header" href="#deficiencies">Deficiencies</a></h2>
<blockquote>
<p>Circumstances that cause or are a product of <strong>unreliability</strong>.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mechanisms"><a class="header" href="#mechanisms">Mechanisms</a></h1>
<h2 id="fault-prevention-avoidance"><a class="header" href="#fault-prevention-avoidance"><a href="concepts/./fault_prevention_avoidance.html">Fault Prevention: Avoidance</a></a></h2>
<h2 id="fault-tolerance"><a class="header" href="#fault-tolerance"><a href="concepts/./fault_tolerance.html">Fault Tolerance</a></a></h2>
<h2 id="fault-prevention-elimination"><a class="header" href="#fault-prevention-elimination"><a href="concepts/./fault_prevention_elimination.html">Fault Prevention: Elimination</a></a></h2>
<h2 id="fault-predictions"><a class="header" href="#fault-predictions"><a href="concepts/./fault_prediction.html">Fault Predictions</a></a></h2>
<h2 id="reliability-tools"><a class="header" href="#reliability-tools"><a href="concepts/./reliability_tools.html">Reliability Tools</a></a></h2>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fault-prevention-avoidance-1"><a class="header" href="#fault-prevention-avoidance-1">Fault Prevention: Avoidance</a></h1>
<p>There are two phases in fault prevention: <strong>avoidance</strong> and <strong>elimination</strong>.</p>
<blockquote>
<p>Avoidance aims to limit the introduction of potentially defective data and objects during the execution of the process.</p>
</blockquote>
<p>Such as:</p>
<ul>
<li>The use of validated and clean information sources, when possible.</li>
<li>The implementation of data cleaning and validation processes for raw data.</li>
<li>The validation of table and column availability within databases.</li>
<li>The introduction of branch operators for effective data management.</li>
<li>The implementation of rigorous code review processes to maintain a clean and secure codebase.</li>
<li>The adoption of standardized coding practices and secure coding guidelines to minimize errors and security vulnerabilities.</li>
<li>The utilization of automated testing frameworks for continuous testing (unit, integration, system) throughout the development cycle.</li>
<li>The application of configuration management tools and practices to oversee changes in software and hardware, ensuring all modifications are authorized and tested.</li>
<li>The engagement in detailed requirement analysis and system design reviews to affirm the system's resilience against potential faults, including the use of modeling and simulation tools.</li>
<li>The incorporation of fail-safe and fail-soft designs to maintain system safety in case of failure, including redundancy strategies for critical components.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fault-tolerance-1"><a class="header" href="#fault-tolerance-1">Fault Tolerance</a></h1>
<blockquote>
<p>Given the limitations in fault prevention, especially as data and processes frequently change, it becomes necessary to resort to fault tolerance.</p>
</blockquote>
<p>There are different levels of fault tolerance:</p>
<ul>
<li><strong>Full tolerance</strong>: there is no management of adverse or unwanted conditions; the process does not adapt to validations and environmental variables or other external information for the execution of tasks.</li>
<li><strong>Controlled degradation</strong> (or graceful degradation): notifications are triggered in the presence of faults, and if they are significant enough to interrupt the task flow (thresholds, non-existence, or unavailability of data), branch operators will select the subsequent tasks.</li>
<li><strong>Fail-safe</strong>: detected faults are significant enough to determine that the process should not occur; a short-circuit or circuit breaker operator cancels the execution of subsequent tasks, stakeholders are notified, and if there is no automatic process to deal with the problem, the data team can take actions such as rerunning the processes that generate the necessary inputs or escalating the case.</li>
</ul>
<p>The design of fault-tolerant processes assumes:</p>
<ul>
<li>The task algorithms have been correctly designed.</li>
<li>All possible failure modes of the components are known.</li>
<li>All possible interactions between the process and its environment have been considered.</li>
</ul>
<h2 id="redundancy"><a class="header" href="#redundancy">Redundancy</a></h2>
<blockquote>
<p>All techniques used to achieve fault tolerance are based on adding external elements to the system to detect and recover from faults. These elements are redundant in the sense that they are not necessary for the system's normal operation; this is called <strong>protective redundancy</strong>. The goal of tolerance is to minimize redundancy while maximizing reliability, always under the constraints of system complexity and size. <em>Care must be taken when designing fault-tolerant systems, as components increase the complexity and maintenance of the entire system, which can in itself lead to less reliable systems</em>.</p>
</blockquote>
<p>Redundancy in systems is classified into static and dynamic. <strong>Static redundancy</strong>, or masking, involves using redundant components to hide the effects of faults. <strong>Dynamic redundancy</strong> is redundancy within a component that makes it indicate, implicitly or explicitly, that the output is erroneous; recovery must be provided by another component. This fault tolerance technique has four phases:</p>
<ol>
<li><strong>Error detection</strong>: no fault tolerance action will be taken until an error has been detected.</li>
<li><strong>Damage confinement and assessment</strong>: when an error is detected, the extent of the system that has been corrupted and its scope must be estimated (error diagnosis).</li>
<li><strong>Error recovery</strong>: this is one of the most important aspects of fault tolerance. Error recovery techniques should direct the corrupted system to a state from which it can continue its normal operation (perhaps with functional degradation).</li>
<li><strong>Failure treatment and service continuation</strong>: an error is a symptom of a failure; although the damage might have been repaired, the failure still exists, and therefore the error may recur unless some form of maintenance is performed.</li>
</ol>
<h3 id="1-error-detection"><a class="header" href="#1-error-detection">1. Error Detection</a></h3>
<blockquote>
<p>The effectiveness of a fault-tolerant system depends on the <strong>effectiveness of error detection</strong>.</p>
</blockquote>
<p>Error detection is classified into:</p>
<ul>
<li><strong>Environmental detections</strong>: Errors are detected in the environment in which the program runs. They are handled by exceptions.</li>
<li><strong>Application detection</strong>: Errors are detected within the application itself.
<ul>
<li><strong>Reverse checks</strong>: Applied in components with an isomorphic relationship (one-to-one) between input and output. In this method, the output is taken and the input is calculated, which is compared with the original input value. For real numbers, inexact comparison techniques must be adopted.</li>
<li><strong>Rationality checks</strong>: Based on the design and construction knowledge of the system. They verify that the state of the data or the value of an object is reasonable based on its intended use.</li>
</ul>
</li>
</ul>
<h3 id="2-damage-confinement-and-assessment"><a class="header" href="#2-damage-confinement-and-assessment">2. Damage Confinement and Assessment</a></h3>
<blockquote>
<p>There will always be a time magnitude between the occurrence of a defect and the detection of the error, making it important to assess any damage that may have occurred in this time interval.</p>
</blockquote>
<p>Although the type of error detected can provide ideas about the damage to the error handling routine, erroneous information could have been disseminated through the system and its environment. Thus, damage assessment is directly related to the precautions taken by the system designer for damage confinement. Damage confinement refers to structuring the system in such a way as to minimize the damage caused by a faulty component.</p>
<p>There are two main techniques for structuring systems to facilitate damage confinement: <strong>modular decomposition</strong> and <strong>atomic actions</strong>. Modular decomposition means that systems should be broken down into components, each represented by one or more modules. The interaction of the components occurs through well-defined interfaces, and the internal details of the modules are hidden and not directly accessible from the outside. This makes it more difficult for an error in one component to indiscriminately pass to another.</p>
<p>Modular decomposition provides the system with a static structure, while atomic actions provide it with a dynamic structure. An action is said to be atomic if there are no interactions between the activity and the system during the course of the action. These actions are used to move the system from one consistent state to another and to restrict the flow of information between components.</p>
<h3 id="3-error-recovery"><a class="header" href="#3-error-recovery">3. Error Recovery</a></h3>
<blockquote>
<p>Once the error situation has been detected and its possible damages have been assessed, error recovery procedures begin. This phase is probably the most important within fault tolerance techniques, which must transform an erroneous state of the system into another from which the system can continue its normal operation, perhaps with some service degradation.</p>
</blockquote>
<p>Here it's worth mentioning two strategies for error recovery: <strong>forward recovery</strong> and <strong>backward recovery</strong>. Forward error recovery attempts to continue from the erroneous state by making selective corrections to the system's state, including protecting any aspect of the controlled environment that could be put at risk or damaged by the failure.</p>
<p>Backward recovery is based on restoring the system to a safe state prior to the one in which the error occurred, and then executing an alternative section of the task. This will have the same functionality as the section that produced the defect, but using a different algorithm. It is expected that this alternative will not produce the same defect as the previous version, so it will rely on the designer's knowledge of the possible failure modes of this component.</p>
<p>The designer must be clear about the levels of service degradation, taking into account the services and processes that depend on it. Error recovery is part of the Corrective Action and Preventive Action processes (CAPA), which will be worked on in two moments: in this same chapter on fault tolerance when corrective actions are worked on, and in the next chapter, fault prevention, when preventive actions are addressed.</p>
<h3 id="4-failure-treatment-and-continued-service"><a class="header" href="#4-failure-treatment-and-continued-service">4. Failure Treatment and Continued Service</a></h3>
<blockquote>
<p>An error is a manifestation of a defect, and although the error recovery phase may have brought the system to an error-free state, the error can recur. Therefore, the final phase of fault tolerance is to eradicate the failure from the system so that normal service can continue.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fault-prevention-elimination-1"><a class="header" href="#fault-prevention-elimination-1">Fault Prevention: Elimination</a></h1>
<p>The second phase of fault prevention is fault elimination. This phase typically involves procedures to find and eliminate the causes of errors. Although techniques such as code reviews (e.g. linters) and local debugging are used, peer reviews and exhaustive testing with various combinations of input states and environments are not always carried out.</p>
<p>QA testing cannot verify that output values are compatible with the business and its applications, so it usually focuses on time-related failure modes (such as timeouts) and <strong>defects</strong>. Unfortunately, system testing cannot be exhaustive and eliminate all potential faults, mainly due to:</p>
<ul>
<li>Tests are used to demonstrate the presence of faults, not their absence.</li>
<li>The difficulty of performing tests in production. Testing failures in production are akin to <strong>live combat</strong>, meaning the consequences of errors can directly impact the business, leading to potentially poor decisions (for example, an incorrect calculation of a KPI can not only lead to erroneous actions but can also decrease the business's confidence in the data processes). There are process design alternatives for fault detection in production, which I will discuss later.</li>
<li>Errors that were introduced during the system requirements stage may not manifest until the system is operational. For example, a DAG (Directed Acyclic Graph) scheduled to run at a time the data source is not yet available or complete. For this specific example, sensors might be used to only continue the execution when the data source is available, or fail if not available within a specific timeframe (timeout).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="failure-prediction"><a class="header" href="#failure-prediction">Failure Prediction</a></h1>
<p>Accurate and rapid prediction of failures allows those of us maintaining processes to ensure higher service availability. Unfortunately, failure prediction is much more complex than detection.</p>
<p>To predict a failure, it must be identified and classified. Failures must also be predictable, meaning there are system (and component) state changes that lead to failure, or the failure occurs regularly following some pattern. Both cases can be translated into time series prediction problems, and sensor and log data can be used to train prediction models.</p>
<p>The collected data will hardly be ready for use by prediction models, so one or more preprocessing tasks must be carried out:</p>
<ul>
<li><strong>Data synchronization</strong>: metrics collected by various agents must be aligned in time.</li>
<li><strong>Data cleaning</strong>: removal of unnecessary data and generation of missing data (e.g., interpolation).</li>
<li><strong>Data normalization</strong>: metric values are normalized to make magnitudes comparable.</li>
<li><strong>Feature selection</strong>: relevant metrics are identified for use in the models.</li>
</ul>
<p>Once the data is preprocessed, it will be used in two pipelines: a training pipeline and an inference pipeline. The training pipeline uses bulk data to train the model to be made available to the inference pipeline. The inference results will indicate the presence or absence of specific types of failures in the monitored metric sample.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reliability-tools-1"><a class="header" href="#reliability-tools-1">Reliability Tools</a></h1>
<h2 id="data-observability-platforms"><a class="header" href="#data-observability-platforms">Data Observability Platforms</a></h2>
<p>Data observability platforms provide comprehensive monitoring and observability into data pipelines, data quality, and system performance. These platforms can automatically detect and alert on data anomalies, pipeline failures, and performance bottlenecks, enabling quick resolution and ensuring data reliability.</p>
<h2 id="version-control-systems-for-data"><a class="header" href="#version-control-systems-for-data">Version Control Systems for Data</a></h2>
<p>Version control systems designed specifically for data, such as DVC (Data Version Control), enable data engineers to track and manage changes to datasets and machine learning models. This helps in maintaining consistency, reproducibility, and rollback capabilities, enhancing data reliability.</p>
<h2 id="data-lineage-tools"><a class="header" href="#data-lineage-tools">Data Lineage Tools</a></h2>
<p>Data lineage tools track the flow of data through various processes and transformations, providing visibility into the data's origin, what changes were made, and where it's used. This transparency helps in diagnosing and correcting errors, ensuring data reliability and trustworthiness.</p>
<h2 id="automated-data-quality-testing"><a class="header" href="#automated-data-quality-testing">Automated Data Quality Testing</a></h2>
<p>Tools like Great Expectations or Deequ allow data engineers to define and automate data quality checks within data pipelines. By continuously testing data for anomalies, inconsistencies, or deviations from defined quality rules, these tools help maintain high data quality standards.</p>
<h2 id="container-orchestration-systems"><a class="header" href="#container-orchestration-systems">Container Orchestration Systems</a></h2>
<p>Container orchestration systems like Kubernetes can enhance the reliability of data applications and services by managing container deployment, scaling, and failover. This ensures that data services are always available and can dynamically scale based on demand.</p>
<h2 id="infrastructure-as-code-iac-tools"><a class="header" href="#infrastructure-as-code-iac-tools">Infrastructure as Code (IaC) Tools</a></h2>
<p>IaC tools like Terraform allow data engineers to define and manage infrastructure using code, ensuring that data environments are reproducible, consistent, and maintainable. This reduces the risk of environment-related inconsistencies and errors.</p>
<h2 id="feature-flags-and-toggle-management"><a class="header" href="#feature-flags-and-toggle-management">Feature Flags and Toggle Management</a></h2>
<p>Feature flags and toggle management tools enable data engineers to roll out new features and changes in a controlled manner. By gradually introducing changes and monitoring their impact, engineers can ensure system stability and quickly revert changes if issues arise.</p>
<h2 id="chaos-engineering-tools"><a class="header" href="#chaos-engineering-tools">Chaos Engineering Tools</a></h2>
<p>Chaos engineering tools, such as Gremlin or Chaos Mesh, introduce controlled disruptions into data systems (like network latency, server failures, or resource exhaustion) to test and improve their resilience. By proactively identifying and addressing potential points of failure, data systems become more robust and reliable.</p>
<h2 id="workflow-orchestration-tools"><a class="header" href="#workflow-orchestration-tools">Workflow Orchestration Tools</a></h2>
<p>Workflow orchestration tools like Apache Airflow or Prefect help manage complex data pipelines by ensuring tasks are executed in the correct order, managing dependencies, and handling retries and failures gracefully. This improves the reliability of data processing workflows.</p>
<h2 id="apache-airflow"><a class="header" href="#apache-airflow">Apache Airflow</a></h2>
<p>In the context of this chapter on ensuring data reliability, Apache Airflow can be classified as a <strong>Workflow Orchestration Tool</strong>. Airflow is designed to author, schedule, and monitor workflows programmatically. It enables data engineers to define, execute, and manage complex data pipelines, ensuring that data tasks are executed in the correct order, adhering to dependencies and handling retries and failures gracefully. By providing robust scheduling and monitoring capabilities for data workflows, Airflow plays a pivotal role in maintaining the reliability and consistency of data processing operations.</p>
<p>Apache Airflow contributes significantly to data reliability through its robust workflow orchestration capabilities. Here's how Airflow enhances the reliability of data processes:</p>
<h3 id="scheduled-and-automated-workflows"><a class="header" href="#scheduled-and-automated-workflows">Scheduled and Automated Workflows</a></h3>
<p>Airflow allows for the scheduling of complex data workflows, ensuring that data processing tasks are executed at the right time and in the right order. This automation reduces the risk of human error and ensures that critical data processes, such as ETL jobs, data validation, and reporting, are run consistently and reliably.</p>
<h3 id="dependency-management"><a class="header" href="#dependency-management">Dependency Management</a></h3>
<p>Airflow's ability to define dependencies between tasks means that data workflows are executed in a manner that respects the logical sequence of data processing steps. This ensures that upstream failures are appropriately handled before proceeding with downstream tasks, maintaining the integrity and reliability of the data pipeline.</p>
<h3 id="retries-and-failure-handling"><a class="header" href="#retries-and-failure-handling">Retries and Failure Handling</a></h3>
<p>Airflow provides built-in mechanisms for retrying failed tasks and alerting when issues occur. This resilience in the face of failures helps to ensure that temporary issues, such as network outages or transient system failures, do not lead to incomplete or incorrect data processing, thereby enhancing data reliability.</p>
<h3 id="extensive-monitoring-and-logging"><a class="header" href="#extensive-monitoring-and-logging">Extensive Monitoring and Logging</a></h3>
<p>With Airflow's comprehensive monitoring and logging capabilities, data engineers can quickly identify and diagnose issues within their data pipelines. This visibility is crucial for maintaining high data quality and reliability, as it allows for prompt intervention and resolution of issues that could compromise data integrity.</p>
<h3 id="dynamic-pipeline-generation"><a class="header" href="#dynamic-pipeline-generation">Dynamic Pipeline Generation</a></h3>
<p>Airflow supports dynamic pipeline generation, allowing for workflows that adapt to changing data or business requirements. This flexibility ensures that data processes remain relevant and reliable, even as the underlying data or the processing needs evolve.</p>
<h3 id="scalability-1"><a class="header" href="#scalability-1">Scalability</a></h3>
<p>Airflow's architecture supports scaling up to handle large volumes of data and complex workflows. This scalability ensures that as data volumes grow, the data processing pipelines can continue to operate efficiently and reliably without degradation in performance.</p>
<p>By orchestrating data workflows with these capabilities, Airflow plays a critical role in ensuring that data processes are reliable, efficient, and aligned with business needs, making it an essential tool in the data engineer's toolkit for maintaining data reliability.</p>
<h2 id="dbt"><a class="header" href="#dbt">dbt</a></h2>
<p>In the context of this chapter, which discusses various tools and methodologies for ensuring data reliability, dbt (data build tool) can be classified as a <strong>Data Transformation and Testing Tool</strong>. It specializes in managing, testing, and documenting data transformations within modern data warehouses. dbt enables data engineers and analysts to write scalable, maintainable SQL code for transforming raw data into structured and reliable datasets suitable for analysis, thereby playing a crucial role in maintaining and enhancing data reliability.</p>
<p>It plays a significant role in enhancing data reliability within modern data engineering practices. It is a command-line tool that enables data analysts and engineers to transform data in their warehouses more effectively by writing, testing, and deploying SQL queries. Here’s how dbt contributes to data reliability:</p>
<h3 id="version-control-and-collaboration"><a class="header" href="#version-control-and-collaboration">Version Control and Collaboration</a></h3>
<p>dbt encourages the use of version control systems like Git for managing transformation scripts, which enhances collaboration among team members and maintains a historical record of changes. This practice ensures consistency and reliability in data transformations, as changes are tracked, reviewed, and documented.</p>
<h3 id="testing-and-validation"><a class="header" href="#testing-and-validation">Testing and Validation</a></h3>
<p>dbt allows for the implementation of data tests that automatically validate the quality and integrity of the transformed data. These tests can include not-null checks, uniqueness tests, referential integrity checks among tables, and custom business logic validations. By catching issues early in the data transformation stage, dbt helps prevent the propagation of errors downstream, thereby improving the reliability of data used for reporting and analytics.</p>
<h3 id="data-documentation"><a class="header" href="#data-documentation">Data Documentation</a></h3>
<p>With dbt, data documentation is treated as a first-class citizen. dbt generates documentation for the data models, including descriptions of tables and columns and the relationships between different models. This documentation is crucial for understanding the data transformations and ensuring that all stakeholders have a clear and accurate view of the data, its sources, and transformations, which is essential for data reliability.</p>
<h3 id="data-lineage"><a class="header" href="#data-lineage">Data Lineage</a></h3>
<p>dbt provides a visual representation of data lineage, showing how different data models are connected and how data flows through the transformations. This visibility into data lineage helps in understanding the impact of changes, troubleshooting issues, and ensuring that data transformations are reliable and maintain the integrity of the data throughout the pipeline.</p>
<h3 id="incremental-processing"><a class="header" href="#incremental-processing">Incremental Processing</a></h3>
<p>dbt supports incremental data processing, which allows for more efficient data transformations by only processing new or changed data since the last run. This approach reduces the likelihood of processing errors due to handling smaller volumes of data at a time and ensures that the data remains up-to-date and reliable.</p>
<h3 id="modular-and-reusable-code"><a class="header" href="#modular-and-reusable-code">Modular and Reusable Code</a></h3>
<p>dbt promotes writing modular and reusable SQL code, which reduces redundancy and potential for errors in data transformation scripts. By using macros and packages, common logic can be standardized and reused across projects, increasing the reliability of data transformations.</p>
<p>By integrating these features and best practices into the data transformation process, dbt helps ensure that the data is accurate, consistent, and reliable, which is crucial for making informed business decisions and maintaining trust in data systems.</p>
<h2 id="reliability-block-diagrams"><a class="header" href="#reliability-block-diagrams">Reliability Block Diagrams</a></h2>
<blockquote>
<p>Reliability Block Diagrams (RBD) are a method for diagramming and identifying how the reliability of components (or subsystems) <em>R(t)</em> contributes to the success or failure of a redundancy. It is a method that can be used to design and optimize components and select redundancies, aiming to lower failure rates.</p>
</blockquote>
<p>An RBD is represented as a series of connected blocks (in series, parallel, or a combination thereof), indicating redundant components, the type of redundancy, and their respective failure rates.</p>
<p>When analyzing the diagram, components that failed and those that did not fail are indicated. If a path can be found between the start and end of the process with components that did not fail, it can be assumed that the process can be completed.</p>
<p>Each RBD should include statements listing all relationships between components, i.e., what conditions led to the use of one component over another in the process execution.</p>
<p>RBDs can be particularly useful in data engineering to ensure the reliability and availability of data pipelines and storage systems. Here's how RBDs could be applied in the context of data engineering:</p>
<h3 id="designing-data-pipelines"><a class="header" href="#designing-data-pipelines">Designing Data Pipelines</a></h3>
<p>Data pipelines consist of various stages like data collection, processing, transformation, and loading (ETL processes). An RBD can represent each stage as a block, with connections illustrating the flow of data. This helps in identifying critical components whose failure could disrupt the entire pipeline, allowing engineers to implement redundancy or failovers specifically for those components.</p>
<h3 id="infrastructure-reliability"><a class="header" href="#infrastructure-reliability">Infrastructure Reliability</a></h3>
<p>In data engineering, the infrastructure includes databases, servers, network components, and storage systems. An RBD can help visualize the relationship between these components and their impact on overall system reliability. For example, a database cluster might be set up with redundancy to ensure that the failure of a single node doesn't result in data loss or downtime, represented in an RBD by parallel blocks for each redundant component.</p>
<h3 id="dependency-analysis"><a class="header" href="#dependency-analysis">Dependency Analysis</a></h3>
<p>RBDs can help data engineers understand how different data sources and processes depend on each other. For instance, if a data pipeline relies on multiple external APIs or data sources, the RBD can illustrate these dependencies, highlighting potential points of failure if one of the external sources becomes unreliable.</p>
<h3 id="optimizing-redundancies"><a class="header" href="#optimizing-redundancies">Optimizing Redundancies</a></h3>
<p>By using RBDs, data engineers can identify areas where redundancies are necessary to maintain data availability and system performance. This is crucial for critical systems where data must be available at all times. For example, in a data replication strategy, the RBD can help determine the number of replicas needed to achieve the desired level of reliability.</p>
<h3 id="failure-mode-analysis"><a class="header" href="#failure-mode-analysis">Failure Mode Analysis</a></h3>
<p>RBDs allow for the identification of single points of failure within the system. Understanding how individual components contribute to the overall system reliability enables data engineers to prioritize efforts in mitigating risks, such as adding backups, introducing data validation steps, or improving error handling mechanisms.</p>
<h3 id="scalability-and-maintenance-planning"><a class="header" href="#scalability-and-maintenance-planning">Scalability and Maintenance Planning</a></h3>
<p>As data systems scale, RBDs can be updated to reflect new components and dependencies, helping engineers plan for maintenance and scalability while minimizing the impact on reliability. This foresight ensures that the system can grow without compromising on performance or data integrity.</p>
<p>In summary, Reliability Block Diagrams offer a systematic approach for data engineers to design, analyze, and optimize data systems for reliability. By visualizing component dependencies and identifying critical points of failure, RBDs facilitate informed decision-making to enhance system robustness and ensure continuous data availability.</p>
<h2 id="failure-reporting-analysis-and-corrective-action-system-fracas"><a class="header" href="#failure-reporting-analysis-and-corrective-action-system-fracas">Failure Reporting, Analysis, and Corrective Action System (FRACAS)</a></h2>
<blockquote>
<p>FRACAS is a defined system or process for reporting, classifying, and analyzing failures, as well as planning corrective actions for such failures. It is part of the process to keep a history of analyses and actions taken.</p>
</blockquote>
<p>Implementing this process involves automating the analysis of data process logs, commits, pull requests, and tickets.</p>
<p>The FRACAS process is cyclical and follows the adapted FRACAS Kaizen Loop:</p>
<ul>
<li><strong>Failure Mode Analysis</strong>: Analysis of failure modes.</li>
<li><strong>Failure Codes Creation</strong>: Creation of failure codes or the methodology for classifying them.</li>
<li><strong>Work Order History Analysis</strong>: Analysis of the history of tickets sent to the data team.</li>
<li><strong>Root Cause Analysis</strong>: Analysis of root causes.</li>
<li><strong>Strategy Adjustment</strong>: Strategy adjustment.</li>
</ul>
<p>Here's how FRACAS could be applied in the context of data engineering:</p>
<h3 id="failure-reporting"><a class="header" href="#failure-reporting">Failure Reporting</a></h3>
<p>Data engineers and stakeholders report failures or anomalies detected in data processing tasks, data quality issues, or any incidents that affect the expected outcomes of data pipelines. This can be done through automated monitoring tools that alert the team to issues such as failed ETL jobs, discrepancies in data validation checks, or performance bottlenecks.</p>
<h3 id="analysis"><a class="header" href="#analysis">Analysis</a></h3>
<p>Once a failure is reported, it is analyzed to understand its nature, scope, and impact. This involves digging into logs, reviewing the data processing steps where the failure occurred, and identifying the specific point of failure. The analysis aims to classify the failure (e.g., data corruption, process failure, infrastructure issue) and understand the underlying reasons for the failure.</p>
<h3 id="corrective-action"><a class="header" href="#corrective-action">Corrective Action</a></h3>
<p>Based on the analysis, corrective actions are determined and implemented to fix the immediate issue. This could involve rerunning a failed job with corrected parameters, fixing a bug in the data transformation logic, or updating data validation rules to catch similar issues in the future.</p>
<h3 id="system-improvement"><a class="header" href="#system-improvement">System Improvement</a></h3>
<p>Beyond immediate corrective actions, FRACAS also focuses on systemic improvements to prevent similar failures from occurring. This could involve redesigning parts of the data pipeline for greater resilience, adding additional checks and balances in data validation, improving data quality monitoring, or enhancing the infrastructure for better performance and reliability.</p>
<h3 id="documentation-and-learning"><a class="header" href="#documentation-and-learning">Documentation and Learning</a></h3>
<p>All steps of the FRACAS process, from initial failure reporting to final corrective actions and system improvements, are documented. This documentation serves as a knowledge base for the data engineering team, helping them understand common failure modes, effective corrective actions, and best practices for designing more reliable data systems.</p>
<h3 id="continuous-improvement"><a class="header" href="#continuous-improvement">Continuous Improvement</a></h3>
<p>FRACAS is an iterative process. The learnings from each incident are fed back into the data engineering processes, leading to continuous improvement in data pipeline reliability and efficiency. Over time, this reduces the incidence of failures and improves the overall quality and trustworthiness of the data.</p>
<p>By applying FRACAS in data engineering, teams can move from reactive problem-solving to a proactive stance on improving data systems' reliability and efficiency, ultimately supporting better decision-making and operational performance across the organization.</p>
<h2 id="spare-parts-stocking-strategy"><a class="header" href="#spare-parts-stocking-strategy">Spare Parts Stocking Strategy</a></h2>
<blockquote>
<p>Ideally, clean data sources with complex transformations and cleanings, which save time and processing and can be used in multiple stages of multiple processes, will always be available. However, they may temporarily be unavailable or fail. Once such sources are identified and found to be critical to a system or process, it is prudent to have minimal cleaning and transformation tasks that work on raw data or sources of the source. These may not result in final data with the same level of detail but will be good enough.</p>
</blockquote>
<p>These tasks are not designed to be part of the normal process flow but are "spare parts" available for use when maintenance times are too long. The use of such tasks should be for the shortest time possible while the team has time to resolve failures in the original task or design its replacement.</p>
<p>In data engineering, a Spare Parts Stocking Strategy can be metaphorically applied to maintain high availability and reliability of data pipelines and systems. While in traditional contexts, this strategy involves keeping physical spare parts for machinery or equipment, in data engineering, it translates to having backup processes, data sources, and systems in place to ensure continuity in data operations. Here’s how it could be used:</p>
<h3 id="backup-data-processes"><a class="header" href="#backup-data-processes">Backup Data Processes</a></h3>
<p>Just as spare parts can replace failed components in machinery, backup data processes can take over when primary data processes fail. For example, if a primary ETL (Extract, Transform, Load) process fails due to an issue with a data source or transformation logic, a backup ETL process can be initiated. This backup process might use a different data source or a simplified transformation logic to ensure that essential data flows continue, albeit possibly at a reduced fidelity or completeness.</p>
<h3 id="redundant-data-sources"><a class="header" href="#redundant-data-sources">Redundant Data Sources</a></h3>
<p>Having alternate data sources is akin to having spare parts for critical components. If a primary data source becomes unavailable (e.g., due to an API outage or data corruption), the data engineering process can switch to a redundant data source to minimize downtime. This ensures that data pipelines are not entirely dependent on a single source and can continue operating even when one source fails.</p>
<h3 id="pre-processed-data-reservoirs"><a class="header" href="#pre-processed-data-reservoirs">Pre-Processed Data Reservoirs</a></h3>
<p>Maintaining pre-processed versions of critical datasets can be seen as having spare parts ready to be used immediately. In case of a processing failure in real-time data pipelines, these pre-processed datasets can be quickly utilized to ensure continuity in data availability for reporting, analytics, or other downstream processes.</p>
<h3 id="simplified-or-degraded-processing-modes"><a class="header" href="#simplified-or-degraded-processing-modes">Simplified or Degraded Processing Modes</a></h3>
<p>In situations where complex data processing cannot be performed due to system failures, having a simplified or degraded mode of operation can serve as a "spare part." This approach involves having predefined, less resource-intensive processes that can provide essential functionality or data outputs until the primary systems are restored.</p>
<h3 id="automated-failover-mechanisms"><a class="header" href="#automated-failover-mechanisms">Automated Failover Mechanisms</a></h3>
<p>Automated systems that can detect failures and switch to backup processes or systems without manual intervention can be seen as having an automated spare parts deployment system. These mechanisms ensure minimal disruption to data services by quickly responding to failures.</p>
<h3 id="documentation-and-testing"><a class="header" href="#documentation-and-testing">Documentation and Testing</a></h3>
<p>Just as spare parts need to be compatible and tested for specific machinery, backup data processes and sources need to be well-documented and regularly tested to ensure they can effectively replace primary processes when needed. Regular drills or simulations of failures can help ensure that the spare processes are ready to be deployed at a moment's notice.</p>
<p>By adopting a Spare Parts Stocking Strategy in data engineering, organizations can enhance the resilience of their data infrastructure, ensuring that data processing and availability are maintained even in the face of system failures or disruptions. This strategy is crucial for businesses where data availability directly impacts decision-making, operations, and customer satisfaction.</p>
<h2 id="availability-controls"><a class="header" href="#availability-controls">Availability Controls</a></h2>
<blockquote>
<p>Availability failures can occur for numerous reasons (from hardware to bugs), and some systems or processes are significant enough that availability controls should be implemented to ensure that certain services or data remain available when such failures occur.</p>
</blockquote>
<p>Availability controls range from using periodic data backups, snapshots, time travel, redundant processes, backup systems in local or cloud servers, etc.</p>
<p>Availability Controls in data engineering are mechanisms and strategies implemented to ensure that data and data processing capabilities are available when needed, particularly in the face of failures, maintenance, or unexpected demand spikes. These controls are crucial for maintaining the reliability and performance of data systems. Here's how they can be used in data engineering:</p>
<h3 id="data-backups"><a class="header" href="#data-backups">Data Backups</a></h3>
<p>Regular data backups are a fundamental availability control. By maintaining copies of critical datasets, data engineers can ensure that data can be restored in the event of corruption, accidental deletion, or data storage failures. Backups can be scheduled at regular intervals and stored in secure, geographically distributed locations to safeguard against site-specific disasters.</p>
<h3 id="redundant-data-storage"><a class="header" href="#redundant-data-storage">Redundant Data Storage</a></h3>
<p>Using redundant data storage solutions, such as RAID configurations in hardware or distributed file systems in cloud environments, can enhance data availability. These systems store copies of data across multiple disks or nodes, ensuring that the failure of a single component does not result in data loss and that data remains accessible even during partial system outages.</p>
<h3 id="high-availability-architectures"><a class="header" href="#high-availability-architectures">High Availability Architectures</a></h3>
<p>Designing data systems with high availability in mind involves deploying critical components in a redundant manner across multiple servers or clusters. This can include setting up active-active or active-passive configurations for databases, ensuring that if one instance fails, another can immediately take over without disrupting data access.</p>
<h3 id="disaster-recovery-plans"><a class="header" href="#disaster-recovery-plans">Disaster Recovery Plans</a></h3>
<p>Disaster recovery planning involves defining strategies and procedures for recovering from major incidents, such as natural disasters, cyber-attacks, or significant hardware failures. This includes not only data restoration from backups but also the rapid provisioning of replacement computing resources and network infrastructure.</p>
<h3 id="load-balancing-and-scaling"><a class="header" href="#load-balancing-and-scaling">Load Balancing and Scaling</a></h3>
<p>Load balancers distribute incoming data requests across multiple servers or services, preventing any single point from becoming overwhelmed, which could lead to failures and data unavailability. Similarly, implementing auto-scaling for data processing and storage resources can ensure that the system can handle varying loads, maintaining availability during peak demand periods.</p>
<h3 id="data-quality-gates"><a class="header" href="#data-quality-gates">Data Quality Gates</a></h3>
<p>Data quality gates are checkpoints in data pipelines where data is validated against predefined quality criteria. By ensuring that only accurate and complete data moves through the system, these gates help prevent errors and inconsistencies that could lead to processing failures and data unavailability.</p>
<h3 id="monitoring-and-alerting"><a class="header" href="#monitoring-and-alerting">Monitoring and Alerting</a></h3>
<p>Continuous monitoring of data systems and pipelines allows for the early detection of issues that could impact availability. Coupled with an alerting system, monitoring ensures that data engineers can quickly respond to and address potential failures, often before they impact end-users.</p>
<h3 id="versioning-and-data-immutability"><a class="header" href="#versioning-and-data-immutability">Versioning and Data Immutability</a></h3>
<p>Implementing data versioning and immutability can prevent data loss and ensure availability in the face of changes or updates. By keeping immutable historical versions of data, systems can revert to previous states if a new data version causes issues.</p>
<p>By employing these Availability Controls, data engineers can create resilient systems that ensure continuous access to data and data processing capabilities, critical for businesses that rely on timely and reliable data for operational decision-making and customer services.</p>
<h2 id="corrective-actions"><a class="header" href="#corrective-actions">Corrective Actions</a></h2>
<blockquote>
<p>Part of the CAPA (Corrective Action and Preventive Action Process), corrective actions (CAP - Corrective Action Process) consist of detecting failures, determining their root causes, corrective actions, and taking preventive measures to prevent the same failure from occurring for the same reasons. The complete definition is found in ISO 9001.</p>
</blockquote>
<p>Various tools and techniques are used in different industries for their application, including PDCA (Plan, Do, Check, Act), DMAIC (Define, Measure, Analyse, Improve, Control), 8D, etc. Generally, any tool, technique, or methodology is summarized in ISO 9001 in seven "steps":</p>
<ol>
<li><strong>Define the problem</strong>.This involves confirming that the problem is real and identifying the Who, What, When, Where, and Why. In the world of data engineering, this step should be as automated as possible, with the failure detected through sensors.</li>
<li><strong>Define the scope</strong>. It involves measuring the problem to be solved, knowing its frequency, which processes or tasks it affects, and which stakeholders are impacted. For data processes, many scope details should already be known information from the design of the processes and tasks, and the frequency can be determined from observability and FRACAS processes.</li>
<li><strong>Containment actions</strong>. These are specific measures adopted for the shortest possible time while working on a definitive solution to the failure. Such measures should already be designed in advance for each task or sub-task. The selection of measures should be automated; if not, they should be implemented immediately.</li>
<li><strong>Root cause identification</strong>. A clear, precise, and comprehensive diagnosis of the failure. Its documentation is part of the FRACAS.</li>
<li><strong>Corrective action planning</strong>. It involves planning corrective actions specifically based on the root cause.</li>
<li><strong>Implementation of corrective actions</strong>. It involves the final implementation of corrective actions in the process, which should automatically be available when similar failures occur.</li>
<li><strong>Follow-up on results</strong>. Documentation, communication, complete FRACAS.</li>
</ol>
<p>Corrective Actions in data engineering involve identifying, addressing, and mitigating the root causes of identified problems within data processes and systems to prevent their recurrence. This approach is systematic and is crucial for maintaining the integrity, reliability, and efficiency of data operations. Here’s how Corrective Actions can be applied in data engineering:</p>
<h3 id="identification-of-issues"><a class="header" href="#identification-of-issues">Identification of Issues</a></h3>
<p>The first step in the Corrective Action process is the accurate identification of issues within data systems. This could range from data quality problems, data pipeline failures, performance bottlenecks, to security vulnerabilities. Automated monitoring tools, data quality frameworks, and alerting systems play a vital role in early detection.</p>
<h3 id="root-cause-analysis-rca"><a class="header" href="#root-cause-analysis-rca">Root Cause Analysis (RCA)</a></h3>
<p>Once an issue is identified, a thorough Root Cause Analysis is conducted to understand the underlying cause of the problem. Techniques such as the Five Whys, fishbone diagrams, or Pareto analysis can be employed. For instance, if a data pipeline fails frequently due to specific data format inconsistencies, RCA would seek to uncover why these inconsistencies are occurring in the first place.</p>
<h3 id="planning-corrective-actions"><a class="header" href="#planning-corrective-actions">Planning Corrective Actions</a></h3>
<p>Based on the findings from the RCA, a plan for corrective actions is developed. This plan outlines the steps needed to address the root cause of the problem. In the data pipeline example, if the root cause is found to be incorrect data formatting at the source, a corrective action could involve implementing stricter data validation checks at the data ingestion stage.</p>
<h3 id="implementation-of-corrective-actions"><a class="header" href="#implementation-of-corrective-actions">Implementation of Corrective Actions</a></h3>
<p>The planned corrective actions are then implemented. This might involve modifying data validation rules, updating ETL scripts, enhancing data quality checks, or even redesigning parts of the data pipeline for better error handling and resilience.</p>
<h3 id="verification-and-monitoring"><a class="header" href="#verification-and-monitoring">Verification and Monitoring</a></h3>
<p>After the corrective actions are implemented, it’s crucial to verify their effectiveness in resolving the issue and to monitor the system for any unintended consequences. This could involve running test cases, monitoring data pipeline runs for a certain period, or employing data quality dashboards to ensure the issue does not recur.</p>
<h3 id="documentation-and-knowledge-sharing"><a class="header" href="#documentation-and-knowledge-sharing">Documentation and Knowledge Sharing</a></h3>
<p>All steps taken, from issue identification to the implementation of corrective actions and their outcomes, should be thoroughly documented. This documentation serves as a knowledge base for future reference and helps in sharing learnings across the data engineering team and wider organization. It contributes to building a culture of continuous improvement.</p>
<h3 id="preventive-measures"><a class="header" href="#preventive-measures">Preventive Measures</a></h3>
<p>Beyond addressing the immediate issue, the insights gained during the corrective action process can inform preventive measures to avoid similar issues in the future. This might include revising data handling policies, enhancing training for data engineers, or adopting new tools and technologies for better data management.</p>
<p>In data engineering, Corrective Actions are not just about fixing problems but also about improving processes and systems for long-term reliability and efficiency. By systematically addressing the root causes of issues, data teams can enhance the quality, security, and performance of their data infrastructure, supporting better decision-making and operational outcomes across the organization.</p>
<h2 id="antifragility"><a class="header" href="#antifragility">Antifragility</a></h2>
<blockquote>
<p>Inspired by Nassim Nicholas Taleb's book <em>Antifragile: Things That Gain from Disorder</em>, antifragility differs from resilience or robustness concepts, where systems seek to maintain their reliability level. Instead, from their design, systems increase their reliability concerning the system's inputs.</p>
</blockquote>
<p>Antifragility proposes a system design change, which are commonly designed to be fragile, meaning they will fail if operated outside their requirements. Antifragility suggests the opposite, designing systems that improve when exposed to loads outside of the requirements. In this sense, systems are not only designed to respond to the expected or anticipated but interact with their environment in real-time and adapt to it.</p>
<p>Examples of antifragile systems:</p>
<ul>
<li>Self-healing</li>
<li>Real time sensoring, monitoring</li>
<li>Live FRACAS</li>
<li>System Health Management</li>
<li>Automatic Repair</li>
</ul>
<p>Methods such as <strong>Real-Time Anomaly Detection and Adaptation</strong> and <strong>Adaptive Load Balancing</strong> might interest data teams, but they are not covered in this book. Adaptive Load Balancing, in particular, might be a interesting topic for Data Platform or Data DevOps teams.</p>
<h2 id="bulkhead-pattern"><a class="header" href="#bulkhead-pattern">Bulkhead Pattern</a></h2>
<blockquote>
<p>In the nautical world, we find bulkheads, wooden plates found in ships, designed to prevent the ship from sinking when a portion of the hull is compromised. The Bulkhead Pattern adapts exactly this idea, that a failure in one portion of the system should not compromise the entire system.</p>
</blockquote>
<p>This design pattern is commonly applied in software development, consisting of not overloading a service with more calls than it can handle at a given time, an example of this is Netflix's Hystrix system.</p>
<p>In the context of data engineering, the Bulkhead Pattern involves segmenting data processing tasks, resources, and services into isolated units so that a failure in one area does not cascade and disrupt the entire system. Here's how it could be used:</p>
<h3 id="segmenting-data-pipelines"><a class="header" href="#segmenting-data-pipelines">Segmenting Data Pipelines</a></h3>
<p>Data pipelines can be divided into independent segments or modules, each handling a specific part of the data processing workflow. If one segment encounters an issue, such as an unexpected data format or a processing error, it can be addressed or bypassed without halting the entire pipeline. This approach ensures that other data processing activities continue unaffected, maintaining overall system availability and reliability.</p>
<h3 id="isolating-services-and-resources"><a class="header" href="#isolating-services-and-resources">Isolating Services and Resources</a></h3>
<p>In a microservices architecture, each data service (e.g., data ingestion, transformation, and storage services) can be isolated, ensuring that issues in one service don't impact others. Similarly, resources like databases and compute instances can be dedicated to specific tasks or services. If one service or resource fails or becomes overloaded, it won't drag down the others, helping maintain the stability of the broader data platform.</p>
<h3 id="rate-limiting-and-throttling"><a class="header" href="#rate-limiting-and-throttling">Rate Limiting and Throttling</a></h3>
<p>Applying rate limiting to APIs and data ingestion endpoints can prevent any single user or service from consuming too many resources, which could lead to system-wide failures. By throttling the number of requests or the amount of data processed within a given timeframe, the system can remain stable even under high load, protecting against cascading failures.</p>
<h3 id="implementing-circuit-breakers"><a class="header" href="#implementing-circuit-breakers">Implementing Circuit Breakers</a></h3>
<p>Circuit breakers can temporarily halt the flow of data or requests to a service or component when a failure is detected, similar to how a bulkhead would seal off a damaged section of a ship. Once the issue is resolved, or after a certain timeout, the circuit breaker can reset, allowing the normal operation to resume. This prevents repeated failures and gives the system time to recover.</p>
<h3 id="use-of-containers-and-virtualization"><a class="header" href="#use-of-containers-and-virtualization">Use of Containers and Virtualization</a></h3>
<p>Deploying data services and applications in containers or virtualized environments can provide natural isolation, acting as bulkheads. If one containerized component fails, it can be restarted or replaced without affecting others, ensuring that the overall system remains operational.</p>
<p>By employing the Bulkhead Pattern in data engineering, organizations can build more resilient data systems that are capable of withstanding localized issues without widespread impact, ensuring continuous data processing and availability.</p>
<h2 id="cold-standby"><a class="header" href="#cold-standby">Cold Standby</a></h2>
<p>Cold Standby is a redundancy technique used in data reliability engineering and system design to ensure high availability and continuity of service in the event of system failure. Unlike hot standby or warm standby, where backup systems or components are kept running or at a near-ready state, in cold standby, the backup systems are kept fully offline and are <em>only activated when the primary system fails or during maintenance periods</em>. Here’s a deeper look into cold standby:</p>
<ul>
<li><strong>Fully Offline</strong>: The standby system is not running during normal operations; it's fully powered down or in a dormant state.</li>
<li><strong>Manual Activation</strong>: Switching to the cold standby system often requires manual intervention to bring the system online, configure it, and start the services.</li>
<li><strong>Data Synchronization</strong>: Data is not continuously synchronized between the primary and cold standby systems. Instead, data is periodically backed up and would need to be restored on the cold standby system upon activation.</li>
<li><strong>Cost-Effective</strong>: Because the standby system is not running, it doesn't incur costs for power or compute resources during normal operations, making it a cost-effective solution for non-critical applications or where downtime can be tolerated for longer periods.</li>
</ul>
<p>Cold standby systems are typically used in scenarios where high availability is not critically required, or the cost of maintaining a hot or warm standby system cannot be justified. Examples include non-critical batch processing systems, archival systems, or in environments where budget constraints do not allow for more sophisticated redundancy setups.</p>
<p>Implementation considerations:</p>
<ul>
<li><strong>Recovery Time</strong>: The time to recover services using a cold standby can be significant since the system needs to be powered up, configured, and data may need to be restored from backups. This recovery time should be considered in the system's SLA (Service Level Agreement).</li>
<li><strong>Regular Testing</strong>: Regular drills or tests should be conducted to ensure that the cold standby system can be brought online effectively and within the expected time frame.</li>
<li><strong>Data Loss Risk</strong>: Given that data synchronization is not continuous, there is a risk of data loss for transactions or data changes that occurred after the last backup. This risk needs to be assessed and mitigated through frequent backups or other means.</li>
<li><strong>Manual Processes</strong>: The need for manual intervention to activate cold standby systems requires well-documented procedures and trained personnel to ensure a smooth transition during a failure event.</li>
</ul>
<p>Cold Standby is a fundamental concept in designing resilient and reliable systems, especially when balancing the need for availability with cost constraints. It provides a basic level of redundancy that can be suitable for certain applications and scenarios in data reliability engineering.</p>
<h2 id="single-point-of-failure-spof"><a class="header" href="#single-point-of-failure-spof">Single Point of Failure (SPOF)</a></h2>
<p>Eliminating Single Point of Failure (SPOF) is a critical strategy in data reliability engineering aimed at enhancing the resilience and availability of data systems. A Single Point of Failure refers to <em>any component, system, or aspect of the infrastructure whose failure would lead to the failure of the entire system</em>. This could be a database, a network component, a server, or even a piece of software that is critical to data processing or storage.</p>
<p>The goal of eliminating SPOFs is to ensure that no single failure can disrupt the entire service or data flow. This is achieved through redundancy, fault tolerance, and careful system design. Here’s how it relates to data reliability:</p>
<h3 id="redundancy-1"><a class="header" href="#redundancy-1">Redundancy</a></h3>
<p>Introducing redundancy involves duplicating critical components or services so that if one fails, the other can take over without interruption. For example, having multiple data servers, redundant network paths, or replicated databases can prevent downtime caused by the failure of any single component.</p>
<h3 id="fault-tolerance-2"><a class="header" href="#fault-tolerance-2">Fault Tolerance</a></h3>
<p>Building systems to be fault-tolerant means they can continue operating correctly even if some components fail. This might involve implementing software that can reroute data flows away from failed components or hardware that can automatically switch to backup systems.</p>
<h3 id="distributed-architectures"><a class="header" href="#distributed-architectures">Distributed Architectures</a></h3>
<p>Designing systems with distributed architectures can spread out the risk, so no single component's failure can affect the entire system. For example, using cloud services that distribute data and processing across multiple geographical locations can safeguard against regional outages.</p>
<h3 id="regular-testing"><a class="header" href="#regular-testing">Regular Testing</a></h3>
<p>Regularly testing the failover and recovery processes is essential to ensure that redundancy measures work as expected when a real failure occurs. This can include disaster recovery drills and using chaos engineering principles to intentionally introduce failures.</p>
<h3 id="continuous-monitoring-and-alerting"><a class="header" href="#continuous-monitoring-and-alerting">Continuous Monitoring and Alerting</a></h3>
<p>Implementing continuous monitoring and alerting systems helps in the early detection of potential SPOFs before they cause system-wide failures. Monitoring can identify over-utilized resources, impending hardware failures, or software errors that could become SPOFs if not addressed.</p>
<p>By eliminating Single Points of Failure, data engineering teams can create more robust and reliable systems that can withstand individual component failures without significant impact on the overall system performance or data availability. This approach is fundamental to maintaining high levels of service and ensuring that data-driven operations can proceed without interruption.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-quality-1"><a class="header" href="#data-quality-1">Data Quality</a></h1>
<blockquote>
<p>Data quality refers to how well-suited data is for its intended use, focusing on aspects like accuracy, completeness, and consistency. In data reliability engineering, data quality is crucial because it ensures that the data systems an organization relies on are dependable and can support accurate decision-making and efficient operations.</p>
</blockquote>
<p>For those interested in data reliability engineering, understanding data quality is essential. High-quality data leads to reliable systems that businesses can trust for their critical operations and strategic decisions. This chapter will dive into the practical side of maintaining and improving data quality, making it a key skill set for data professionals.</p>
<p>We'll cover important topics like master data management, which helps keep data consistent across the organization, and data governance, ensuring data remains accurate and secure. We'll also look at different data quality models that provide frameworks for assessing and improving data quality. These topics are geared towards giving you actionable insights and tools to enhance the reliability of your data systems.</p>
<p>The goal of this chapter is to bridge the gap between theoretical data quality concepts and their practical application in data reliability engineering, providing actionable insights for improving data systems' robustness and dependability, and to introduce a variety of data quality models, standards, and best practices, enabling data professionals to assess, monitor, and enhance the quality of data within their organizations, thus contributing to overall system reliability.</p>
<p>The topics in this chapter on Data Quality are based on ideas from the book "Calidad de Datos" (Data Quality) by Ismael Caballero Muñoz-Reja and others. The book is published by "Ediciones de la U" and "Ra-Ma". We chose to follow this book's approach to make sure we cover data quality thoroughly and in a way that's useful for Data Reliability Engineering. This way, we're using trusted information from experts to help you understand data quality clearly and systematically.</p>
<p>As a very special note, this chapter mentions a lot the term <strong>Data Reliability</strong>, which is not the same as <strong>Data Reliability Engineering</strong>. Data reliability refers to the trustworthiness and dependability of data, while data reliability engineering is the practice of designing, implementing, and maintaining systems and processes to ensure data remains reliable. Both terms were oversimplified here, but both will be explored further along the book.</p>
<p>This chapter is divided in five parts:</p>
<p><a href="concepts/./data_quality_foundations.html"><strong>Foundations of Data Quality</strong></a></p>
<blockquote>
<p>This section explains how governance, data management, and data quality management differ and work together, highlighting their importance in aligning with ISO/IEC 38500 standards to meet organizational goals and manage data risks efficiently. We'll also explore the concept of data lifecycle.</p>
</blockquote>
<p><a href="concepts/./data_quality_master_data.html"><strong>Master Data</strong></a></p>
<blockquote>
<p>Master data is the core information an organization uses across its systems, and master data management is the process of organizing, securing, and maintaining this information to ensure it's accurate and consistent. This section explores entities resolution, master data architecture, maturity models, and standards.</p>
</blockquote>
<p><a href="concepts/./data_quality_management.html"><strong>Data Management</strong></a></p>
<blockquote>
<p>Here we'll explore various frameworks and models that guide how organizations can systematically improve the handling and quality of their data. Including DAMA DMBOK, Aiken's Model, Data Management Maturity Model (DMM), Gartner's Model, Total Quality Data Management (TQDM), Data Management Capability Assessment Model (DCAM), and the Model for Assessing Data Management (MAMD).</p>
</blockquote>
<p><a href="concepts/./data_quality_models.html"><strong>Data Quality Models</strong></a></p>
<blockquote>
<p>Data Quality Models are fundamental frameworks that define, measure, and evaluate the quality of data within an organization. Here we'll explore various criteria, known as dimensions, that help evaluate and enhance the quality of organizational data.</p>
</blockquote>
<p><a href="concepts/./data_quality_conclusions.html"><strong>Final Thoughts on Data Quality</strong></a></p>
<blockquote>
<p>This section emphasizes that good data quality, covering aspects like accuracy and completeness, is essential for data reliability and underlies trustworthy business decisions, with a focus on proactive measures to ensure data's integrity during integration, influenced by solid data architecture and metadata management.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="foundations-of-data-quality"><a class="header" href="#foundations-of-data-quality">Foundations of Data Quality</a></h1>
<h2 id="data-lifecycle"><a class="header" href="#data-lifecycle">Data Lifecycle</a></h2>
<h3 id="dama"><a class="header" href="#dama">DAMA</a></h3>
<p>The Data Management Association International (DAMA) provides a comprehensive framework for understanding and managing the data lifecycle within organizations. This lifecycle encompasses all stages through which data passes, from its initial creation or capture to its eventual archiving or deletion. DAMA emphasizes the importance of managing each stage with best practices to ensure the overall quality and reliability of data.</p>
<h3 id="posmad-data-flow-model"><a class="header" href="#posmad-data-flow-model">POSMAD Data Flow Model</a></h3>
<p>The POSMAD model, which stands for Plan, Obtain, Store, Maintain, Apply, and Dispose, offers a structured approach to managing the data lifecycle:</p>
<ol>
<li>
<p><strong>Plan</strong>:
Define the objectives and requirements for data collection, including what data is needed, for what purpose, and how it will be managed throughout its lifecycle.</p>
</li>
<li>
<p><strong>Obtain</strong>:
Acquire data from various sources, ensuring that the data collection methods maintain the integrity and quality of the data.</p>
</li>
<li>
<p><strong>Store</strong>:
Securely store the data in a manner that maintains its accuracy, accessibility, and compliance with any regulatory requirements.</p>
</li>
<li>
<p><strong>Maintain</strong>:
Regularly update and cleanse the data to ensure it remains accurate, relevant, and of high quality over time.</p>
</li>
<li>
<p><strong>Apply</strong>:
Utilize the data in analyses, decision-making processes, or operational workflows, applying it in a way that maximizes its value and utility.</p>
</li>
<li>
<p><strong>Dispose</strong>:
When data is no longer needed or has reached the end of its useful life, it should be securely archived or destroyed in accordance with data governance policies and regulatory requirements.</p>
</li>
</ol>
<p>Understanding and managing the data lifecycle is crucial for data teams to ensure that the data they work with is accurate, timely, and relevant. Each stage of the POSMAD model presents opportunities to enhance data quality and mitigate risks associated with data mismanagement. For instance, during the "Maintain" stage, data teams can implement quality checks and balances to correct any inaccuracies, ensuring the data's reliability for downstream applications.</p>
<p>The data lifecycle directly influences the design and structure of an organization's data architecture. Data architecture must accommodate the requirements of each lifecycle stage, providing the necessary infrastructure, tools, and processes to support data collection, storage, maintenance, and usage. For example, the "Store" stage necessitates a robust data storage solution that can handle the volume, velocity, and variety of data, while ensuring its accessibility and security.</p>
<p>The management of the data lifecycle, as outlined by DAMA and the POSMAD model, is inherently tied to data reliability. Each stage of the lifecycle offers a checkpoint for ensuring data quality and integrity, which are foundational to data reliability. By adhering to best practices throughout the data lifecycle, data teams can significantly reduce the risk of data errors, inconsistencies, and losses, thereby enhancing the overall reliability of data systems and the insights derived from them.</p>
<p>In summary, a thorough understanding and management of the data lifecycle, from the perspective of DAMA and the POSMAD model, are essential for maintaining data quality and reliability. It ensures that data remains a valuable asset for the organization, supporting informed decision-making and efficient operations.</p>
<h3 id="cobit"><a class="header" href="#cobit">COBIT</a></h3>
<p>The data lifecycle according to COBIT (Control Objectives for Information and Related Technologies) framework involves a structured approach to managing and governing information and technology in an enterprise. COBIT's perspective on the data lifecycle focuses on governance and management practices that ensure data integrity, security, and availability throughout its lifecycle stages. While COBIT does not explicitly define a "data lifecycle" in the same way as DAMA's POSMAD model, its principles and processes can be applied across various stages of data management to enhance data quality and reliability.</p>
<p>Data Lifecycle Stages in the Context of COBIT:</p>
<ol>
<li>
<p><strong>Identification and Classification</strong>:
In this initial stage, data is identified, classified, and categorized based on its importance, sensitivity, and relevance to the business objectives. COBIT emphasizes the need for clear governance structures and policies to manage data effectively from the outset.</p>
</li>
<li>
<p><strong>Acquisition and Creation</strong>:
Data acquisition and creation involve collecting data from various sources and generating new data. COBIT recommends implementing strong control measures and practices to ensure the accuracy, completeness, and reliability of the collected and created data.</p>
</li>
<li>
<p><strong>Storage and Organization</strong>:
Once data is acquired, it needs to be stored securely and organized efficiently. COBIT suggests designing and maintaining data storage solutions that ensure data integrity, confidentiality, and availability, aligning with the enterprise's information security policies.</p>
</li>
<li>
<p><strong>Usage and Processing</strong>:
Data is then used and processed for various business operations, decision-making, and analytics. COBIT advocates for robust IT processes and controls to manage data access, processing, and usage, ensuring that data is utilized effectively and responsibly within the organization.</p>
</li>
<li>
<p><strong>Maintenance and Quality Assurance</strong>:
Regular maintenance, including data cleansing, deduplication, and quality checks, is vital to preserve data quality. COBIT stresses continuous improvement and quality assurance practices to ensure that data remains accurate, relevant, and reliable over time.</p>
</li>
<li>
<p><strong>Archiving and Retention</strong>:
Data that is no longer actively used but needs to be retained for legal, regulatory, or historical reasons is archived. COBIT recommends establishing clear data retention policies and secure archiving solutions that comply with legal and regulatory requirements.</p>
</li>
<li>
<p><strong>Disposal and Destruction</strong>:
Finally, data that is no longer needed or has surpassed its retention period should be securely disposed of or destroyed. COBIT emphasizes the importance of secure data disposal practices to protect sensitive information and ensure compliance with data protection regulations.</p>
</li>
</ol>
<p>For data teams, applying COBIT's governance and management frameworks to the data lifecycle ensures that data handling practices are aligned with broader enterprise governance objectives, enhancing data security, quality, and reliability. By adopting COBIT's principles, data teams can implement structured, standardized processes for managing data, reducing risks, and ensuring that data remains a reliable asset for informed decision-making.</p>
<p>In summary, COBIT's approach to the data lifecycle underscores the importance of governance, risk management, and compliance practices in every stage of data management. By integrating these practices, organizations can enhance the reliability and value of their data, supporting strategic objectives and operational efficiency.</p>
<h2 id="governance-vs-data-management-vs-data-quality-management"><a class="header" href="#governance-vs-data-management-vs-data-quality-management">Governance vs. Data Management vs. Data Quality Management</a></h2>
<p>Understanding the distinctions between governance, data management, and data quality management is crucial for data teams to effectively organize their roles, responsibilities, and processes. Aligning these activities with the ISO/IEC 38500 standards can further ensure that data practices contribute positively to the organization's strategic objectives, manage risks associated with IT and data, and optimize the performance of data and IT resources.</p>
<p>By integrating these frameworks, organizations can create a cohesive and efficient approach to data handling that not only ensures high data quality but also aligns with broader governance goals and compliance requirements, thereby enhancing overall data reliability.</p>
<h3 id="governance"><a class="header" href="#governance">Governance</a></h3>
<p>Data Governance refers to the overarching framework or system of decision rights and accountabilities regarding data and information assets within an organization. It involves setting policies, standards, and principles for data usage, security, and compliance, ensuring that data across the organization is managed as a valuable resource. Governance encompasses the strategies and policies that dictate how data is acquired, stored, accessed, and used, ensuring alignment with business objectives and regulatory requirements.</p>
<h3 id="data-management"><a class="header" href="#data-management">Data Management</a></h3>
<p>Data Management is the implementation of architectures, policies, practices, and procedures that manage the information lifecycle needs of an enterprise. It's more tactical and operational compared to governance and involves the day-to-day activities and technical aspects of handling data, including data architecture, modeling, storage, security, and integration. Data management ensures that data is available, reliable, consistent, and accessible to meet the needs of the organization.</p>
<h3 id="data-quality-management"><a class="header" href="#data-quality-management">Data Quality Management</a></h3>
<p>Data Quality Management (DQM) is a subset of data management focused specifically on maintaining high-quality data throughout the data lifecycle. It involves the processes, methodologies, and systems used to measure, monitor, and improve the quality of data. DQM covers various dimensions of data quality such as accuracy, completeness, consistency, reliability, and timeliness. It includes activities like data profiling, cleansing, validation, and enrichment to ensure that data meets the quality standards set by the organization.</p>
<h3 id="isoiec-38500-family"><a class="header" href="#isoiec-38500-family">ISO/IEC 38500 Family</a></h3>
<p>The ISO/IEC 38500 family provides standards for corporate governance of information technology (IT). It offers guidance to those advising, informing, or assisting directors on the effective and acceptable use of IT within the organization. The ISO/IEC 38500 standards are designed to help organizations ensure that their IT investments are aligned with their business objectives, that IT risks are managed appropriately, and that the organization realizes the full potential of its IT resources.</p>
<p>Key Principles of ISO/IEC 38500:</p>
<ul>
<li><strong>Responsibility</strong>: Everyone in the organization has some responsibility for IT, from top-level executives to end-users.</li>
<li><strong>Strategy</strong>: IT strategy should align with the organization's overall business strategy, supporting its goals and objectives.</li>
<li><strong>Acquisition</strong>: IT acquisitions should be made for valid reasons, with clear and transparent decision-making processes.</li>
<li><strong>Performance</strong>: IT should be used efficiently to deliver value to the organization, with its performance regularly monitored and evaluated.</li>
<li><strong>Conformance</strong>: IT usage should comply with all relevant laws, regulations, and internal policies.</li>
<li><strong>Human Behavior</strong>: IT policies and practices should respect the needs and rights of all stakeholders, including employees, customers, and partners.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="master-data"><a class="header" href="#master-data">Master Data</a></h1>
<p>Master Data refers to the core data within an organization that is essential for its operations and decision-making processes. This data is non-transactional and represents the business's key entities such as customers, products, employees, suppliers, and more. Master data is characterized by its stability and consistency across the organization and is used across various systems, applications, and processes.</p>
<p>Master data is critical because it provides a common point of reference for the organization, ensuring that everyone is working with the same information. Consistency in master data across different business units and systems reduces ambiguity and errors, leading to more accurate analytics, reporting, and business intelligence.</p>
<h2 id="master-data-management-mdm"><a class="header" href="#master-data-management-mdm">Master Data Management (MDM)</a></h2>
<p>Master Data Management (MDM) is a comprehensive method of defining, managing, and controlling master data entities, processes, policies, and governance to ensure that master data is consistent, accurate, and available throughout the organization. MDM involves the integration, cleansing, enrichment, and maintenance of master data across various systems and platforms within the enterprise.</p>
<p>Key Components of MDM:</p>
<ul>
<li><strong>Data Governance</strong>: Establishing policies, standards, and procedures for managing master data, including data ownership, data quality standards, and data security.</li>
<li><strong>Data Stewardship</strong>: Assigning responsibility for managing, maintaining, and ensuring the quality of master data to specific roles within the organization.</li>
<li><strong>Data Integration</strong>: Aggregating and consolidating master data from disparate sources to create a single source of truth.</li>
<li><strong>Data Quality Management</strong>: Implementing processes and tools to ensure the accuracy, completeness, consistency, and timeliness of master data.</li>
<li><strong>Data Enrichment</strong>: Enhancing master data with additional attributes or corrections to increase its value to the organization.</li>
</ul>
<h2 id="resolving-entities"><a class="header" href="#resolving-entities">Resolving Entities</a></h2>
<p>Resolving entities in the context of Master Data and Master Data Management (MDM) is crucial for ensuring consistency, accuracy, and a single source of truth for core business entities such as customers, products, employees, suppliers, etc. Entity resolution involves identifying, linking, and merging records that refer to the same real-world entities across different systems and datasets.</p>
<p>Here's how entity resolution can be approached:</p>
<ol>
<li>
<p><strong>Identification</strong>:
The first step involves identifying potential matches among entities across different systems or datasets. This can be challenging due to variations in data entry, abbreviations, misspellings, or incomplete records. Techniques Used: Pattern matching, fuzzy matching, and using algorithms that can handle variations and typos.</p>
</li>
<li>
<p><strong>Deduplication</strong>:
Deduplication involves removing duplicate records of the same entity within a single dataset or system. This step is crucial to prevent redundancy and ensure each entity is represented once. Techniques Used: Hashing, similarity scoring, and machine learning models to recognize duplicates even when data is not identical.</p>
</li>
<li>
<p><strong>Linking</strong>:
Linking is the process of associating related records across different datasets or systems that refer to the same real-world entity. This step creates a unified view of each entity. Techniques Used: Record linkage techniques, probabilistic matching, and reference matching where a common identifier or set of identifiers is used to link records.</p>
</li>
<li>
<p><strong>Merging</strong>:
Merging involves consolidating linked records into a single, comprehensive record that provides a complete view of the entity. Decisions must be made about which data elements to retain, merge, or discard. Techniques Used: Survivorship rules that define which attributes to keep (e.g., most recent, most complete, source-specific priorities).</p>
</li>
<li>
<p><strong>Data Enrichment</strong>:
After resolving and merging entities, data enrichment can be applied to enhance the master records with additional information from external sources, improving the depth and value of the master data. Techniques Used: Integrating third-party data, leveraging public datasets, and using APIs to fetch additional information.</p>
</li>
<li>
<p><strong>Continuous Monitoring and Updating</strong>:
Entity resolution is not a one-time task. Continuous monitoring and updating are necessary to accommodate new data, changes to existing entities, and evolving relationships among entities. Techniques Used: Implementing feedback loops, periodic reviews, and automated monitoring systems to identify and resolve new or changed entities.</p>
</li>
</ol>
<h2 id="master-data-architecture"><a class="header" href="#master-data-architecture">Master Data Architecture</a></h2>
<p>Master Data Architecture refers to the framework and models used to manage and organize an organization's master data, which typically includes core business entities like customers, products, employees, and suppliers. The architecture aims to ensure that master data is consistent, accurate, and available across the enterprise.</p>
<p>Key Components:</p>
<ul>
<li><strong>Master Data Hub</strong>: A central repository where master data is consolidated, managed, and maintained. It ensures a single source of truth for master data entities across the organization.</li>
<li><strong>Data Integration Layer</strong>: Mechanisms for extracting, transforming, and loading (ETL) data from various source systems into the master data hub. This layer handles data cleansing, deduplication, and standardization.</li>
<li><strong>Data Governance Framework</strong>: Policies, standards, and procedures that govern how master data is collected, maintained, and utilized, ensuring data quality and compliance.</li>
<li><strong>Data Quality Services</strong>: Tools and processes for continuously monitoring and improving the quality of master data, including validation, enrichment, and error correction.</li>
<li><strong>Application Interfaces</strong>: APIs and services that enable other systems and applications within the organization to access and interact with the master data.</li>
</ul>
<h3 id="4-variants-of-master-data-architecture"><a class="header" href="#4-variants-of-master-data-architecture">4 Variants of Master Data Architecture</a></h3>
<p>Jochen and Weisbecker (2014) proposed four variants of master data architecture to address different organizational needs and data management strategies. Each variant offers a unique approach to handling master data, considering factors like centralization, data governance, and system integration. Here's a summary of each:</p>
<ol>
<li><strong>Centralized Master Data Management</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: This architecture involves a single, centralized repository where all master data is stored and managed. It serves as the authoritative source for all master data across the organization.</li>
<li><strong>Advantages</strong>: Ensures consistency and uniformity of master data across the enterprise, simplifies governance, and reduces data redundancy.</li>
<li><strong>Challenges</strong>: Requires significant investment in a centralized system, can lead to bottlenecks, and may be less responsive to local or departmental needs.</li>
</ul>
<ol start="2">
<li><strong>Decentralized Master Data Management</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: In this variant, master data is managed locally within different departments or business units without a central repository. Each unit maintains its own master data.</li>
<li><strong>Advantages</strong>: Offers flexibility and allows departments to manage data according to their specific needs and processes, enabling quicker responses to local requirements.</li>
<li><strong>Challenges</strong>: Increases the risk of data inconsistencies across the organization, complicates data integration efforts, and makes enterprise-wide data governance more challenging.</li>
</ul>
<ol start="3">
<li><strong>Registry Model</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: The registry model uses a centralized registry that stores references (links or keys) to master data but not the master data itself. The actual data remains in local systems.</li>
<li><strong>Advantages</strong>: Provides a unified view of where master data is located across the organization without centralizing the data itself, facilitating data integration and consistency checks.</li>
<li><strong>Challenges</strong>: Does not eliminate data redundancies and may require complex synchronization mechanisms to ensure data consistency across systems.</li>
</ul>
<ol start="4">
<li><strong>Hub and Spoke Model</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: This architecture features a central hub where master data is consolidated, synchronized, and distributed to various "spoke" systems throughout the organization.</li>
<li><strong>Advantages</strong>: Balances centralization and decentralization by allowing data to be managed centrally while also supporting local system requirements. It facilitates data sharing and consistency.</li>
<li><strong>Challenges</strong>: Can be complex to implement and maintain, requiring robust integration and data synchronization capabilities between the hub and spoke systems.</li>
</ul>
<p>Each of these master data architecture variants offers distinct benefits and poses unique challenges, making them suitable for different organizational contexts and data management objectives. The choice among these variants depends on factors such as the organization's size, complexity, data governance maturity, and specific business needs.</p>
<h3 id="information-architecture-principles"><a class="header" href="#information-architecture-principles">Information Architecture Principles</a></h3>
<p>Information Architecture (IA) principles guide the design and organization of information to make it accessible and usable. In the context of master data management, these principles help ensure that master data is effectively organized and can support business needs.</p>
<p>Key Principles:</p>
<ul>
<li><strong>Clarity and Understandability</strong>: Information should be presented in a clear and understandable manner, with consistent terminology and categorization that aligns with business operations.</li>
<li><strong>Accessibility</strong>: Master data should be easily accessible to authorized users and systems, with appropriate interfaces and query capabilities.</li>
<li><strong>Scalability</strong>: The architecture should be able to accommodate growth in data volume, variety, and usage, ensuring that it can support future business requirements.</li>
<li><strong>Flexibility</strong>: The architecture should be flexible enough to adapt to changes in business processes, data models, and technology landscapes.</li>
<li><strong>Security and Privacy</strong>: Ensuring that master data is protected from unauthorized access and breaches, and that it complies with data protection regulations.</li>
<li><strong>Integration</strong>: The architecture should facilitate the integration of master data with other business processes and systems, ensuring seamless data flow and interoperability.</li>
<li><strong>Data Quality Focus</strong>: A continual emphasis on maintaining and improving the quality of master data through validation, cleansing, and governance practices.</li>
</ul>
<h2 id="master-data-management-maturity-models"><a class="header" href="#master-data-management-maturity-models">Master Data Management Maturity Models</a></h2>
<p>Master Data Management (MDM) maturity models are frameworks that help organizations assess their current state of MDM practices and identify areas for improvement to achieve more effective management of their master data.</p>
<p>MDM maturity models typically outline a series of stages or levels through which an organization progresses as it improves its master data management capabilities. These models often start with an initial stage characterized by ad-hoc and uncoordinated master data efforts and progress through more sophisticated stages involving standardized processes, integrated systems, and eventually, optimized and business-aligned MDM practices.</p>
<p>The levels in an MDM maturity model might include:</p>
<ul>
<li><strong>Initial/Ad-Hoc</strong>: Master data is managed in an uncoordinated way, often within siloed departments.</li>
<li><strong>Repeatable</strong>: Some processes are defined, and there might be local consistency within departments, but efforts are not yet standardized across the organization.</li>
<li><strong>Defined</strong>: Organization-wide standards and policies for MDM are established, leading to greater consistency and control.</li>
<li><strong>Managed</strong>: MDM processes are monitored and measured, and data quality is actively managed across the enterprise.</li>
<li><strong>Optimized</strong>: Continuous improvement processes are in place, and MDM is fully aligned with business strategy, driving value and innovation.</li>
</ul>
<h3 id="loshins-mdm-maturity-model"><a class="header" href="#loshins-mdm-maturity-model">Loshin's MDM Maturity Model</a></h3>
<p>David Loshin's MDM maturity model is particularly insightful because it not only outlines stages of maturity but also focuses on the alignment of MDM processes with business objectives, emphasizing the strategic role of master data in driving business success.</p>
<p>Loshin's model includes the following key stages:</p>
<ul>
<li><strong>Awareness</strong>: The organization recognizes the importance of master data but lacks formal management practices.</li>
<li><strong>Concept/Definition</strong>: Initial efforts to define master data and understand its impact on business processes are undertaken.</li>
<li><strong>Construction and Integration</strong>: Systems and processes are developed for managing master data, with a focus on integrating MDM into existing IT infrastructure.</li>
<li><strong>Operationalization</strong>: MDM processes are put into operation, and the organization starts to see benefits in terms of data consistency and quality.</li>
<li><strong>Governance</strong>: Formal governance structures are established to ensure ongoing data quality, compliance, and alignment with business objectives.</li>
<li><strong>Optimization</strong>: The organization continuously improves its MDM practices, leveraging master data as a strategic asset to drive business innovation and value.</li>
</ul>
<p>Loshin emphasizes the importance of not just the technical aspects of MDM but also the governance, organizational, and strategic components. The model encourages organizations to progress from merely managing data to leveraging it as a key factor in strategic decision-making and business processes optimization.</p>
<h2 id="iso-8000"><a class="header" href="#iso-8000">ISO 8000</a></h2>
<p>The ISO 8000 standard series is focused on data quality and master data management, providing guidelines and best practices to ensure that data is accurate, complete, and fit for use in various business contexts. This series covers a wide range of topics related to data quality, from terminology and principles to data provenance and master data exchange.</p>
<p>Let's explore some of the key parts of the ISO 8000 series relevant to Master Data and Data Quality:</p>
<h3 id="iso-8000-100-data-quality-management-principles"><a class="header" href="#iso-8000-100-data-quality-management-principles">ISO 8000-100: Data Quality Management Principles</a></h3>
<blockquote>
<p>This part of the ISO 8000 series outlines the foundational principles for managing data quality, establishing a framework for assessing, improving, and maintaining the quality of data within an organization.</p>
</blockquote>
<h3 id="iso-8000-102-data-quality-provenance"><a class="header" href="#iso-8000-102-data-quality-provenance">ISO 8000-102: Data Quality Provenance</a></h3>
<blockquote>
<p>Focuses on the provenance of data, detailing how to document the source of data and its lineage. This is crucial for understanding the origins of data, assessing its reliability, and ensuring traceability.</p>
</blockquote>
<h3 id="iso-8000-110-syntax-and-semantic-encoding"><a class="header" href="#iso-8000-110-syntax-and-semantic-encoding">ISO 8000-110: Syntax and Semantic Encoding</a></h3>
<blockquote>
<p>Addresses the importance of using standardized syntax and semantics to ensure that data is consistently understood and interpreted across different systems and stakeholders.</p>
</blockquote>
<h3 id="iso-8000-115-master-data-exchange-of-characteristic-data"><a class="header" href="#iso-8000-115-master-data-exchange-of-characteristic-data">ISO 8000-115: Master Data: Exchange of characteristic data</a></h3>
<blockquote>
<p>Provides guidelines for the exchange of master data, particularly focusing on the characteristics of products and services. It emphasizes the standardization of data formats to facilitate accurate and efficient data exchange.</p>
</blockquote>
<h3 id="iso-8000-116-data-quality-information-and-data-quality-vocabulary"><a class="header" href="#iso-8000-116-data-quality-information-and-data-quality-vocabulary">ISO 8000-116: Data Quality: Information and Data Quality Vocabulary</a></h3>
<blockquote>
<p>Defines a set of terms and definitions related to data and information quality, helping organizations to establish a common understanding of key concepts in data quality management.</p>
</blockquote>
<h3 id="iso-8000-120-master-data-quality-prerequisites-for-data-quality"><a class="header" href="#iso-8000-120-master-data-quality-prerequisites-for-data-quality">ISO 8000-120: Master Data Quality: Prerequisites for data quality</a></h3>
<blockquote>
<p>Discusses the prerequisites for achieving high-quality master data, including the establishment of data governance, data quality metrics, and continuous monitoring processes.</p>
</blockquote>
<h3 id="iso-8000-130-data-quality-management-process-reference-model"><a class="header" href="#iso-8000-130-data-quality-management-process-reference-model">ISO 8000-130: Data Quality Management: Process reference model</a></h3>
<blockquote>
<p>Introduces a process reference model for data quality management, outlining the key processes involved in establishing, implementing, maintaining, and improving data quality within an organization.</p>
</blockquote>
<h3 id="iso-8000-140-data-quality-management-assessment-and-measurement"><a class="header" href="#iso-8000-140-data-quality-management-assessment-and-measurement">ISO 8000-140: Data Quality Management: Assessment and measurement</a></h3>
<blockquote>
<p>Focuses on the assessment and measurement of data quality, providing methodologies for evaluating data quality against defined criteria and metrics.</p>
</blockquote>
<h3 id="iso-8000-150-master-data-quality-master-data-quality-assessment-framework"><a class="header" href="#iso-8000-150-master-data-quality-master-data-quality-assessment-framework">ISO 8000-150: Master Data Quality: Master data quality assessment framework</a></h3>
<blockquote>
<p>Offers a comprehensive framework for assessing the quality of master data, including methodologies for evaluating data against specific quality dimensions such as accuracy, completeness, and consistency.</p>
</blockquote>
<h2 id="isoiec-22745"><a class="header" href="#isoiec-22745">ISO/IEC 22745</a></h2>
<p>The ISO/IEC 22745 standard, titled "Industrial automation systems and integration — Open technical dictionaries and their application to master data," is a series of international standards developed to facilitate the exchange and understanding of product data. This standard is particularly significant in the context of industrial automation and integration, providing a framework for creating, managing, and deploying open technical dictionaries. These dictionaries ensure that product data is consistent, interoperable, and can be seamlessly exchanged between different systems and organizations, enhancing data quality and reliability across the supply chain.</p>
<p>ISO/IEC 22745 is crucial for organizations involved in manufacturing, supply chain management, and industrial automation because it standardizes the way product and service data is described, categorized, and exchanged. This standardization supports more efficient procurement processes, reduces the risk of misinterpretation of product data, and enhances interoperability between different IT systems and platforms. By implementing ISO/IEC 22745, organizations can improve the accuracy and reliability of their master data, leading to better decision-making and operational efficiencies.</p>
<h3 id="part-1-overview-and-fundamental-principles"><a class="header" href="#part-1-overview-and-fundamental-principles">Part 1: Overview and Fundamental Principles</a></h3>
<blockquote>
<p>Provides a general introduction to the standard, outlining its scope, objectives, and fundamental principles. It sets the foundation for the development and use of open technical dictionaries.</p>
</blockquote>
<h3 id="part-2-vocabulary"><a class="header" href="#part-2-vocabulary">Part 2: Vocabulary</a></h3>
<blockquote>
<p>Establishes the terms and definitions used throughout the ISO/IEC 22745 series, ensuring a common understanding of key concepts related to open technical dictionaries and master data exchange.</p>
</blockquote>
<h3 id="part-10-exchange-of-characteristic-data-syntax-and-semantic-encoding-rules"><a class="header" href="#part-10-exchange-of-characteristic-data-syntax-and-semantic-encoding-rules">Part 10: Exchange of characteristic data: Syntax and semantic encoding rules</a></h3>
<blockquote>
<p>Specifies the syntax and semantic encoding rules for exchanging characteristic data, ensuring that data exchanged between systems maintains its meaning and integrity.</p>
</blockquote>
<h3 id="part-11-methodology-for-the-development-and-validation-of-open-technical-dictionaries"><a class="header" href="#part-11-methodology-for-the-development-and-validation-of-open-technical-dictionaries">Part 11: Methodology for the development and validation of open technical dictionaries</a></h3>
<blockquote>
<p>Details the methodology for developing and validating open technical dictionaries, including processes for creating, approving, and maintaining dictionary entries.</p>
</blockquote>
<h3 id="part-13-identification-and-referencing-of-requirements-of-product-data"><a class="header" href="#part-13-identification-and-referencing-of-requirements-of-product-data">Part 13: Identification and referencing of requirements of product data</a></h3>
<blockquote>
<p>Focuses on the identification and referencing of product data requirements, providing guidelines for documenting and referencing product specifications and standards.</p>
</blockquote>
<h3 id="part-14-guidelines-for-the-formulation-of-requests-for-master-data"><a class="header" href="#part-14-guidelines-for-the-formulation-of-requests-for-master-data">Part 14: Guidelines for the formulation of requests for master data</a></h3>
<blockquote>
<p>Provides guidelines for formulating requests for master data, ensuring that data requests are clear, structured, and capable of being fulfilled accurately.</p>
</blockquote>
<h3 id="part-20-presentation-of-characteristic-data"><a class="header" href="#part-20-presentation-of-characteristic-data">Part 20: Presentation of characteristic data</a></h3>
<blockquote>
<p>Addresses the presentation of characteristic data, outlining how data should be formatted and displayed to ensure clarity and usability.</p>
</blockquote>
<h3 id="part-30-registration-and-publication-of-open-technical-dictionaries"><a class="header" href="#part-30-registration-and-publication-of-open-technical-dictionaries">Part 30: Registration and publication of open technical dictionaries</a></h3>
<blockquote>
<p>Covers the registration and publication processes for open technical dictionaries, ensuring that dictionaries are accessible, authoritative, and maintained over time.</p>
</blockquote>
<h3 id="part-35-identification-and-referencing-of-terminology"><a class="header" href="#part-35-identification-and-referencing-of-terminology">Part 35: Identification and referencing of terminology</a></h3>
<blockquote>
<p>Discusses the identification and referencing of terminology within open technical dictionaries, ensuring consistent use of terms and definitions.</p>
</blockquote>
<h3 id="part-40-master-data-repository"><a class="header" href="#part-40-master-data-repository">Part 40: Master data repository</a></h3>
<blockquote>
<p>Describes the requirements and structure of a master data repository, a centralized system for storing and managing master data in accordance with the principles of ISO/IEC 22745.</p>
</blockquote>
<h2 id="mdm-tools-implementation-considerations"><a class="header" href="#mdm-tools-implementation-considerations">MDM Tools Implementation Considerations</a></h2>
<p>There are several MDM tools available, including SAP Master Data Governance (MDG), Informatica MDM, IBM InfoSphere MDM, Microsoft SQL Server Master Data Services (MDS), Oracle MDM, Talend MDM, ECCMA, PILOG, TIBCO MDM, Ataccama MDC, VisionWare Multivue MDM, and many others.</p>
<p>When implementing these master data tools, companies typically go through a series of steps including:</p>
<ul>
<li><strong>Assessment</strong>: Evaluating the current state of master data, identifying key data domains, and understanding the data lifecycle.</li>
<li><strong>Strategy Development</strong>: Defining objectives, governance structures, and key performance indicators (KPIs) for the MDM initiative.</li>
<li><strong>Tool Selection</strong>: Choosing an MDM tool that aligns with the company's IT infrastructure, data domains, and business objectives.</li>
<li><strong>Integration</strong>: Integrating the MDM tool with existing systems and data sources to ensure seamless data flow and synchronization.</li>
<li><strong>Data Cleansing and Migration</strong>: Cleaning existing data to remove duplicates and inconsistencies before migrating it into the MDM system.</li>
<li><strong>Governance and Maintenance</strong>: Establishing ongoing data governance practices to maintain data quality, including monitoring, auditing, and updating data as needed.</li>
</ul>
<p>Master data tools are essential for organizations to maintain a "<strong>single source of truth</strong>" for their critical business entities, enabling more informed decision-making, improved customer experiences, and streamlined operations.</p>
<h2 id="using-a-commercial-mdm-tool-vs-building-an-in-house-mdm-service"><a class="header" href="#using-a-commercial-mdm-tool-vs-building-an-in-house-mdm-service">Using a Commercial MDM Tool vs. Building an In-House MDM Service</a></h2>
<p>Deciding between using a commercial Master Data Management (MDM) tool and building an in-house MDM service involves weighing various factors, including cost, scalability, customization, and maintenance. Each approach has its unique set of challenges, advantages, and disadvantages.</p>
<h3 id="using-a-commercial-mdm-tool"><a class="header" href="#using-a-commercial-mdm-tool">Using a Commercial MDM Tool</a></h3>
<p><strong>Pros</strong>:</p>
<ul>
<li><strong>Speed of Deployment</strong>: Commercial MDM tools offer out-of-the-box solutions that can be quickly deployed, allowing organizations to benefit from improved data management in a shorter timeframe.</li>
<li><strong>Proven Reliability</strong>: These tools are developed by experienced vendors, tested across diverse industries and scenarios, ensuring a level of reliability and robustness.</li>
<li><strong>Support and Updates</strong>: Vendors provide ongoing support, regular updates, and enhancements, which helps in keeping the MDM system current with the latest data management trends and technologies.</li>
<li><strong>Built-in Best Practices</strong>: Commercial tools often incorporate industry best practices in data governance, data quality, and data integration, reducing the learning curve and implementation risk.</li>
<li><strong>Scalability</strong>: Most commercial MDM solutions are designed to scale with the growth of the business, accommodating increasing data volumes and complexity without significant rework.</li>
</ul>
<p><strong>Cons</strong>:</p>
<ul>
<li><strong>Cost</strong>: Licensing fees for commercial MDM tools can be substantial, especially for large enterprises or when scaling up, and there might be additional costs for support and customization.</li>
<li><strong>Limited Customization</strong>: While these tools offer configuration options, there may be limitations to how much they can be tailored to meet unique business requirements.</li>
<li><strong>Vendor Lock-in</strong>: Relying on a vendor's tool can lead to dependency, making it challenging to switch solutions or integrate with non-supported platforms and data sources in the future.</li>
</ul>
<p><strong>Challenges</strong>:</p>
<ul>
<li>Navigating complex licensing structures and ensuring the tool fits within the budget constraints.</li>
<li>Integrating the MDM tool with legacy systems and diverse data sources.</li>
</ul>
<h3 id="building-an-in-house-mdm-service"><a class="header" href="#building-an-in-house-mdm-service">Building an In-House MDM Service</a></h3>
<p><strong>Pros</strong>:</p>
<ul>
<li><strong>Customization</strong>: Building an MDM service in-house allows for complete customization to the specific needs, processes, and data models of the organization.</li>
<li><strong>Integration</strong>: An in-house solution can be designed to integrate seamlessly with existing systems and data sources, providing a more cohesive data ecosystem.</li>
<li><strong>Control</strong>: Organizations maintain full control over the development, maintenance, and evolution of the MDM service, making it easier to adapt to changing business needs.</li>
</ul>
<p><strong>Cons</strong>:</p>
<ul>
<li><strong>Resource Intensive</strong>: Developing an MDM service requires significant upfront investment in terms of time, skilled personnel, and infrastructure.</li>
<li><strong>Maintenance and Support</strong>: The organization is responsible for ongoing maintenance, updates, and support, which can divert resources from other critical IT functions or business initiatives.</li>
<li><strong>Risk of Obsolescence</strong>: Without continuous investment in keeping the MDM service up-to-date with the latest data management trends and technologies, there's a risk it could become obsolete.</li>
<li><strong>Longer Time to Value</strong>: Designing, developing, and deploying an in-house MDM solution can take considerably longer, delaying the realization of benefits.</li>
</ul>
<p><strong>Challenges</strong>:</p>
<ul>
<li>Ensuring the in-house team has the required expertise in data management best practices, technologies, and regulatory compliance.</li>
<li>Balancing the ongoing resource requirements for development, maintenance, and upgrades of the MDM service.</li>
</ul>
<p>When creating a Master Data Management (MDM) service, organizations need to consider various architectural options to best meet their business requirements, data governance policies, and technical landscape. These options range from centralized systems to more distributed approaches, each with its advantages and challenges. Here are some common MDM architecture options:</p>
<ol>
<li><strong>Centralized MDM Architecture</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: A single, central MDM system serves as the authoritative source for all master data across the organization. All applications and systems that require master data integrate with this central repository.</li>
<li><strong>Pros</strong>: Ensures consistency and a single version of the truth for master data; simplifies governance and data quality management.</li>
<li><strong>Cons</strong>: Can create bottlenecks; may be less responsive to local or department-specific needs; single point of failure risk.</li>
<li><strong>Challenges</strong>: Requires significant upfront investment and effort to integrate disparate systems and data sources.</li>
</ul>
<ol start="2">
<li><strong>Decentralized MDM Architecture</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: Master data is managed locally within different departments or business units, with no overarching central MDM system. Each unit maintains its own master data according to its specific needs.</li>
<li><strong>Pros</strong>: Offers flexibility; allows departments to manage data according to their unique requirements; can be quicker to implement within individual departments.</li>
<li><strong>Cons</strong>: Risk of data inconsistencies and duplication across the organization; challenges in achieving a unified view of data; more complex data integration efforts.</li>
<li><strong>Challenges</strong>: Coordinating data governance and ensuring data quality across decentralized systems can be complex.</li>
</ul>
<ol start="3">
<li><strong>Registry MDM Architecture</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: A centralized registry holds references (links or keys) to master data but not the master data itself. Actual data remains in source systems, and the registry provides a unified view.</li>
<li><strong>Pros</strong>: Reduces data redundancy; easier to implement than a fully centralized model; provides a unified view without moving data.</li>
<li><strong>Cons</strong>: Data quality and consistency must still be managed in each source system; requires robust integration and synchronization mechanisms.</li>
<li><strong>Challenges</strong>: Ensuring real-time synchronization and maintaining the accuracy of links or references in the registry.</li>
</ul>
<ol start="4">
<li><strong>Hub and Spoke MDM Architecture</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: Combines elements of centralized and decentralized architectures. A central hub manages core master data, which is then synchronized with "spoke" systems where additional, local master data management may occur.</li>
<li><strong>Pros</strong>: Balances central control with flexibility for local departments; facilitates data sharing and consistency.</li>
<li><strong>Cons</strong>: Complexity in managing and synchronizing data between the hub and spokes; potential for data conflicts between central and local systems.</li>
<li><strong>Challenges</strong>: Designing effective synchronization and conflict resolution mechanisms; managing the scalability of the system.</li>
</ul>
<ol start="5">
<li><strong>Federated MDM Architecture</strong></li>
</ol>
<ul>
<li>**Description: A federated approach integrates multiple MDM systems, each managing master data for specific domains (e.g., customers, products) or regions, without a single central system.</li>
<li><strong>Pros</strong>: Allows specialized management of different data domains; can accommodate different governance models; suitable for large, geographically dispersed organizations.</li>
<li><strong>Cons</strong>: Complex data integration and interoperability challenges; risk of inconsistencies between federated systems.</li>
<li><strong>Challenges</strong>: Ensuring seamless data integration and consistent governance across federated MDM systems.</li>
</ul>
<ol start="6">
<li><strong>Multi-Domain MDM Architecture</strong></li>
</ol>
<ul>
<li><strong>Description</strong>: A single MDM system is designed to manage multiple master data domains (e.g., customers, products) within one platform, providing a unified approach to managing diverse data types.</li>
<li><strong>Pros</strong>: Simplifies the IT landscape; reduces integration complexity; offers a consistent approach to data governance and quality across domains.</li>
<li><strong>Cons</strong>: Requires a flexible and scalable MDM solution; may be challenging to meet the specific needs of each data domain within a single system.</li>
<li><strong>Challenges</strong>: Balancing the flexibility needed for different data domains with the desire for a unified MDM platform.</li>
</ul>
<h2 id="mdm-ownership"><a class="header" href="#mdm-ownership">MDM Ownership</a></h2>
<p>Responsibility for Master Data Management (MDM) within an organization can vary significantly depending on the company's size, structure, and how data-driven its operations are. Regardless of company size, it's crucial for MDM responsibilities to involve collaboration between IT departments (who understand the technical aspects of data management and integration) and business units (who understand the data's practical use and business implications). This collaborative approach ensures that MDM efforts are aligned with business objectives and that master data is both technically sound and relevant to business needs.</p>
<h3 id="small-companies"><a class="header" href="#small-companies">Small Companies</a></h3>
<p>In smaller companies, MDM responsibilities might fall to a single individual or a small team. This could be the IT Manager, a Data Analyst, or even a Business Manager who has a good understanding of the company's data needs.</p>
<p>A startup with a lean team might have its CTO or a senior developer overseeing MDM as part of their broader responsibilities. They might focus on essential MDM tasks such as defining key data entities and ensuring data quality in critical systems like CRM and ERP.</p>
<h3 id="medium-sized-companies"><a class="header" href="#medium-sized-companies">Medium-sized Companies</a></h3>
<p>As companies grow, they often establish dedicated roles or departments for data management. This might include a Data Manager, MDM Specialist, or a small Data Governance team.</p>
<p>A mid-sized retail company might have an MDM Specialist within the IT department responsible for coordinating master data across various systems like inventory management, customer databases, and supplier information. This role might work closely with department heads to ensure data consistency and quality.</p>
<h3 id="large-enterprises"><a class="header" href="#large-enterprises">Large Enterprises</a></h3>
<p>In large enterprises, MDM is typically a significant function that involves multiple roles and departments. This can include a Chief Data Officer (CDO) at the strategic level, Data Stewards who oversee data quality and compliance in specific domains, and an MDM team that handles the day-to-day management of master data.</p>
<p>A multinational corporation, for example, might have a CDO responsible for the overall data strategy, including MDM. Under the CDO, there might be Data Stewards for different data domains (e.g., customer data, product data) and a dedicated MDM team that works on integrating, cleansing, and maintaining master data across global systems.</p>
<h3 id="industry-specific-considerations"><a class="header" href="#industry-specific-considerations">Industry-specific Considerations</a></h3>
<ul>
<li><strong>Healthcare</strong>: In a hospital or healthcare provider, the responsibility for MDM might fall to a Health Information Manager or a dedicated team within the medical records department, ensuring patient data accuracy across systems.</li>
<li><strong>Finance</strong>: In a bank or financial services firm, MDM might be overseen by a Chief Information Officer (CIO) or a specific data governance committee that ensures compliance with financial regulations and data consistency across customer accounts and transactions.</li>
</ul>
<h2 id="master-data-and-the-data-warehouse"><a class="header" href="#master-data-and-the-data-warehouse">Master Data and the Data Warehouse</a></h2>
<blockquote>
<p>In a data warehouse, master data is often managed through dimension tables. These tables store attributes about the business entities and are used to filter, group, and label data in the warehouse, enabling comprehensive and consistent analytics.</p>
</blockquote>
<p>A data warehouse is a centralized repository designed for query and analysis, integrating data from multiple sources into a consistent format. Master data is critical in a data warehouse to ensure consistency across various subject areas like sales, finance, and customer relations. Master data entities like customers, products, and employees provide a unified reference that ensures different data sources are aligned and can be analyzed together effectively.</p>
<h2 id="master-data-and-the-data-lake"><a class="header" href="#master-data-and-the-data-lake">Master Data and the Data Lake</a></h2>
<blockquote>
<p>Master data in a data lake context is used to tag and organize data, making it searchable and useful for specific business purposes. It can help in categorizing and relating different pieces of data within the lake, ensuring that users can find and interpret the data correctly.</p>
</blockquote>
<p>A data lake is a more extensive repository that stores structured and unstructured data in its native format. While data lakes offer flexibility in handling vast amounts of diverse data, master data is essential for adding structure and meaning to this data, enabling effective analysis and utilization.</p>
<h2 id="master-data-and-data-marts"><a class="header" href="#master-data-and-data-marts">Master Data and Data Marts</a></h2>
<blockquote>
<p>Master data ensures that each data mart, whether for marketing, finance, or operations, uses a consistent definition and format for key business entities. This consistency is crucial for comparing and combining data across different parts of the organization.</p>
</blockquote>
<p>Data marts are subsets of data warehouses designed to meet the needs of specific business units or departments. Master data is vital for data marts to ensure that the data presented is consistent with the enterprise's overall data strategy and with other departments' data marts.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-quality-data-management-and-data-process-quality"><a class="header" href="#data-quality-data-management-and-data-process-quality">Data Quality, Data Management, and Data Process Quality</a></h1>
<blockquote>
<p>These three pillars form the foundation upon which reliable, actionable insights are built, driving business strategies and operational efficiencies. This chapter delves into the core concepts and frameworks that govern these critical areas, exploring established models and methodologies designed to elevate an organization's data capabilities.</p>
</blockquote>
<h3 id="data-quality-the-bedrock-of-trustworthy-data"><a class="header" href="#data-quality-the-bedrock-of-trustworthy-data">Data Quality: The Bedrock of Trustworthy Data</a></h3>
<p>Data quality encompasses the characteristics that determine the <strong>reliability and effectiveness of data</strong>, including <strong>accuracy, completeness, consistency, timeliness, and relevance</strong>. High-quality data is indispensable for accurate analytics, reporting, and business intelligence, directly impacting strategic decisions and operational processes. The pursuit of data quality involves continuous monitoring, cleansing, and validation to ensure data integrity across the data lifecycle.</p>
<h3 id="data-management-the-framework-for-data-excellence"><a class="header" href="#data-management-the-framework-for-data-excellence">Data Management: The Framework for Data Excellence</a></h3>
<p>Data management represents the overarching discipline that encompasses all the processes, policies, practices, and architectures involved in managing an organization's data assets. Effective data management <strong>ensures that data is accessible, secure, usable, and stored efficiently, facilitating its optimal use across the organization</strong>. It covers a wide array of functions, from data governance and data architecture to data security and storage, providing the structure within which data quality initiatives thrive.</p>
<h3 id="data-process-quality-ensuring-operational-efficacy"><a class="header" href="#data-process-quality-ensuring-operational-efficacy">Data Process Quality: Ensuring Operational Efficacy</a></h3>
<p>Data process quality focuses on the <strong>efficiency, reliability, and effectiveness of the processes that create, manipulate, and utilize data</strong>. It involves optimizing data workflows, ensuring that data processing activities like collection, storage, transformation, and analysis are conducted in a manner that upholds data quality and meets business needs. High data process quality minimizes errors, reduces redundancies, and enhances the overall agility and responsiveness of data operations.</p>
<p>The synergy between data quality, data management, and data process quality is undeniable. Robust data management practices provide the foundation for maintaining high data quality, while the quality of data processes ensures that data management and data quality efforts are effectively implemented and sustained. Together, they form a cohesive system that ensures data is a reliable, strategic asset.</p>
<p>This chapter will explore key models and frameworks that guide organizations in enhancing these areas, including:</p>
<ul>
<li><strong>DAMA DMBOK</strong>: A comprehensive guide to data management best practices.</li>
<li><strong>Aiken's Model</strong>: A framework for assessing and improving data process quality.</li>
<li><strong>Data Management Maturity Model (DMM)</strong>: A model for evaluating and enhancing data management practices.</li>
<li><strong>Gartner's Model</strong>: Gartner's insights and methodologies for data management.</li>
<li><strong>TQDM (Total Quality Data Management)</strong>: A holistic approach to integrating quality principles into data management.</li>
<li><strong>DCAM (Data Capability Assessment Model)</strong>: A framework for assessing data management capabilities and maturity.</li>
<li><strong>MAMD Model</strong>: A model focusing on the maturity assessment of data management disciplines.</li>
</ul>
<h2 id="dama-dmbok"><a class="header" href="#dama-dmbok">DAMA DMBOK</a></h2>
<p>The Data Management Association International (DAMA) Data Management Body of Knowledge (DMBOK) is a comprehensive framework that provides standard industry guidelines and best practices for data management. It serves as a definitive guide for data professionals, outlining the processes, policies, and standards that should be implemented to manage data effectively across an organization. The DMBOK covers a wide range of data management areas, aiming to promote high standards of data quality, integrity, and security.</p>
<p>The DAMA Data Management Framework presents a structured approach to managing an organization's data assets, emphasizing the importance of data as a critical resource for business success. The framework is divided into several knowledge areas, each addressing a specific aspect of data management:</p>
<ul>
<li><strong>Data Governance</strong>: Establishing the policies, standards, and accountability for data management within an organization.</li>
<li><strong>Data Architecture</strong>: Defining the structure, integration, and alignment of data assets with business goals.</li>
<li><strong>Data Modeling and Design</strong>: Creating data models that ensure data quality and support business processes.</li>
<li><strong>Data Storage and Operations</strong>: Managing the storage, maintenance, and support of data in various forms.</li>
<li><strong>Data Security</strong>: Ensuring the confidentiality, integrity, and availability of data.</li>
<li><strong>Data Integration and Interoperability</strong>: Enabling the seamless sharing and use of data across different systems and platforms.</li>
<li><strong>Document and Content Management</strong>: Managing unstructured data, including documents and multimedia content.</li>
<li><strong>Reference and Master Data</strong>: Managing key business entities and ensuring consistency across the enterprise.</li>
<li><strong>Data Warehousing and Business Intelligence</strong>: Supporting decision-making through the aggregation, analysis, and presentation of data.</li>
<li><strong>Metadata Management</strong>: Managing data about data, ensuring that data assets are easily discoverable and understandable.</li>
<li><strong>Data Quality Management</strong>: Ensuring that data is accurate, complete, and reliable for business purposes.</li>
</ul>
<p>Some examples on how the framework can be applied accross different industries:</p>
<ul>
<li>
<p><strong>Financial Services</strong>: Implementing the Data Governance and Data Security aspects of the DAMA DMBOK to ensure compliance with financial regulations (e.g., GDPR, CCPA, SOX). This includes establishing data governance policies, data stewardship roles, and security measures to protect sensitive financial information.</p>
</li>
<li>
<p><strong>Healthcare</strong>: Applying the Data Quality Management and Metadata Management components of the framework to ensure the accuracy, completeness, and interoperability of patient data. This involves setting data quality standards, implementing data cleansing processes, and managing metadata to support electronic health records (EHR) systems.</p>
</li>
<li>
<p><strong>Retail and E-commerce</strong>: Utilizing the Reference and Master Data, and Data Warehousing and Business Intelligence knowledge areas to manage product information and customer data across multiple channels. This includes standardizing product data, integrating customer data from various touchpoints, and leveraging BI tools for market analysis and personalized marketing.</p>
</li>
<li>
<p><strong>Manufacturing</strong>: Leveraging the Data Integration and Interoperability and Data Modeling and Design parts of the DAMA DMBOK to streamline supply chain operations. This can involve creating data models that reflect the supply chain structure and implementing data integration solutions to ensure seamless data flow between suppliers, manufacturers, and distributors.</p>
</li>
<li>
<p><strong>Public Sector</strong>: Adopting the Data Architecture and Document and Content Management aspects to manage public records, policy documents, and citizen data. This includes designing a data architecture that supports the accessibility and preservation of public records and implementing content management systems for document storage and retrieval.</p>
</li>
<li>
<p>*<strong>Across All Industries</strong>: Establishing a cross-functional data governance committee to oversee the implementation of the DAMA DMBOK framework across the organization. This committee would be responsible for defining data policies, setting data quality standards, and coordinating efforts to improve data management practices in line with the framework.</p>
</li>
</ul>
<h3 id="maturity-model"><a class="header" href="#maturity-model">Maturity Model</a></h3>
<p>The DAMA DMBOK also introduces a Maturity Model to help organizations assess their current data management capabilities and identify areas for improvement. The model outlines different levels of maturity, from initial/ad-hoc processes to optimized and managed data management practices. Organizations can use this model to benchmark their data management practices against industry standards, set realistic goals for improvement, and develop a roadmap for advancing their data management capabilities.</p>
<p>The model consists of 6 levels:</p>
<h4 id="level-0-non-existent"><a class="header" href="#level-0-non-existent">Level 0: Non-existent</a></h4>
<blockquote>
<p>Data management practices are absent or chaotic. There is no formal recognition of the value of data management, leading to inconsistent, unreliable data handling.</p>
</blockquote>
<p>One example is a small startup with no dedicated data management policies or roles, where data is managed ad-hoc by whoever needs it. To advance to the next level of maturity, the company should recognize the value of structured data management and start developing basic data handling policies and procedures.</p>
<h4 id="level-1-initialad-hoc"><a class="header" href="#level-1-initialad-hoc">Level 1: Initial/Ad Hoc</a></h4>
<blockquote>
<p>Some data management activities occur, but they are informal and inconsistent. There's a lack of standardized processes, leading to inefficiencies and data quality issues.</p>
</blockquote>
<p>One example is a growing business where individual departments manage their data independently, resulting in siloed and inconsistent data practices. To advance to the next maturity level, companies should begin to standardize data management practices across projects or teams and appoint individuals responsible for overseeing data quality and consistency.</p>
<h4 id="level-2-repeatable"><a class="header" href="#level-2-repeatable">Level 2: Repeatable</a></h4>
<blockquote>
<p>The organization has developed and applied data management practices that can be repeated across projects or teams. However, these practices may not yet be uniformly enforced or optimized.</p>
</blockquote>
<p>One example is a medium-sized enterprise where certain departments have established successful data management routines that are recognized and beginning to be adopted by other parts of the organization. To adance to the next maturity level, companies should formalize data management practices into documented policies and procedures, ensuring consistency across the organization.</p>
<h4 id="level-3-defined"><a class="header" href="#level-3-defined">Level 3: Defined</a></h4>
<blockquote>
<p>Data management processes are documented, standardized, and integrated into daily operations across the organization. There's a clear understanding of roles and responsibilities related to data management.</p>
</blockquote>
<p>One example is a large corporation with established data governance frameworks, clear data stewardship roles, and department-wide adherence to data management standards. To advance to the next maturity level, companies should implement metrics to evaluate the effectiveness of data management practices and introduce continuous improvement mechanisms.</p>
<h4 id="level-4-managed"><a class="header" href="#level-4-managed">Level 4: Managed</a></h4>
<blockquote>
<p>The organization monitors and measures compliance with data management standards. There's a focus on continuous improvement based on quantitative performance metrics.</p>
</blockquote>
<p>One example is an enterprise with advanced data governance structures, where data management processes are regularly reviewed for efficiency and effectiveness, and improvements are data-driven. To advance to the next maturity level, companies should foster a culture of innovation in data management, experimenting with new technologies and methodologies to enhance data handling and usage.</p>
<h4 id="level-5-optimizing"><a class="header" href="#level-5-optimizing">Level 5: Optimizing</a></h4>
<blockquote>
<p>At this level, data management practices are continuously optimized through controlled experimentation and innovation. The organization adapts and evolves its data management capabilities to meet future needs and leverage new opportunities.</p>
</blockquote>
<p>One example is a market-leading company that pioneers the use of cutting-edge data technologies and methodologies, setting industry standards for data management and leveraging data as a key competitive advantage. Once in this maturity level, companies should maintain the culture of continuous improvement, staying ahead of industry trends and regularly reassessing and refining data management practices.</p>
<h2 id="aikens-model"><a class="header" href="#aikens-model">Aiken's Model</a></h2>
<blockquote>
<p>Aiken's Model for Data Management Maturity provides a structured approach to assessing and improving an organization's data management capabilities</p>
</blockquote>
<p>While both Aiken's Model and DAMA's DMBOK aim to enhance data management practices, they differ in scope and focus. DAMA's DMBOK provides a comprehensive framework covering a wide range of data management areas, from governance and architecture to data quality and security. Aiken's Model is more narrowly <strong>focused on the maturity progression of data management practices</strong>.</p>
<p>DAMA's DMBOK is broader, offering guidelines and best practices across various knowledge areas. Aiken's Model is specifically concerned with <strong>assessing and advancing the maturity of data management practices through a structured pathway</strong>.</p>
<p>DAMA's DMBOK serves as a reference guide for establishing robust data management practices across the organization. Aiken's Model provides a <strong>roadmap for maturing those practices over time, emphasizing continuous improvement</strong>.</p>
<h3 id="levels-of-measurement-in-aikens-model"><a class="header" href="#levels-of-measurement-in-aikens-model">Levels of Measurement in Aiken's Model</a></h3>
<p>Aiken's Model typically outlines several levels of maturity for data management, from basic, ad-hoc practices to advanced, optimized processes. While the exact levels can vary based on the interpretation of Aiken's principles, a common approach includes:</p>
<h4 id="initialad-hoc"><a class="header" href="#initialad-hoc">Initial/Ad-Hoc</a></h4>
<blockquote>
<p>Data management is unstructured and reactive, with no formal policies or standards.</p>
</blockquote>
<p>To advance to the next level, start by recognizing the importance of structured data management and initiate basic documentation of data processes.</p>
<h4 id="repeatable"><a class="header" href="#repeatable">Repeatable</a></h4>
<blockquote>
<p>Some data management practices are established and can be repeated across projects, but they are not yet standardized or consistently applied.</p>
</blockquote>
<p>To advance to the next level, develop standardized data management policies and ensure they are applied across different teams and projects.</p>
<h4 id="defined"><a class="header" href="#defined">Defined</a></h4>
<blockquote>
<p>Data management processes are formally defined, documented, and integrated into regular business operations.</p>
</blockquote>
<p>To advance to the next level, implement training programs to ensure all team members understand and adhere to established data management practices.</p>
<h4 id="managed"><a class="header" href="#managed">Managed</a></h4>
<blockquote>
<p>The organization regularly measures and evaluates the effectiveness of its data management practices, using metrics to guide improvements.</p>
</blockquote>
<p>To advance to the next level, use insights from data management metrics to identify areas for process optimization and implement targeted improvements.</p>
<h4 id="optimized"><a class="header" href="#optimized">Optimized</a></h4>
<blockquote>
<p>Data management practices are continuously refined and enhanced through feedback loops and the adoption of new technologies and best practices.</p>
</blockquote>
<p>To maintain this level, foster a culture of innovation within the data management team, encouraging experimentation with new tools and methodologies.</p>
<h3 id="implementing-aikens-model"><a class="header" href="#implementing-aikens-model">Implementing Aiken's Model</a></h3>
<p>Implementing Aiken's Model involves a step-by-step approach to maturing an organization's data management practices:</p>
<ul>
<li><strong>Assessment</strong>: Begin with a thorough assessment of current data management practices to identify the current maturity level.</li>
<li><strong>Goal Setting</strong>: Define clear, achievable goals for the next level of maturity, including specific improvements to be made.</li>
<li><strong>Policy Development</strong>: Develop or refine data management policies and standards to support the desired level of maturity.</li>
<li><strong>Training and Communication</strong>: Ensure that all relevant stakeholders are trained on new policies and practices and understand their roles in data management.</li>
<li><strong>Monitoring and Evaluation</strong>: Implement mechanisms to regularly monitor data management practices and measure their effectiveness against defined metrics.</li>
<li><strong>Continuous Improvement</strong>: Use feedback from monitoring and evaluation to continuously improve data management processes.</li>
</ul>
<p>Let's now use three companies as example: one small tech startup in Initial phase, a medium-sized retail company in Repeatable, and a multinational corporation in Managed phase.</p>
<p>The small company, with a few dozen employees, has data scattered across various platforms (e.g., Google Sheets, Dropbox, a simple database). Data management practices are informal, leading to inefficiencies and data quality issues. They plan to advance by implementing the following steps:</p>
<ul>
<li><strong>Assessment</strong>: The startup recognizes the need for structured data management to support growth.</li>
<li><strong>Goal Setting</strong>: Aim to reach the "Repeatable" level by establishing basic data management practices, such as centralized data storage and naming conventions.</li>
<li><strong>Implementation</strong>: The startup decides to consolidate data into a cloud-based platform, providing a single source of truth. They document simple, repeatable processes for data entry, update, and backup.</li>
<li><strong>Advancement</strong>: As these practices become embedded in daily operations, the startup plans to standardize data management policies and provide training to all team members.</li>
</ul>
<p>The medium-sized retail company, with several hundred employees, has basic data management practices in place for customer and inventory data but lacks consistency across departments. Their plan is:</p>
<ul>
<li><strong>Assessment</strong>: The company evaluates its data management practices and identifies inconsistencies in how customer data is handled across sales, marketing, and customer service departments.</li>
<li><strong>Goal Setting</strong>: Aim to reach the "Defined" level by creating a unified customer data management policy and integrating data systems.</li>
<li><strong>Implementation</strong>: The company develops a comprehensive data management policy, standardizing how customer data is collected, stored, and accessed. They implement a CRM system to centralize customer data and provide training to ensure compliance with the new policy.</li>
<li><strong>Advancement</strong>: With standardized data management practices in place, the company focuses on monitoring compliance and effectiveness, setting the stage for further optimization.</li>
</ul>
<p>The multinational corporation, with thousands of employees, has well-established data management practices and uses advanced analytics for strategic decision-making. However, they seek to leverage data more innovatively to maintain a competitive edge. Thei plan consists of:</p>
<ul>
<li><strong>Assessment</strong>: The enterprise conducts a thorough review of its data management practices, looking for opportunities to leverage new technologies and methodologies.</li>
<li><strong>Goal Setting</strong>: Aim to reach the "Optimized" level by incorporating AI and machine learning into data processes for predictive analytics and enhanced decision-making.</li>
<li><strong>Implementation</strong>: The enterprise invests in AI and machine learning tools to analyze large datasets for insights. They initiate pilot projects in strategic business areas, applying advanced analytics to improve product development and customer engagement.</li>
<li><strong>Advancement</strong>: The successful integration of AI and machine learning sets a new standard for data management within the enterprise, driving continuous innovation and optimization of data processes.</li>
</ul>
<h2 id="seis-data-management-maturity-model-dmm"><a class="header" href="#seis-data-management-maturity-model-dmm">SEI's Data Management Maturity Model (DMM)</a></h2>
<blockquote>
<p>The DMM model is particularly useful for organizations seeking a structured approach to assessing and improving their data management maturity, with clear categories and maturity levels.</p>
</blockquote>
<p>While the SEI's DMM, DAMA DMBOK, and Aiken's Model all aim to improve data management practices, they have different focuses and structures. SEI's DMM offers a comprehensive and structured assessment model focusing on maturity levels across specific categories of data management. It is particularly useful for organizations looking to benchmark their data management capabilities and develop a roadmap for improvement.</p>
<p>The Data Management Maturity (DMM) Model developed by the Software Engineering Institute (SEI) provides a structured framework for assessing and improving an organization's data management practices. The model is organized into six categories, each focusing on a different aspect of data management:</p>
<ol>
<li><strong>Data Governance</strong>: Focuses on establishing the policies, responsibilities, and processes to ensure effective data management and utilization across the organization.
Example: A financial institution implements a data governance committee to oversee data policies, ensuring compliance with financial regulations and internal data standards.</li>
<li><strong>Data Quality</strong>: Focuses on ensuring the accuracy, completeness, and reliability of data throughout its lifecycle.
Example: An e-commerce company develops automated data quality checks within its product information management system to ensure product descriptions and pricing are accurate and up-to-date.</li>
<li><strong>Data Operations</strong>: Focuses on managing the day-to-day activities involved in data collection, storage, maintenance, and archiving.
Example: A healthcare provider standardizes its patient data entry processes across all clinics to streamline data collection and reduce errors.</li>
<li><strong>Platform and Architecture</strong>: Focuses on establishing the technical infrastructure and architecture to support data management needs.
Example: A technology startup adopts cloud-based data storage solutions and microservices architecture to enhance scalability and data integration capabilities.</li>
<li><strong>Data Management Process</strong>: Focuses on defining and optimizing the processes involved in managing data, from creation to retirement.
Example: A manufacturing company maps out its entire data flow, from raw material procurement data to production and sales data, optimizing each step for efficiency and accuracy.</li>
<li><strong>Supporting Processes</strong>: Focuses on implementing auxiliary processes that support core data management activities, such as security, privacy, and compliance.
Example: An online retailer enhances its data encryption practices and implements stricter access controls to protect customer data and comply with privacy regulations.</li>
</ol>
<h3 id="dmm-model-maturity-levels"><a class="header" href="#dmm-model-maturity-levels">DMM Model Maturity Levels</a></h3>
<p>The DMM Model is structured around specific maturity levels that describe an organization's progression in data management capabilities, focusing on measurable improvements across various categories like Data Governance, Data Quality, and Data Operations. The levels typically range from:</p>
<ol>
<li><strong>Ad Hoc</strong>: Data management practices are unstructured and inconsistent.</li>
<li><strong>Managed</strong>: Basic data management processes are in place but are department-specific.</li>
<li><strong>Standardized</strong>: Organization-wide data management standards and policies are established.</li>
<li><strong>Quantitatively Managed</strong>: Data management processes are measured and controlled.</li>
<li><strong>Optimizing</strong>: Continuous process improvement is embedded in data management practices.</li>
</ol>
<p>DAMA DMBOK does not explicitly define maturity levels in the same structured manner as the DMM Model. Instead, it provides a comprehensive framework covering various knowledge areas essential for effective data management. Aiken's Model outlines a progression through which organizations can develop their data management practices. The comparative analysis for these models are:</p>
<ul>
<li><strong>Structure and Explicitness</strong>: The DMM Model provides a structured and explicit set of maturity levels, making it easier for organizations to benchmark their current state. In contrast, DAMA DMBOK focuses more on the breadth of knowledge areas, leaving maturity assessment more implicit. Aiken's Model offers a clear progression but is more focused on the journey of improving data management practices than on defining specific organizational capabilities at each level.</li>
<li><strong>Focus Areas</strong>: The DMM Model and Aiken's Model both emphasize the evolution of data management practices, but the DMM Model is more granular in its assessment across different data management categories. DAMA DMBOK, while not explicitly structured around maturity levels, covers a broader array of data management disciplines, providing a comprehensive framework that organizations can adapt to their maturity assessment processes.</li>
<li><strong>Application and Goals</strong>: Organizations looking for a detailed roadmap to improve their data management capabilities might lean towards the DMM Model or Aiken's Model for their structured approach to maturity. In contrast, those seeking to ensure comprehensive coverage of all data management areas might use DAMA DMBOK as a guiding framework, supplementing it with maturity concepts from the other models.</li>
</ul>
<p>In practice, organizations might blend elements from each of these frameworks, using DAMA DMBOK's comprehensive knowledge areas as a foundation, Aiken's Model for understanding the staged progression of capabilities, and the DMM Model for specific benchmarks and metrics to gauge and advance their maturity in data management.</p>
<h2 id="gartners-model-for-enterprise-information-management-eim"><a class="header" href="#gartners-model-for-enterprise-information-management-eim">Gartner's Model for Enterprise Information Management (EIM)</a></h2>
<blockquote>
<p>Gartner's EIM model emphasizes the strategic use of information as an asset to drive business value and competitive advantage.</p>
</blockquote>
<p>Gartner's model for Enterprise Information Management (EIM) provides a strategic framework for managing an organization's information assets. Unlike traditional data management models that often focus on the technical aspects of managing data, Gartner's EIM model emphasizes the strategic use of information as an asset to drive business value and competitive advantage. The model integrates data management practices with business strategy, aligning data and information initiatives with broader organizational goals.</p>
<p>Gartner's EIM model distinguishes itself from DAMA's DMBOK and the DMM model by its strong emphasis on aligning information management with business strategy and treating information as a strategic asset. While DAMA's DMBOK provides a comprehensive knowledge framework for data management and the DMM model offers a structured approach to assessing data management maturity, Gartner's EIM model focuses on the strategic integration of information management into business processes and decision-making, aiming to leverage data for competitive advantage.</p>
<p>Gartner's model is more strategic, emphasizing the role of information in achieving business objectives. In contrast, Aiken's model has a more operational focus, concentrating on improving the internal processes and capabilities of data management. Gartner's levels are explicitly aligned with the integration of data management into business strategy, whereas Aiken's stages are more about the maturity and sophistication of data management practices themselves. Gartner's model applies broadly to how an organization manages all its information assets in alignment with business goals, while Aiken's Model is more narrowly focused on the maturity of data management practices.</p>
<h3 id="maturity-levels-in-gartners-eim-model"><a class="header" href="#maturity-levels-in-gartners-eim-model">Maturity Levels in Gartner's EIM Model</a></h3>
<p>Gartner's EIM model outlines several maturity levels, detailing an organization's progression from basic, uncoordinated information management to a mature, optimized, and strategically aligned EIM practice. While Gartner may update its model periodically, a typical progression might include:</p>
<ul>
<li><strong>Awareness</strong>: The organization recognizes the importance of information management but lacks formal strategies and systems. Information is managed in silos, leading to inefficiencies.</li>
<li><strong>Reactive</strong>: The organization begins to address information management in response to specific problems or regulatory requirements. Efforts are project-based and lack cohesion.</li>
<li><strong>Proactive</strong>: There's a shift towards a more proactive approach to information management. The organization starts to implement standardized policies, tools, and governance structures across departments.</li>
<li><strong>Service-Oriented</strong>: Information management is centralized, and services are provided to the entire organization through a shared-service model. There is a focus on efficiency, quality, and supporting business objectives.</li>
<li><strong>Strategic</strong>: Information is fully integrated into business strategy. The organization leverages information as a strategic asset, driving innovation, customer value, and competitive differentiation.</li>
</ul>
<h3 id="metrics-for-assessing-eim-maturity"><a class="header" href="#metrics-for-assessing-eim-maturity">Metrics for Assessing EIM Maturity</a></h3>
<p>To gauge progress and effectiveness at each maturity level, Gartner suggests using a range of metrics that can include, but are not limited to:</p>
<ul>
<li><strong>Data Quality Metrics</strong>: Accuracy, completeness, consistency, and timeliness of data.</li>
<li><strong>Governance Metrics</strong>: Compliance rates with data policies, number of data stewards, and governance initiatives in place.</li>
<li><strong>Usage and Adoption Metrics</strong>: The extent of EIM tool adoption across the organization, user satisfaction scores, and the integration of EIM practices into daily operations.</li>
<li><strong>Business Impact Metrics</strong>: The measurable impact of EIM on business outcomes, such as increased revenue, cost savings, improved customer satisfaction, and reduced risk.</li>
</ul>
<h3 id="advancing-through-the-levels"><a class="header" href="#advancing-through-the-levels">Advancing Through the Levels</a></h3>
<p>Progressing from one maturity level to the next in Gartner's EIM model involves:</p>
<ul>
<li><strong>Strategic Alignment</strong>: Ensuring that information management strategies are aligned with business goals and objectives.</li>
<li><strong>Governance and Leadership</strong>: Establishing strong governance structures and leadership to guide EIM initiatives.</li>
<li><strong>Technology and Tools</strong>: Implementing and integrating the right technologies and tools to support effective information management.</li>
<li><strong>Culture and Collaboration</strong>: Fostering a culture that values information as an asset and promotes collaboration across departments.</li>
<li><strong>Continuous Improvement</strong>: Regularly reviewing and refining EIM practices to adapt to changing business needs and technological advancements.</li>
</ul>
<h2 id="total-quality-data-management-tqdm"><a class="header" href="#total-quality-data-management-tqdm">Total Quality Data Management (TQDM)</a></h2>
<p>Total Quality Data Management (TQDM) is an approach that integrates the principles of Total Quality Management (TQM) into data management practices. TQDM emphasizes continuous improvement, customer (user) satisfaction, and the involvement of all members of an organization in enhancing the quality of data. This approach recognizes data as a critical asset that directly impacts decision-making, operational efficiency, and customer satisfaction.</p>
<p>Compared to traditional data management approaches, TQDM is more holistic and continuous. While traditional data management might focus on specific projects or initiatives to improve data quality, TQDM integrates quality into every aspect of data management, making it an ongoing priority. TQDM's emphasis on user satisfaction, process improvement, and employee involvement also distinguishes it from more technologically focused data management strategies.</p>
<h3 id="key-principles-of-tqdm"><a class="header" href="#key-principles-of-tqdm">Key Principles of TQDM</a></h3>
<p><strong>Customer Focus</strong>: Just as TQM focuses on customer satisfaction, TQDM emphasizes meeting or exceeding the data needs of internal and external users. Understanding and addressing the data requirements of business users, customers, and partners is central to TQDM.</p>
<p><strong>Continuous Improvement</strong>: TQDM adopts the principle of Kaizen, or continuous improvement, applying it to data processes. It involves regularly assessing and enhancing data collection, storage, management, and analysis processes to improve data quality and utility.</p>
<p><strong>Process-Oriented Approach</strong>: Data quality is seen as the result of quality data management processes. TQDM focuses on optimizing these processes to ensure they are efficient, effective, and capable of producing high-quality data.</p>
<p><strong>Employee Involvement</strong>: TQDM encourages the involvement of employees across the organization in data quality initiatives. Data quality is seen as a shared responsibility, with training and empowerment provided to employees to contribute to data management efforts.</p>
<p><strong>Fact-Based Decision Making</strong>: Decisions within a TQDM framework are made based on data and analysis, emphasizing the importance of accurate, reliable data for strategic and operational decision-making.</p>
<h3 id="implementing-tqdm"><a class="header" href="#implementing-tqdm">Implementing TQDM</a></h3>
<p>Implementing TQDM involves several steps, including:</p>
<ul>
<li><strong>Assessing Data Quality Needs</strong>: Identifying the critical data elements and understanding the data quality requirements from the perspective of different data users.</li>
<li><strong>Defining Data Quality Metrics</strong>: Establishing clear, measurable indicators of data quality, such as accuracy, completeness, timeliness, and relevance.</li>
<li><strong>Improving Data Processes</strong>: Analyzing and optimizing data-related processes, from data collection and entry to storage, maintenance, and usage, to enhance quality.</li>
<li><strong>Training and Empowerment</strong>: Providing employees with the knowledge and tools they need to contribute to data quality and making them stakeholders in data management.</li>
<li><strong>Monitoring and Feedback</strong>: Establishing systems for ongoing monitoring of data quality and processes, and creating feedback loops for continuous improvement.</li>
</ul>
<h3 id="benefits-of-tqdm"><a class="header" href="#benefits-of-tqdm">Benefits of TQDM</a></h3>
<ul>
<li><strong>Improved Data Quality</strong>: By focusing on the processes that create and manage data, TQDM helps ensure higher data quality across the organization.</li>
<li><strong>Enhanced Decision Making</strong>: Better data quality leads to more informed decision-making at all levels of the organization.</li>
<li><strong>Increased User Satisfaction</strong>: Addressing the data needs and requirements of users increases satisfaction and trust in the organization's data assets.</li>
<li><strong>Operational Efficiency</strong>: Optimized data processes reduce redundancies and errors, leading to more efficient operations.</li>
</ul>
<h2 id="data-management-capability-assessment-model-dcam"><a class="header" href="#data-management-capability-assessment-model-dcam">Data Management Capability Assessment Model (DCAM)</a></h2>
<p>The Data Management Capability Assessment Model (DCAM) is a comprehensive framework developed by the EDM Council, a global association created to elevate the practice of data management. DCAM provides a structured approach for assessing and improving data management practices, focusing on the capabilities necessary to establish a sustainable data management program. It's designed to help organizations benchmark their data management practices against industry standards and identify areas for improvement.</p>
<p>Compared to models like DAMA DMBOK and TQDM, DCAM provides a more structured approach to assessing data management capabilities, offering a clear maturity model and specific components to guide improvement efforts. While DAMA DMBOK offers a comprehensive knowledge framework for data management, DCAM focuses more on the capability and maturity aspects, providing a benchmarking tool for organizations to measure their progress. TQDM emphasizes quality management principles in data management, whereas DCAM provides a broader assessment model covering all aspects of data management, from governance and quality to technology and analytics.</p>
<h3 id="components-of-dcam"><a class="header" href="#components-of-dcam">Components of DCAM</a></h3>
<p>DCAM is structured around several core components, each addressing critical aspects of data management:</p>
<ul>
<li><strong>Data Management Strategy</strong>: Outlines the overarching approach and objectives for data management within the organization, ensuring alignment with business goals.</li>
<li><strong>Data Governance</strong>: Focuses on the establishment of data governance structures and roles, defining responsibilities and policies for data across the organization.</li>
<li><strong>Data Quality</strong>: Emphasizes the importance of maintaining high data quality through continuous monitoring, measurement, and improvement processes.</li>
<li><strong>Data Operations</strong>: Covers the operational aspects of data management, including data lifecycle management, data security, and data issue resolution.</li>
<li><strong>Data Architecture and Integration</strong>: Addresses the design of data architecture and the integration of data across systems to support accessibility, consistency, and usability.</li>
<li><strong>Business Process and Data Alignment</strong>: Ensures that data management practices are integrated into business processes, supporting operational efficiency and decision-making.</li>
<li><strong>Data Innovation and Analytics</strong>: Encourages the innovative use of data, leveraging analytics and advanced data technologies to drive business value.</li>
<li><strong>Technology and Infrastructure</strong>: Considers the technological foundation required to support effective data management, including data storage, processing, and analytics platforms.</li>
</ul>
<h3 id="dcam-maturity-model"><a class="header" href="#dcam-maturity-model">DCAM Maturity Model</a></h3>
<p>The DCAM framework includes a maturity model that helps organizations assess their level of data management capability across the components mentioned above. The model typically defines several maturity levels, from basic to advanced:</p>
<ul>
<li><strong>Ad Hoc/Undefined</strong>: Data management practices are informal and unstructured, with no clear policies or standards in place.</li>
<li><strong>Performed/Repeatable</strong>: Basic data management practices are being performed, but they may not be consistent or standardized across the organization.</li>
<li><strong>Defined</strong>: Formal data management policies and standards are established and documented, providing a clear framework for data management activities.</li>
<li><strong>Managed and Measurable</strong>: Data management practices are monitored and measured against defined metrics, with active management of data quality and governance.</li>
<li><strong>Optimized</strong>: Continuous improvement processes are in place, with data management practices being regularly refined and optimized based on performance metrics and business needs.</li>
</ul>
<h2 id="model-for-assessing-data-management-mamd"><a class="header" href="#model-for-assessing-data-management-mamd">Model for Assessing Data Management (MAMD)</a></h2>
<p>The Model for Assessing Data Management (MAMD) is a conceptual framework designed to evaluate an organization's data management practices and identify areas for improvement. While not as widely recognized as other models like DAMA DMBOK or DCAM, the principles behind an assessment model like MAMD can provide valuable insights into the maturity and effectiveness of data management within an organization.</p>
<p>Compared to DAMA DMBOK and DCAM, a conceptual model like MAMD would similarly offer a structured approach to assessing and improving data management practices. However, the specific focus areas and maturity levels might vary based on the unique aspects of the MAMD framework. While DAMA DMBOK provides a comprehensive knowledge framework and DCAM offers a capability and maturity assessment model, MAMD would combine evaluation and maturity assessment to guide organizations in enhancing their data management practices systematically.</p>
<h3 id="mamd-evaluation-model"><a class="header" href="#mamd-evaluation-model">MAMD Evaluation Model</a></h3>
<p>The evaluation model within MAMD typically focuses on various dimensions of data management, such as data quality, data governance, data architecture, and data operations, similar to other frameworks. The evaluation process involves:</p>
<ul>
<li><strong>Assessment of Current Practices</strong>: Reviewing current data management practices against best practices and standards to identify gaps and areas of non-compliance.</li>
<li><strong>Stakeholder Engagement</strong>: Involving key stakeholders from across the organization to gather insights into data management challenges and needs.</li>
<li><strong>Data Management Capabilities</strong>: Evaluating the organization's capabilities in managing data across different lifecycle stages, from creation and storage to use and disposal.</li>
<li><strong>Technology and Tools</strong>: Assessing the adequacy of the technology and tools in place to support effective data management.</li>
<li><strong>Compliance and Risk Management</strong>: Evaluating how well data management practices align with regulatory requirements and manage data-related risks.</li>
</ul>
<h3 id="mamd-maturity-model"><a class="header" href="#mamd-maturity-model">MAMD Maturity Model</a></h3>
<p>Like other data management maturity models, the MAMD maturity model would typically categorize an organization's data management practices into several levels, from initial to optimized stages:</p>
<ul>
<li><strong>Initial (Ad-Hoc)</strong>: Data management is unstructured and reactive, with no formal policies or procedures in place.</li>
<li><strong>Developing</strong>: Some data management processes and policies are being developed, but they may not be consistently applied across the organization.</li>
<li><strong>Defined</strong>: Formal data management policies and procedures are documented and implemented, covering key areas of data management.</li>
<li><strong>Managed</strong> : Data management practices are regularly monitored and reviewed, with performance measured against predefined metrics.</li>
<li><strong>Optimized</strong>: Continuous improvement processes are in place for data management, with practices regularly refined based on performance feedback and evolving business needs.</li>
</ul>
<h2 id="conclusion-to-data-process-quality-models"><a class="header" href="#conclusion-to-data-process-quality-models">Conclusion to Data Process Quality Models</a></h2>
<h3 id="comprehensive-data-governance-dama-dcam"><a class="header" href="#comprehensive-data-governance-dama-dcam">Comprehensive Data Governance (DAMA, DCAM)</a></h3>
<p>Models like DAMA DMBOK and DCAM emphasize robust data governance, which is foundational for designing data infrastructures that ensure data quality, security, and compliance. Implementing strong governance frameworks influences how data warehouses, lakes, and marts are structured to enforce policies, standards, and roles effectively.</p>
<h3 id="maturity-and-capability-focus-dmm-mamd"><a class="header" href="#maturity-and-capability-focus-dmm-mamd">Maturity and Capability Focus (DMM, MAMD)</a></h3>
<p>The maturity models provided by DMM and conceptual models like MAMD offer a roadmap for organizations to evolve their data management practices. This progression impacts data infrastructure design by encouraging scalable, flexible architectures that can adapt to growing data management sophistication, from basic data warehousing to advanced analytics in data lakes.</p>
<h3 id="strategic-alignment-gartners-eim"><a class="header" href="#strategic-alignment-gartners-eim">Strategic Alignment (Gartner's EIM)</a></h3>
<p>Gartner's focus on integrating data management with business strategy ensures that data infrastructures are designed not just for operational efficiency but also to drive business value. This approach encourages the alignment of data warehouses, lakes, and marts with strategic business objectives, ensuring they support decision-making and innovation.</p>
<h3 id="quality-driven-processes-tqdm-aikens-model"><a class="header" href="#quality-driven-processes-tqdm-aikens-model">Quality-Driven Processes (TQDM, Aiken's Model)</a></h3>
<p>The emphasis on continuous quality improvement in TQDM and the operational improvement focus of Aiken's Model impact data infrastructure design by promoting architectures that support ongoing data quality initiatives. This includes incorporating data quality tools and processes into data lakes and warehouses, and designing data marts that provide high-quality, business-specific insights.</p>
<h3 id="user-centric-design"><a class="header" href="#user-centric-design">User-Centric Design</a></h3>
<p>Across all models, there's an underlying theme of designing data infrastructures that meet the needs of end-users, whether they're business analysts, data scientists, or operational teams. This user-centric approach ensures that data warehouses, lakes, and marts are accessible, understandable, and valuable to all stakeholders, enhancing adoption and driving better business outcomes.</p>
<h3 id="innovation-and-adaptability"><a class="header" href="#innovation-and-adaptability">Innovation and Adaptability</a></h3>
<p>Models like DCAM and Gartner's EIM framework encourage organizations to stay abreast of technological advancements and evolving best practices. This influences data infrastructure design to be adaptable and open to integrating new technologies such as cloud storage, real-time analytics, and machine learning capabilities within data lakes and warehouses.</p>
<h2 id="final-thoughts-on-data-process-quality"><a class="header" href="#final-thoughts-on-data-process-quality">Final Thoughts on Data Process Quality</a></h2>
<p>In conclusion, while each data management model offers distinct methodologies and focuses, collectively, they underscore the importance of strategic, quality-focused, and user-centric approaches to data management. The impact on data infrastructure design is profound, guiding organizations toward building data warehouses, lakes, and marts that are not only efficient and compliant but also agile, scalable, and aligned with business strategies. By adopting principles from these models, organizations can ensure their data infrastructure is well-positioned to support current and future data management needs, driving insights, innovation, and competitive advantage in an increasingly data-driven world.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-quality-models"><a class="header" href="#data-quality-models">Data Quality Models</a></h1>
<blockquote>
<p>Data Quality Models are fundamental frameworks that define, measure, and evaluate the quality of data within an organization. These models are crucial because they provide a structured approach to identifying and quantifying the various aspects of data quality, which are essential for ensuring that data is accurate, consistent, reliable, and fit for its intended use.</p>
</blockquote>
<p>Data Quality Models are particularly important for data teams, data engineers, and data analysts who are responsible for managing the lifecycle of data, from its creation and storage to its processing and analysis. By applying these models, professionals can ensure that the data they work with meets the necessary standards of quality, thereby supporting effective decision-making, optimizing business processes, and enhancing customer satisfaction.</p>
<p>A Data Quality Model is a conceptual framework used to define, understand, and measure the quality of data. It outlines specific criteria and dimensions that are essential for assessing the fitness of data for its intended use. These models serve as a guideline for data teams, including data engineers and data analysts, to systematically evaluate and improve the quality of the data within their systems.</p>
<h3 id="key-criteria-and-dimensions-of-data-quality"><a class="header" href="#key-criteria-and-dimensions-of-data-quality">Key Criteria and Dimensions of Data Quality</a></h3>
<p>Data quality can be assessed through various dimensions, each representing a critical aspect of the data's overall quality. While different models may emphasize different dimensions, the following are widely recognized and form the core of most Data Quality Models:</p>
<ul>
<li><strong>Accuracy</strong>: Refers to the correctness and precision of the data. Data is considered accurate if it correctly represents the real-world values it is intended to model.</li>
<li><strong>Completeness</strong>: Measures whether all the required data is present. Incomplete data can lead to gaps in analysis and decision-making.</li>
<li><strong>Consistency</strong>: Ensures that the data does not contain conflicting or contradictory information across the dataset or between multiple data sources.</li>
<li><strong>Timeliness</strong>: Pertains to the availability of data when it is needed. Timely data is crucial for decision-making processes that rely on up-to-date information.</li>
<li><strong>Relevance</strong>: Assesses whether the data is applicable and helpful for the context in which it is used. Data should meet the needs of its intended purpose.</li>
<li><strong>Reliability</strong>: Focuses on the trustworthiness of the data. Reliable data is sourced from credible sources and maintained through dependable processes.</li>
<li><strong>Uniqueness</strong>: Ensures that entities within the data are represented only once. Duplicate records can skew analysis and lead to inaccurate conclusions.</li>
<li><strong>Validity</strong>: Measures whether the data conforms to the specific syntax (format, type, range) defined by the data model and business rules.</li>
<li><strong>Accessibility</strong>: Data should be easily retrievable and usable by authorized individuals, ensuring that data consumers can access the data when needed.</li>
<li><strong>Integrity</strong>: Refers to the maintenance of data consistency and accuracy over its lifecycle, including relationships within the data that enforce logical rules and constraints.</li>
</ul>
<h3 id="applying-a-data-quality-model"><a class="header" href="#applying-a-data-quality-model">Applying a Data Quality Model</a></h3>
<p>In practice, data teams apply these dimensions by:</p>
<ul>
<li><strong>Setting Benchmarks</strong>: Defining acceptable levels or thresholds for each data quality dimension relevant to their business context.</li>
<li><strong>Data Profiling and Auditing</strong>: Using tools and techniques to assess the current state of data against the defined benchmarks.</li>
<li><strong>Implementing Controls</strong>: Establishing processes and controls to maintain data quality, such as validation checks during data entry or automated cleansing routines.</li>
<li><strong>Continuous Monitoring</strong>: Regularly monitoring data quality metrics to identify areas for improvement and to ensure ongoing compliance with quality standards.</li>
</ul>
<h3 id="impact-on-data-infrastructure"><a class="header" href="#impact-on-data-infrastructure">Impact on Data Infrastructure</a></h3>
<p>The application of a Data Quality Model has a direct impact on the design and architecture of data infrastructure:</p>
<ul>
<li><strong>Data Warehouses and Data Lakes</strong>: Ensuring that data stored in these repositories meets quality standards is crucial for reliable reporting and analytics.</li>
<li><strong>Data Marts</strong>: Tailored for specific business functions, the quality of data in data marts directly affects the accuracy and reliability of business insights derived from them.</li>
<li><strong>ETL Processes</strong>: Extract, Transform, Load (ETL) processes must incorporate data quality checks to cleanse, validate, and standardize data as it moves between systems.</li>
</ul>
<h3 id="scope"><a class="header" href="#scope">Scope</a></h3>
<p>Before delving into the specific dimensions of data quality, it's important to outline the components of the data infrastructure ecosystem that will be under consideration:</p>
<ul>
<li><strong>Data Source (Operational Data)</strong>: This refers to the original sources of data that feed into data lakes, data warehouses, and data marts. It's primarily operational data that originates from business activities and transactions.</li>
<li><strong>ELTs (Extract, Load, Transform)</strong>: These are the processes responsible for ingesting Operational Data into the data infrastructure, which could be a database, a data lake, or a data warehouse. Tools like AWS DMS (Database Migration Service), Airbyte, Fivetran, or services connecting to data sources through APIs, ODBC, message queues, etc.</li>
<li><strong>Data Lake</strong>: This component acts as a vast repository for storing a wide array of data types, including Structured, Semi-Structured, and Unstructured data. An example of a data lake is AWS S3 Buckets.</li>
<li><strong>Data Warehouse</strong>: Serving as a centralized repository, a data warehouse enables the analysis of data to support informed decision-making. Some examples include Snowflake, AWS Redshift, and Databricks Data Lakehouse.</li>
<li><strong>Data Marts</strong>: These are focused segments of data warehouses tailored to meet the specific requirements of different business units or departments, facilitating more targeted data analysis.</li>
<li><strong>ETLs (Extract, Transform, Load)</strong>: This process is centered around data transformation. Tools such as dbt, pandas, and Informatica are commonly used for this purpose.</li>
</ul>
<p>Depending on the use case, the presence and significance of these components may vary. Similarly, the dimensions of data quality being assessed might also differ based on the specific requirements and context of each scenario.</p>
<h2 id="dimensions-of-data-quality"><a class="header" href="#dimensions-of-data-quality">Dimensions of Data Quality</a></h2>
<h3 id="accuracy-dimension-in-data-quality"><a class="header" href="#accuracy-dimension-in-data-quality">Accuracy Dimension in Data Quality</a></h3>
<blockquote>
<p>Accuracy is one of the most critical dimensions of data quality, referring to the closeness of data values to the true values they are intended to represent. Ensuring accuracy is fundamental across all stages of the data infrastructure, from data sources through ELTs (Extract, Load, Transform) processes, data lakes, data warehouses, to data marts, and ultimately in reports and dashboards.</p>
</blockquote>
<p>To measure accuracy, data teams employ various metrics and techniques, often tailored to the specific type of data and its intended use. Here's how accuracy can be measured throughout the data infrastructure:</p>
<h4 id="data-sources-operational-data"><a class="header" href="#data-sources-operational-data">Data Sources (Operational Data)</a></h4>
<ul>
<li><strong>Metric</strong>: Error Rate
<ul>
<li><strong>Formula</strong>: \( Error \ Rate = \frac{Number\ of \ Incorrect \ Records}{Total \ Number \ of \ Records} \times 100 \)</li>
<li><strong>Application</strong>: Assess the error rate in operational data by comparing recorded data values against verified true values (from trusted sources or manual verification).</li>
</ul>
</li>
</ul>
<h4 id="elt-processes"><a class="header" href="#elt-processes">ELT Processes</a></h4>
<ul>
<li><strong>Metric</strong>: Transformation Accuracy Rate
<ul>
<li><strong>Formula</strong>: \( Transformation \ Accuracy \ Rate = \frac{Number \ of \ Correctly \ Transformed \ Records}{Total \ Number \ of \ Transformed \ Records} \times 100 \)</li>
<li><strong>Application</strong>: Validate the accuracy of data post-transformation by comparing pre- and post-ELT data against expected results based on transformation logic.</li>
</ul>
</li>
</ul>
<h4 id="data-lakes-and-data-warehouses"><a class="header" href="#data-lakes-and-data-warehouses">Data Lakes and Data Warehouses</a></h4>
<ul>
<li><strong>Metric</strong>: Data Conformity Rate
<ul>
<li><strong>Formula</strong>: \( Data \ Conformity \ Rate = \frac{Number \ of \ Records \ Conforming \ to \ Data \ Models}{Total \ Number \ of \ Records} \times 100 \)</li>
<li><strong>Application</strong>: Ensure that data in lakes and warehouses conforms to predefined data models and schemas, indicating accurate structuring and categorization.</li>
</ul>
</li>
</ul>
<h4 id="data-marts"><a class="header" href="#data-marts">Data Marts</a></h4>
<ul>
<li><strong>Metric</strong>: Attribute Accuracy
<ul>
<li><strong>Formula</strong>: \( Attribute \ Accuracy = \frac{Number \ of \ Correct \ Attribute \ Values}{Total \ Number \ of \ Attribute \ Values} \times 100 \)</li>
<li><strong>Application</strong>: For each attribute in a data mart, compare the values against a set of true values or rules to assess attribute-level accuracy.</li>
</ul>
</li>
</ul>
<h4 id="reports-and-dashboards"><a class="header" href="#reports-and-dashboards">Reports and Dashboards</a></h4>
<ul>
<li><strong>Metric</strong>: Reporting Accuracy
<ul>
<li><strong>Formula</strong>: No fixed formula; involves qualitative assessments and cross-validation with known true values or external benchmarks.</li>
<li><strong>Application</strong>: Verify the accuracy of reports and dashboards by cross-referencing with external benchmarks, known datasets, or through manual validation of sample data points.</li>
</ul>
</li>
</ul>
<h4 id="ensuring-and-improving-accuracy"><a class="header" href="#ensuring-and-improving-accuracy">Ensuring and Improving Accuracy</a></h4>
<p>Ensuring accuracy across the data infrastructure involves several key practices:</p>
<ul>
<li><strong>Data Profiling and Cleaning</strong>: Regularly profile data at source and post-ELT to identify inaccuracies. Implement data cleaning routines to correct identified inaccuracies.</li>
<li><strong>Validation Rules</strong>: Establish comprehensive validation rules that data must meet before entering the system, ensuring only accurate data is processed and stored.</li>
<li><strong>Automated Testing and Monitoring</strong>: Implement automated testing of data transformations and monitoring of data quality metrics to continuously assess and ensure accuracy.</li>
<li><strong>Feedback Loops</strong>: Create mechanisms for users to report inaccuracies in reports and dashboards, feeding back into data cleaning and improvement processes.</li>
</ul>
<h4 id="accuracy-measurement-example"><a class="header" href="#accuracy-measurement-example">Accuracy Measurement Example</a></h4>
<p>Measuring accuracy in a data infrastructure involves a series of steps and tools that ensure data remains consistent and true to its source throughout its lifecycle. Here's a detailed example incorporating dbt (data build tool), Soda Core, and SQL queries, illustrating how accuracy can be measured from the moment data is loaded into a data lake or warehouse, through transformation processes, and finally when it is ingested into a data mart, in a different process or pipeline, of course. Each pipeline is orchestrated by Apache Airflow.</p>
<p><strong>Pipeline 1: Validating Operational Data Post-Load</strong></p>
<ul>
<li>
<p><strong>Scenario</strong>: Once AWS DMS (Database Migration Service) or any ELT tool finishes loading operational data into the data lake or data warehouse, immediate validation is crucial to ensure data accuracy.</p>
</li>
<li>
<p><strong>Implementation</strong>:</p>
<ul>
<li><strong>Soda Core</strong>: Use Soda Core to run validation checks on the newly ingested data. Soda Core can be configured to perform checks such as row counts, null value checks, or even more complex validations against known data quality rules.</li>
<li><strong>SQL Query</strong>: Write a SQL query to validate specific data accuracy metrics, such as comparing sums, counts, or specific field values against expected values or historical data.</li>
</ul>
</li>
<li>
<p><strong>Saving Metrics</strong>: Store the results of these validations in a dedicated metrics or audit database, capturing details like the timestamp of the check, the specific checks performed, and the outcomes.</p>
<ul>
<li><strong>Sample</strong>: 'transactions_yesterday_count' | 1634264 | '2024-02-19T19:12:21.310Z' | 'order_service' | 'orders'</li>
</ul>
</li>
</ul>
<p><strong>Pipeline 2: Transforming Data with dbt</strong></p>
<ul>
<li>
<p><strong>Scenario</strong>: Transformations are applied to the ingested data to prepare it for use in data marts, using dbt for data modeling and transformations. After transformations, data is ready to be ingested into data marts for specific business unit analyses.</p>
</li>
<li>
<p><strong>Implementation</strong>:</p>
<ul>
<li><strong>dbt Tests</strong>: Use dbt's built-in testing capabilities to validate the accuracy of transformed data. This can include unique tests, referential integrity tests, or custom SQL tests that assert data accuracy post-transformation.</li>
<li><strong>dbt Metrics</strong>: Define and calculate key data accuracy metrics within dbt, leveraging its ability to capture and model data quality metrics alongside the transformation logic.</li>
<li><strong>Metric Comparison</strong>: Before the final ingestion into data marts, compare the dbt-calculated accuracy metrics with the initially captured metrics in the audit database to ensure that the transformation process has not introduced inaccuracies.</li>
<li><strong>Automated Alerts</strong>: Implement automated alerts to notify data teams if discrepancies exceed predefined thresholds, indicating potential accuracy issues that require investigation. This can be set in Apache Airflow.</li>
</ul>
</li>
</ul>
<h3 id="completeness-dimension-in-data-quality"><a class="header" href="#completeness-dimension-in-data-quality">Completeness Dimension in Data Quality</a></h3>
<blockquote>
<p>Completeness is a crucial dimension of data quality, referring to the extent to which all required data is present within a dataset. It measures the absence of missing values or records in the data and ensures that datasets are fully populated with all necessary information for accurate analysis and decision-making.</p>
</blockquote>
<p>To assess completeness, data teams utilize various measures and metrics that quantify the presence of data across different stages of the data infrastructure. Here's how completeness can be evaluated throughout the data ecosystem:</p>
<h4 id="data-sources-operational-data-1"><a class="header" href="#data-sources-operational-data-1">Data Sources (Operational Data)</a></h4>
<ul>
<li><strong>Metric</strong>: Missing Data Ratio
<ul>
<li><strong>Formula</strong>: \( Missing \ Data \ Ratio = \frac{Number\ of \ Missing \ Values}{Total \ Number \ of \ Values} \times 100 \)</li>
<li><strong>Application</strong>: Analyze operational data to identify missing values across critical fields. Use SQL queries or data profiling tools to calculate the missing data ratio for key attributes.</li>
</ul>
</li>
</ul>
<h4 id="elt-processes-1"><a class="header" href="#elt-processes-1">ELT Processes</a></h4>
<ul>
<li><strong>Metric</strong>: Record Completeness Rate
<ul>
<li><strong>Formula</strong>: \( Record \ Completeness \ Rate = \frac{Number \ of \ Complete \ Records}{Total \ Number \ of \ Records} \times 100 \)</li>
<li><strong>Application</strong>: After ELT processes, validate the completeness of records by checking for the presence of all expected fields. Automated data quality tools or custom scripts can be used to perform this validation.</li>
</ul>
</li>
</ul>
<h4 id="data-lakes-and-data-warehouses-1"><a class="header" href="#data-lakes-and-data-warehouses-1">Data Lakes and Data Warehouses</a></h4>
<ul>
<li><strong>Metric</strong>: Dataset Completeness
<ul>
<li><strong>Formula</strong>: No fixed formula; involves assessing the presence of all expected datasets and their completeness.</li>
<li><strong>Application</strong>: Ensure that all expected data is loaded into the data lake or warehouse and that datasets are complete. This can involve cross-referencing dataset inventories or metadata against expected data sources.</li>
</ul>
</li>
</ul>
<h4 id="data-marts-1"><a class="header" href="#data-marts-1">Data Marts</a></h4>
<ul>
<li><strong>Metric</strong>: Attribute Completeness
<ul>
<li><strong>Formula</strong>: \( Attribute \ Completeness = \frac{Number \ of \ Records \ with \ Non-Missing \ Attribute \ Values}{Total \ Number \ of \ Records} \times 100 \)</li>
<li><strong>Application</strong>: For data marts tailored to specific business functions, assess the completeness of critical attributes that support business analysis. SQL queries or data quality tools can automate this assessment.</li>
</ul>
</li>
</ul>
<h4 id="reports-and-dashboards-1"><a class="header" href="#reports-and-dashboards-1">Reports and Dashboards</a></h4>
<ul>
<li><strong>Metric</strong>: Information Completeness
<ul>
<li><strong>Formula</strong>: Qualitative assessment based on user feedback and data validation checks.</li>
<li><strong>Application</strong>: Ensure that reports and dashboards reflect complete information, with no missing data that could lead to incorrect insights. User feedback and manual validation play a key role in this stage.</li>
</ul>
</li>
</ul>
<h4 id="ensuring-and-improving-completeness"><a class="header" href="#ensuring-and-improving-completeness">Ensuring and Improving Completeness</a></h4>
<p>To maintain high levels of completeness across the data infrastructure, several best practices can be implemented:</p>
<ul>
<li><strong>Data Profiling and Auditing</strong>: Regularly profile and audit data at each stage of the pipeline to identify and address missing values or records.</li>
<li><strong>Data Quality Rules</strong>: Implement data quality rules that enforce the presence of critical data elements during data entry and processing.</li>
<li><strong>Data Integration Checks</strong>: During ELT processes, include checks to ensure all expected data is extracted and loaded, particularly when integrating data from multiple sources.</li>
<li><strong>Null Value Handling</strong>: Develop strategies for handling null values, such as data imputation or default values, where appropriate, to maintain analytical integrity.</li>
<li><strong>User Training and Guidelines</strong>: Educate data producers on the importance of data completeness and provide clear guidelines for data entry and maintenance.</li>
</ul>
<h3 id="consistency-dimension-in-data-quality"><a class="header" href="#consistency-dimension-in-data-quality">Consistency Dimension in Data Quality</a></h3>
<blockquote>
<p>Consistency in data quality refers to the absence of discrepancy and contradiction in the data across different datasets, systems, or time periods. It ensures that data remains uniform, coherent, and aligned with predefined rules or formats across the entire data infrastructure, minimizing conflicts and errors that can arise from inconsistent data.</p>
</blockquote>
<p>To evaluate consistency, data teams apply specific metrics that help identify discrepancies within and across datasets. Here's how consistency can be assessed at various stages of the data infrastructure:</p>
<h4 id="data-sources-operational-data-2"><a class="header" href="#data-sources-operational-data-2">Data Sources (Operational Data)</a></h4>
<ul>
<li><strong>Metric</strong>: Cross-System Consistency Rate
<ul>
<li><strong>Formula</strong>: \( Consistency \ Ratio = \frac{Number\ of \ Consistent \ Records \ Across \ Systems}{Total \ Number \ of \ Compared \ Records} \times 100 \)</li>
<li><strong>Application</strong>: Compare key data elements (e.g., customer information, product details) across different operational systems to identify inconsistencies. SQL queries or data comparison tools can facilitate this process.</li>
</ul>
</li>
</ul>
<h4 id="elt-processes-2"><a class="header" href="#elt-processes-2">ELT Processes</a></h4>
<ul>
<li><strong>Metric</strong>: Transformation Consistency Check
<ul>
<li><strong>Formula</strong>: No fixed formula; involves verifying that data transformations produce consistent results across different batches or datasets.</li>
<li><strong>Application</strong>: Implement automated checks or tests within ELT pipelines to ensure that loaded data maintains data integrity.</li>
</ul>
</li>
</ul>
<h4 id="data-lakes-and-data-warehouses-2"><a class="header" href="#data-lakes-and-data-warehouses-2">Data Lakes and Data Warehouses</a></h4>
<ul>
<li><strong>Metric</strong>: Historical Data Consistency
<ul>
<li><strong>Formula</strong>: \( Historical \ Consistency \ Rate = \frac{Number\ of \ Records \ Matching \ Historical \ Patterns}{Total \ Number \ of \ Records} \times 100 \)</li>
<li><strong>Application</strong>: Analyze time-series data or historical records within the data lake or warehouse to ensure that data remains consistent over time. This may involve trend analysis or anomaly detection techniques.</li>
</ul>
</li>
</ul>
<h4 id="data-marts-2"><a class="header" href="#data-marts-2">Data Marts</a></h4>
<ul>
<li><strong>Metric</strong>: Dimensional Consistency
<ul>
<li><strong>Formula</strong>: \( Dimensional \ Consistency \ Rate = \frac{Number\ of \ Consistent \ Dimension \ Records}{Total \ Number \ of \ Dimension \ Records} \times 100 \)</li>
<li><strong>Application</strong>: In data marts, assess the consistency of dimension tables (e.g., time dimensions, geographical hierarchies) to ensure they align with business rules and definitions.</li>
</ul>
</li>
</ul>
<h4 id="reports-and-dashboards-2"><a class="header" href="#reports-and-dashboards-2">Reports and Dashboards</a></h4>
<ul>
<li><strong>Metric</strong>: Reporting Data Consistency
<ul>
<li><strong>Formula</strong>: Qualitative assessment based on cross-validation and user feedback.</li>
<li><strong>Application</strong>: Validate that reports and dashboards present consistent information over time and across different data sources. This might involve cross-validation with external benchmarks or authoritative data sources.</li>
</ul>
</li>
</ul>
<h4 id="ensuring-and-improving-consistency"><a class="header" href="#ensuring-and-improving-consistency">Ensuring and Improving Consistency</a></h4>
<p>Strategies to maintain and enhance data consistency across the data infrastructure include:</p>
<ul>
<li><strong>Standardization</strong>: Develop and enforce data standards and conventions across the organization to ensure consistency in data entry, formatting, and processing.</li>
<li><strong>Centralized Data Catalogs</strong>: Maintain centralized data catalogs or dictionaries that define data elements, their acceptable values, and formats to guide consistent data usage.</li>
<li><strong>Automated Validation</strong>: Incorporate automated validation rules and checks in data pipelines to detect and correct inconsistencies as data moves through ELT processes.</li>
<li><strong>Master Data Management (MDM)</strong>: Implement MDM practices to manage key data entities centrally, ensuring consistent reference data across systems.</li>
<li><strong>Data Reconciliation</strong>: Regularly perform data reconciliation exercises to align data across different systems, particularly after significant data migrations or integrations.</li>
</ul>
<h3 id="timeliness-dimension-in-data-quality"><a class="header" href="#timeliness-dimension-in-data-quality">Timeliness Dimension in Data Quality</a></h3>
<blockquote>
<p>Timeliness refers to the degree to which data is up-to-date and available when required. It's a critical dimension of data quality that ensures data is current and provided within an acceptable timeframe, making it particularly relevant for time-sensitive decisions and operations.</p>
</blockquote>
<p>Assessing timeliness involves metrics that quantify the availability and currency of data across the data infrastructure. Here's how timeliness can be evaluated at different stages:</p>
<h4 id="data-sources-operational-data-3"><a class="header" href="#data-sources-operational-data-3">Data Sources (Operational Data)</a></h4>
<ul>
<li><strong>Metric</strong>: Data Latency
<ul>
<li><strong>Formula</strong>: \( Data \ Latency = Current \ Time - Data \ Creation \ Time \)</li>
<li><strong>Application</strong>: Measure the time taken for data generated by operational systems to become available for use. Lower latency indicates higher timeliness.</li>
</ul>
</li>
</ul>
<h4 id="elt-processes-3"><a class="header" href="#elt-processes-3">ELT Processes</a></h4>
<ul>
<li><strong>Metric</strong>: Process Duration
<ul>
<li><strong>Formula</strong>: \( Process \ Duration = Process \ End \ Time - Process \ Start \ Time \)</li>
<li><strong>Application</strong>: Track the duration of ELT processes to ensure data is processed and made available within expected timeframes. Monitoring tools or logging within ELT pipelines can facilitate this measurement.</li>
</ul>
</li>
</ul>
<h4 id="data-lakes-and-data-warehouses-3"><a class="header" href="#data-lakes-and-data-warehouses-3">Data Lakes and Data Warehouses</a></h4>
<ul>
<li><strong>Metric</strong>: Refresh Rate
<ul>
<li><strong>Formula</strong>: \( Refresh \ Rate = \frac{1}{Time \ Between \ Data \ Refreshes} \)</li>
<li><strong>Application</strong>: Assess the frequency at which data in the data lake or warehouse is updated. Higher refresh rates indicate more timely data.</li>
</ul>
</li>
</ul>
<h4 id="data-marts-3"><a class="header" href="#data-marts-3">Data Marts</a></h4>
<ul>
<li><strong>Metric</strong>: Data Availability Delay
<ul>
<li><strong>Formula</strong>: \( Data \ Availability \ Delay = Data \ Mart \ Availability \ Time - Data \ Warehouse \ Availability \ Time \)</li>
<li><strong>Application</strong>: Measure the time lag between data being updated in the data warehouse and its availability in specific data marts. Shorter delays signify better timeliness. In the case of multiple data sources, consider the time of the last available data.</li>
</ul>
</li>
</ul>
<h4 id="reports-and-dashboards-3"><a class="header" href="#reports-and-dashboards-3">Reports and Dashboards</a></h4>
<ul>
<li><strong>Metric</strong>: Report Generation Time
<ul>
<li><strong>Formula</strong>: \( Report \ Generation \ Time = Report \ Ready \ Time - Report \ Request \ Time \)</li>
<li><strong>Application</strong>: Quantify the time taken for reports and dashboards to be generated and made available to users after a request. Faster generation times enhance timeliness.</li>
</ul>
</li>
</ul>
<h4 id="ensuring-and-improving-timeliness"><a class="header" href="#ensuring-and-improving-timeliness">Ensuring and Improving Timeliness</a></h4>
<p>To maintain and boost the timeliness of data across the data infrastructure, consider the following strategies:</p>
<ul>
<li><strong>Real-Time Data Processing</strong>: Implement real-time or near-real-time data processing capabilities to minimize latency and ensure data is promptly available for decision-making.</li>
<li><strong>Optimize ELT Processes</strong>: Regularly review and optimize ELT processes to reduce processing time, employing parallel processing, efficient algorithms, and appropriate hardware resources.</li>
<li><strong>Incremental Updates</strong>: Rather than full refreshes, use incremental data updates where possible to reduce the time taken to update data stores.</li>
<li><strong>Monitoring and Alerts</strong>: Establish monitoring systems to track the timeliness of data processes, with alerts set up to notify relevant teams of any delays or issues.</li>
<li><strong>Service Level Agreements (SLAs)</strong>: Define SLAs for data timeliness, clearly outlining expected timeframes for data availability at each stage of the data infrastructure.</li>
</ul>
<h3 id="relevance-dimension-in-data-quality"><a class="header" href="#relevance-dimension-in-data-quality">Relevance Dimension in Data Quality</a></h3>
<blockquote>
<p>Relevance in data quality refers to the extent to which data is applicable and useful for the purposes it is intended for. It ensures that the data collected and maintained aligns with the current needs and objectives of the business, supporting effective decision-making and operational processes.</p>
</blockquote>
<p>Assessing the relevance of data involves evaluating how well the data meets the specific requirements and objectives of various stakeholders, including business units, data analysts, and decision-makers. Here's how relevance can be evaluated across different stages of the data infrastructure:</p>
<h4 id="data-sources-operational-data-4"><a class="header" href="#data-sources-operational-data-4">Data Sources (Operational Data)</a></h4>
<ul>
<li><strong>Metric</strong>: Data Utilization Rate
<ul>
<li><strong>Formula</strong>: \( Data \ Utilization \ Rate = \frac{Number\ of \ Data \ Elements \ Used \ in \ Decision-Making}{Total \ Number \ of \ Data \ Elements \ Available} \times 100 \)</li>
<li><strong>Application</strong>: Analyze operational data to identify which data elements are actively used in decision-making processes. This can be done through user surveys, data access logs, or analytics on database queries.</li>
</ul>
</li>
</ul>
<h4 id="data-lakes-and-data-warehouses-4"><a class="header" href="#data-lakes-and-data-warehouses-4">Data Lakes and Data Warehouses</a></h4>
<ul>
<li><strong>Metric</strong>: Data Coverage Ratio
<ul>
<li><strong>Formula</strong>: \( Data \ Coverage \ Ratio = \frac{Number\ of \ Business \ Questions \ Answerable \ with \ Data}{Total \ Number \ of \ Business \ Questions} \times 100 \)</li>
<li><strong>Application</strong>: Evaluate the extent to which data stored in the data lake or warehouse can answer key business questions. This may involve mapping data elements to specific business use cases or analytics requirements.</li>
</ul>
</li>
</ul>
<h4 id="data-marts-4"><a class="header" href="#data-marts-4">Data Marts</a></h4>
<ul>
<li><strong>Metric</strong>: Business Alignment Index
<ul>
<li><strong>Formula</strong>: Qualitative assessment based on alignment with departmental objectives and key performance indicators (KPIs).</li>
<li><strong>Application</strong>: In data marts designed for specific business functions, assess how well the data aligns with the department's KPIs and objectives. This could involve regular reviews with department heads and key users to ensure the data remains relevant to their needs.</li>
</ul>
</li>
</ul>
<h4 id="reports-and-dashboards-4"><a class="header" href="#reports-and-dashboards-4">Reports and Dashboards</a></h4>
<ul>
<li><strong>Metric</strong>: User Engagement Score
<ul>
<li><strong>Formula</strong>: \( User \ Engagement \ Score = \frac{Number\ of \ Active \ User \ Interactions \ with \ Reports \ or \ Dashboards}{Total \ Number \ of \ Reports \ or \ Dashboards \ Available} \)</li>
<li><strong>Application</strong>: Monitor user engagement with reports and dashboards to gauge their relevance. High interaction rates may suggest that the information presented is relevant and useful to the users.</li>
</ul>
</li>
</ul>
<h4 id="ensuring-and-improving-relevance"><a class="header" href="#ensuring-and-improving-relevance">Ensuring and Improving Relevance</a></h4>
<p>Strategies to maintain and enhance the relevance of data across the data infrastructure include:</p>
<ul>
<li><strong>Regular Needs Assessment</strong>: Conduct periodic assessments with data users and stakeholders to understand their evolving data needs and ensure that the data infrastructure aligns with these requirements.</li>
<li><strong>Agile Data Management</strong>: Adopt agile data management practices that allow for the flexible and rapid adaptation of data processes and structures in response to changing business needs.</li>
<li><strong>Feedback Loops</strong>: Implement mechanisms for collecting ongoing feedback from data users on the relevance of data and reports, using this feedback to guide data collection, transformation, and presentation efforts.</li>
<li><strong>Data Lifecycle Management</strong>: Establish policies for data archiving and purging, ensuring that only relevant, current data is actively maintained and available for use, reducing clutter and focusing on valuable data assets.</li>
</ul>
<h3 id="reliability-dimension-in-data-quality"><a class="header" href="#reliability-dimension-in-data-quality">Reliability Dimension in Data Quality</a></h3>
<blockquote>
<p>Reliability in the context of data quality refers to the degree of trustworthiness and dependability of the data, ensuring it consistently produces the same results under similar conditions and over time. Reliable data is crucial for maintaining the integrity of analyses, reports, and business decisions derived from that data.</p>
</blockquote>
<p>To evaluate the reliability of data, it's essential to consider various aspects such as source credibility, data collection consistency, and the stability of data values over time. Here's how reliability can be assessed across different stages of the data infrastructure:</p>
<h4 id="data-sources-operational-data-5"><a class="header" href="#data-sources-operational-data-5">Data Sources (Operational Data)</a></h4>
<ul>
<li><strong>Metric</strong>: Source Credibility Score
<ul>
<li><strong>Formula</strong>: Qualitative assessment based on the source's historical accuracy, authority, and trustworthiness.</li>
<li><strong>Application</strong>: Evaluate each data source's reliability by considering its track record, reputation, and any third-party certifications or audits. This could involve a review of source documentation and user feedback.</li>
</ul>
</li>
</ul>
<h4 id="elt-processes-4"><a class="header" href="#elt-processes-4">ELT Processes</a></h4>
<ul>
<li><strong>Metric</strong>: Process Stability Index
<ul>
<li><strong>Formula</strong>: \( Process \ Stability \ Index = \frac{Number \ of \ Successful \ ELT \ Runs}{Total \ Number \ of \ ELT \ Runs} \times 100 \)</li>
<li><strong>Application</strong>:  Monitor the stability and consistency of ELT processes by tracking the success rate of data extraction, loading, and transformation jobs. High stability indicates reliable data processing.</li>
</ul>
</li>
</ul>
<h4 id="data-lakes-and-data-warehouses-5"><a class="header" href="#data-lakes-and-data-warehouses-5">Data Lakes and Data Warehouses</a></h4>
<ul>
<li><strong>Metric</strong>: Data Variation Coefficient
<ul>
<li><strong>Formula</strong>: \( Data \ Variation \ Coefficient = \frac{Standard \ Deviation \ of \ Data \ Values}{Mean \ of \ Data \ Values} \)</li>
<li><strong>Application</strong>: Analyze the variation in data values stored in the data lake or warehouse, especially for key metrics, to assess the stability and reliability of the data over time.</li>
</ul>
</li>
</ul>
<h4 id="data-marts-5"><a class="header" href="#data-marts-5">Data Marts</a></h4>
<ul>
<li><strong>Metric</strong>: Data Consensus Ratio
<ul>
<li><strong>Formula</strong>: \( Data \ Consensus \ Ratio = \frac{Number \ of \ Data \ Points \ in \ Agreement \ with \ Consensus \ Value}{Total \ Number \ of \ Data \ Points} \times 100 \)</li>
<li><strong>Application</strong>: For data marts serving specific business functions, evaluate the consistency of data with established benchmarks or consensus values, ensuring that the data reliably reflects business realities.</li>
</ul>
</li>
</ul>
<h4 id="reports-and-dashboards-5"><a class="header" href="#reports-and-dashboards-5">Reports and Dashboards</a></h4>
<ul>
<li><strong>Metric</strong>: User Trust Index
<ul>
<li><strong>Formula</strong>: Qualitative assessment based on user surveys and feedback regarding their trust in the data presented.</li>
<li><strong>Application</strong>: Gauge the level of trust users have in reports and dashboards by collecting feedback on their experiences and perceptions of data accuracy, consistency, and reliability.</li>
</ul>
</li>
</ul>
<h4 id="ensuring-and-improving-reliability"><a class="header" href="#ensuring-and-improving-reliability">Ensuring and Improving Reliability</a></h4>
<p>Strategies to maintain and enhance data reliability across the data infrastructure include:</p>
<ul>
<li><strong>Data Source Validation</strong>: Regularly validate and audit data sources to ensure they continue to meet quality and reliability standards.</li>
<li><strong>Robust Data Processing</strong>: Design ELT processes with error handling, logging, and recovery mechanisms to maintain consistency and reliability in data processing.</li>
<li><strong>Historical Data Tracking</strong>: Maintain historical data records and change logs to track data stability and reliability over time, facilitating audits and reliability assessments.</li>
<li><strong>User Education and Communication</strong>: Educate users about the sources, processes, and controls in place to ensure data reliability, building user trust and confidence in the data.</li>
</ul>
<h3 id="uniqueness-dimension-in-data-quality"><a class="header" href="#uniqueness-dimension-in-data-quality">Uniqueness Dimension in Data Quality</a></h3>
<blockquote>
<p>Uniqueness is a critical dimension of data quality that ensures each data item or entity is represented only once within a dataset or across integrated systems. It aims to prevent duplicates, which can lead to inaccuracies in analysis, reporting, and decision-making processes. Ensuring uniqueness is particularly important in databases, data warehouses, and customer relationship management (CRM) systems where the integrity of data like customer records, product information, and transaction details is important.</p>
</blockquote>
<p>To assess the uniqueness of data, data teams utilize specific metrics that help identify and quantify duplicate entries within their datasets. Here's how uniqueness can be evaluated across different stages of the data infrastructure:</p>
<h4 id="data-sources-operational-data-6"><a class="header" href="#data-sources-operational-data-6">Data Sources (Operational Data)</a></h4>
<ul>
<li><strong>Metric</strong>: Duplication Rate
<ul>
<li><strong>Formula</strong>: \( Duplication \ Rate = \frac{Number\ of \ Duplicate \ Records}{Total \ Number \ of \ Records} \times 100 \)</li>
<li><strong>Application</strong>: Analyze operational data for duplicate entries by comparing key identifiers (e.g., customer IDs, product codes) within the source system. SQL queries or data profiling tools can facilitate this process.</li>
</ul>
</li>
</ul>
<h4 id="data-lakes-and-data-warehouses-6"><a class="header" href="#data-lakes-and-data-warehouses-6">Data Lakes and Data Warehouses</a></h4>
<ul>
<li><strong>Metric</strong>: Entity Uniqueness Score
<ul>
<li><strong>Formula</strong>: \( Entity \ Uniqueness \ Score = \frac{Number \ of \ Unique \ Entity \ Records}{Total \ Number \ of \ Entity \ Records} \times 100 \)</li>
<li><strong>Application</strong>: In data lakes and warehouses, assess the uniqueness of entities across datasets by comparing key attributes. Data quality tools can automate the identification of duplicates across disparate datasets.</li>
</ul>
</li>
</ul>
<h4 id="data-marts-6"><a class="header" href="#data-marts-6">Data Marts</a></h4>
<ul>
<li><strong>Metric</strong>: Dimensional Key Uniqueness
<ul>
<li><strong>Formula</strong>: \( Dimensional \ Key \ Uniqueness = \frac{Number \ of \ Unique \ Dimension \ Keys}{Total \ Number \ of \ Dimension \ Records} \times 100 \)</li>
<li><strong>Application</strong>: For data marts, ensure that dimensional keys (e.g., time dimensions, product dimensions) are unique to maintain data integrity and accurate reporting.</li>
</ul>
</li>
</ul>
<h4 id="reports-and-dashboards-6"><a class="header" href="#reports-and-dashboards-6">Reports and Dashboards</a></h4>
<ul>
<li><strong>Metric</strong>: Report Data Redundancy Check
<ul>
<li><strong>Formula</strong>: Qualitative assessment based on user validation and automated data checks.</li>
<li><strong>Application</strong>: Validate that reports and dashboards do not present redundant information, which could mislead decision-making. This involves both user feedback and automated data validation techniques.</li>
</ul>
</li>
</ul>
<h4 id="ensuring-and-improving-uniqueness"><a class="header" href="#ensuring-and-improving-uniqueness">Ensuring and Improving Uniqueness</a></h4>
<p>To maintain high levels of uniqueness across the data infrastructure, several best practices can be implemented:</p>
<ul>
<li><strong>De-duplication Processes</strong>: Establish automated de-duplication routines within ELT processes to identify and resolve duplicates before they enter the data warehouse or data marts.</li>
<li><strong>Master Data Management (MDM)</strong>: Implement MDM practices to manage key entities centrally, ensuring a single source of truth and preventing duplicates across systems.</li>
<li><strong>Key and Index Management</strong>: Use primary keys and unique indexes in database design to enforce uniqueness at the data storage level.</li>
<li><strong>Regular Data Audits</strong>: Conduct periodic audits of data to identify and rectify duplication issues, ensuring ongoing data quality.</li>
<li><strong>User Training and Guidelines</strong>: Educate data entry personnel on the importance of data uniqueness and provide clear guidelines for maintaining it during data collection and entry.</li>
</ul>
<h3 id="validity-dimension-in-data-quality"><a class="header" href="#validity-dimension-in-data-quality">Validity Dimension in Data Quality</a></h3>
<blockquote>
<p>Validity in data quality refers to the degree to which data conforms to specific syntax (format, type, range) and semantic (meaningful and appropriate content) rules defined by the data model and business requirements. Valid data adheres to predefined formats, standards, and constraints, ensuring that it is both structurally sound and contextually meaningful for its intended use.</p>
</blockquote>
<p>Assessing validity involves checking data against established rules and constraints to ensure it meets the required standards for format, type, range, and content. Here's how validity can be evaluated across different stages of the data infrastructure:</p>
<h4 id="data-sources-operational-data-7"><a class="header" href="#data-sources-operational-data-7">Data Sources (Operational Data)</a></h4>
<ul>
<li><strong>Metric</strong>: Format Conformance Rate
<ul>
<li><strong>Formula</strong>: \( Format \ Conformance \ Rate = \frac{Number\ of \ Records \ Meeting \ Format \ Specifications}{Total \ Number \ of \ Records} \times 100 \)</li>
<li><strong>Application</strong>: Analyze operational data to ensure that it conforms to expected formats (e.g., date formats, postal codes). This can be done using SQL queries or data profiling tools to check data formats against predefined patterns.</li>
</ul>
</li>
</ul>
<h4 id="data-lakes-and-data-warehouses-7"><a class="header" href="#data-lakes-and-data-warehouses-7">Data Lakes and Data Warehouses</a></h4>
<ul>
<li><strong>Metric</strong>: Data Type Integrity Score
<ul>
<li><strong>Formula</strong>: \( Data \ Type \ Integrity \ Rate = \frac{Number \ of \ Records \ with \ Correct \ Data \ Types}{Total \ Number \ of \ Records} \times 100 \)</li>
<li><strong>Application</strong>: In data lakes and warehouses, assess the integrity of data types to ensure that data is stored in the correct format (e.g., numeric fields are stored as numbers). Automated data quality tools can scan datasets to identify type mismatches.</li>
</ul>
</li>
</ul>
<h4 id="data-marts-7"><a class="header" href="#data-marts-7">Data Marts</a></h4>
<ul>
<li><strong>Metric</strong>: Business Rule Compliance Rate
<ul>
<li><strong>Formula</strong>: \( Business \ Rule \ Compliance \ Rate = \frac{Number \ of \ Records \ Complying \ with \ Business \ Rules}{Total \ Number \ of \ Records} \times 100 \)</li>
<li><strong>Application</strong>: For data marts, ensure that data complies with specific business rules relevant to the department or function. This involves setting up rule-based validation checks that can be run on the data mart contents.</li>
</ul>
</li>
</ul>
<h4 id="reports-and-dashboards-7"><a class="header" href="#reports-and-dashboards-7">Reports and Dashboards</a></h4>
<ul>
<li><strong>Metric</strong>: Content Validity Check
<ul>
<li><strong>Formula</strong>: Qualitative assessment based on user feedback and automated validation checks.</li>
<li><strong>Application</strong>: Ensure that reports and dashboards contain valid data by implementing content validity checks, which might include verifying that displayed data falls within expected ranges or adheres to specific content rules.</li>
</ul>
</li>
</ul>
<h4 id="ensuring-and-improving-validity"><a class="header" href="#ensuring-and-improving-validity">Ensuring and Improving Validity</a></h4>
<p>Strategies to maintain and enhance data validity across the data infrastructure include:</p>
<ul>
<li><strong>Validation Rules and Constraints</strong>: Implement comprehensive validation rules and constraints at the point of data entry and throughout data processing pipelines to ensure data validity.</li>
<li><strong>Data Quality Tools</strong>: Utilize data quality tools that offer automated validation capabilities, allowing for the continuous checking of data against validity rules.</li>
<li><strong>Data Cleansing</strong>: Engage in regular data cleansing activities to correct invalid data, using scripts or data quality platforms to identify and rectify issues.</li>
<li><strong>Metadata Management</strong>: Maintain detailed metadata that specifies the valid format, type, and constraints for each data element, guiding data handling and validation processes.</li>
<li><strong>User Education and Guidelines</strong>: Educate users involved in data entry and management about the importance of data validity and provide clear guidelines and training on maintaining it.</li>
</ul>
<h3 id="accessibility-dimension-in-data-quality"><a class="header" href="#accessibility-dimension-in-data-quality">Accessibility Dimension in Data Quality</a></h3>
<blockquote>
<p>Accessibility in data quality refers to the ease with which data can be retrieved and used by authorized individuals or systems. It ensures that data is available when needed, through appropriate channels, and in usable formats, while also maintaining necessary security and privacy controls. Accessibility is crucial for efficient decision-making, operational processes, and ensuring that data serves its intended purpose effectively.</p>
</blockquote>
<p>Evaluating accessibility involves assessing the systems, protocols, and permissions in place that enable or restrict access to data. Here’s how accessibility can be gauged across different stages of the data infrastructure:</p>
<h4 id="data-sources-operational-data-8"><a class="header" href="#data-sources-operational-data-8">Data Sources (Operational Data)</a></h4>
<ul>
<li><strong>Metric</strong>: Data Access Success Rate
<ul>
<li><strong>Formula</strong>: \( Data \ Access \ Success \ Rate = \frac{Number\ of \ Successful \ Data \ Retrieval \ Attempts}{Total \ Number \ of \ Data \ Retrieval \ Attempts} \times 100 \)</li>
<li><strong>Application</strong>: Monitor and log access attempts to operational databases or systems to identify and address any access issues, ensuring that data can be successfully retrieved when needed.</li>
</ul>
</li>
</ul>
<h4 id="elt-processes-5"><a class="header" href="#elt-processes-5">ELT Processes</a></h4>
<ul>
<li><strong>Metric</strong>: Data Integration Accessibility Score
<ul>
<li><strong>Formula</strong>: Qualitative assessment based on the integration system's ability to access and pull data from various sources.</li>
<li><strong>Application</strong>: Evaluate the accessibility of data sources by ELT tools and processes, ensuring that there are no barriers to accessing the required data for integration tasks.</li>
</ul>
</li>
</ul>
<h4 id="data-lakes-and-data-warehouses-8"><a class="header" href="#data-lakes-and-data-warehouses-8">Data Lakes and Data Warehouses</a></h4>
<ul>
<li><strong>Metric</strong>: Query Performance Index
<ul>
<li><strong>Formula</strong>: \( Query \ Performance \ Index = Average \ Response \ Time \ for \ Data \ Retrieval \ Queries \)</li>
<li><strong>Application</strong>: Measure the performance of data retrieval queries in data lakes and warehouses to assess how quickly and efficiently data can be accessed, considering factors like indexing and query optimization.</li>
</ul>
</li>
</ul>
<h4 id="data-marts-8"><a class="header" href="#data-marts-8">Data Marts</a></h4>
<ul>
<li><strong>Metric</strong>: User Access Rate
<ul>
<li><strong>Formula</strong>: \( User \ Access \ Rate = \frac{Number \ of \ Unique \ Users \ Accessing \ the \ Data \ Mart}{Total \ Number \ of \ Authorized \ Users} \times 100 \)</li>
<li><strong>Application</strong>: Track the usage of data marts by authorized users to ensure that they can access the data they need for analysis and reporting.</li>
</ul>
</li>
</ul>
<h4 id="reports-and-dashboards-8"><a class="header" href="#reports-and-dashboards-8">Reports and Dashboards</a></h4>
<ul>
<li><strong>Metric</strong>: Dashboard Availability Score
<ul>
<li><strong>Formula</strong>: \( Dashboard \ Availability \ Score = \frac{Time \ Dashboards \ are \ Operational}{Total \ Time} \times 100 \)</li>
<li><strong>Application</strong>: Monitor the availability and uptime of reports and dashboards to ensure that users can access key insights and information without disruptions.</li>
</ul>
</li>
</ul>
<h4 id="ensuring-and-improving-accessibility"><a class="header" href="#ensuring-and-improving-accessibility">Ensuring and Improving Accessibility</a></h4>
<p>To maintain and enhance data accessibility across the data infrastructure, consider the following strategies:</p>
<ul>
<li><strong>Robust Data Architecture</strong>: Design data systems and architectures that support efficient data retrieval and query performance, incorporating features like indexing, caching, and data partitioning.</li>
<li><strong>Access Control Policies</strong>: Implement comprehensive access control policies that define who can access what data, ensuring that data is accessible to authorized users while maintaining security and privacy.</li>
<li><strong>User-Centric Design</strong>: Ensure that data repositories, reports, and dashboards are designed with the end-user in mind, focusing on usability, intuitive navigation, and user-friendly interfaces.</li>
<li><strong>Monitoring and Alerts</strong>: Set up monitoring systems to track data system performance and accessibility, with alerts for any issues that might impede access, allowing for prompt resolution.</li>
<li><strong>Training and Support</strong>: Provide training and support to users on how to access and use data systems, tools, and platforms effectively, enhancing their ability to retrieve and utilize data.</li>
</ul>
<h3 id="integrity-dimension-in-data-quality"><a class="header" href="#integrity-dimension-in-data-quality">Integrity Dimension in Data Quality</a></h3>
<blockquote>
<p>Integrity in data quality refers to the consistency, accuracy, and trustworthiness of data across its lifecycle. It involves maintaining data's completeness, coherence, and credibility, ensuring that it remains unaltered from its source through various transformations and usage. Data integrity is crucial for ensuring that the information used for decision-making, reporting, and analysis is reliable and reflects the true state of affairs.</p>
</blockquote>
<p>Evaluating data integrity involves assessing the processes, controls, and systems in place to prevent unauthorized data alteration and to ensure data remains consistent and accurate. Here’s how integrity can be assessed across different stages of the data infrastructure:</p>
<h4 id="data-sources-operational-data-9"><a class="header" href="#data-sources-operational-data-9">Data Sources (Operational Data)</a></h4>
<ul>
<li><strong>Metric</strong>: Source-to-Target Consistency Rate
<ul>
<li><strong>Formula</strong>: \( Source-to-Target \ Consistency \ Rate = \frac{Number\ of \ Consistent \ Records \ Between \ Source \ and \ Target}{Total \ Number \ of \ Records \ Reviewed} \times 100 \)</li>
<li><strong>Application</strong>: Compare data records in the operational systems (source) with those in the data warehouse or lake (target) to ensure data has been transferred accurately and remains unaltered.</li>
</ul>
</li>
</ul>
<h4 id="data-lakes-and-data-warehouses-9"><a class="header" href="#data-lakes-and-data-warehouses-9">Data Lakes and Data Warehouses</a></h4>
<ul>
<li><strong>Metric</strong>: Referential Integrity Score
<ul>
<li><strong>Formula</strong>: \( Referential \ Integrity \ Score = \frac{Number \ of \ Records \ with \ Valid \ References}{Total \ Number \ of \ Records} \times 100 \)</li>
<li><strong>Application</strong>: Validate referential integrity within the data lake or warehouse, ensuring that all foreign key relationships are consistent and that related records are present.</li>
</ul>
</li>
</ul>
<h4 id="data-marts-9"><a class="header" href="#data-marts-9">Data Marts</a></h4>
<ul>
<li><strong>Metric</strong>: Dimensional Integrity Index
<ul>
<li><strong>Formula</strong>: \( Dimensional \ Integrity \ Index = \frac{Number \ of \ Dimension \ Records \ with \ Consistent \ Attributes}{Total \ Number \ of \ Dimension \ Records} \times 100 \)</li>
<li><strong>Application</strong>: Check the integrity of dimension tables in data marts, ensuring that attributes like time dimensions, geographical hierarchies, or product categories remain consistent and accurate.</li>
</ul>
</li>
</ul>
<h4 id="reports-and-dashboards-9"><a class="header" href="#reports-and-dashboards-9">Reports and Dashboards</a></h4>
<ul>
<li><strong>Metric</strong>: Data Traceability Index
<ul>
<li><strong>Formula</strong>: Qualitative assessment based on the ability to trace data back to its source.</li>
<li><strong>Application</strong>: Ensure that data presented in reports and dashboards can be traced back to its original source or to the transformation logic applied, maintaining a clear lineage for auditability and verification.</li>
</ul>
</li>
</ul>
<h4 id="ensuring-and-improving-integrity"><a class="header" href="#ensuring-and-improving-integrity">Ensuring and Improving Integrity</a></h4>
<p>To maintain and enhance data integrity across the data infrastructure, consider implementing the following strategies:</p>
<ul>
<li><strong>Data Validation Rules</strong>: Establish validation rules that check data for integrity at every stage of its movement and transformation within the system.</li>
<li><strong>Audit Trails and Data Lineage</strong>: Maintain comprehensive audit trails and clear data lineage documentation, enabling the tracking of data from its source through all transformations to its final form.</li>
<li><strong>Access Controls and Security Measures</strong>: Implement robust access controls and security measures to prevent unauthorized data access or alteration, protecting data integrity.</li>
<li><strong>Regular Data Audits</strong>: Conduct periodic audits of data and data management processes to identify and rectify any integrity issues, ensuring ongoing compliance with data integrity standards.</li>
<li><strong>Error Handling and Correction Procedures</strong>: Develop standardized procedures for handling data errors and anomalies detected during processing, ensuring that integrity issues are promptly and effectively addressed.</li>
</ul>
<h2 id="conclusion-navigating-data-quality-dimensions-in-diverse-use-cases"><a class="header" href="#conclusion-navigating-data-quality-dimensions-in-diverse-use-cases">Conclusion: Navigating Data Quality Dimensions in Diverse Use Cases</a></h2>
<p>In this chapter, we explored several critical dimensions of data quality, including Accuracy, Completeness, Consistency, Relevance, Reliability, Uniqueness, Validity, Accessibility, and Integrity. Each of these dimensions plays a vital role in ensuring that data serves its intended purpose effectively, supporting decision-making, operational efficiency, and strategic initiatives.</p>
<p>However, it's important to recognize that not every use case will require an exhaustive focus on all these dimensions. The relevance and priority of each dimension can vary significantly depending on factors such as industry norms, organizational size, team composition, and the maturity of the data infrastructure in place. For instance:</p>
<ul>
<li>
<p>A financial institution might prioritize Accuracy and Integrity due to the regulatory and fiduciary responsibilities inherent in the industry.</p>
</li>
<li>
<p>A retail business may focus more on Completeness and Relevance to ensure customer data supports effective marketing and sales strategies.</p>
</li>
<li>
<p>A startup with a lean data team might concentrate on Accessibility and Validity to quickly derive value from limited data resources.</p>
</li>
</ul>
<p>Moreover, the metrics presented for measuring each dimension, while broadly applicable, may not be entirely relevant or sufficient for every context. Organizations may find that industry-specific metrics, company-size considerations, team capabilities, or the particularities of their data infrastructure necessitate the development of custom metrics tailored to their unique use cases.</p>
<p>For example:</p>
<p>A large enterprise with a complex data ecosystem might develop sophisticated metrics to measure data lineage and impact analysis, ensuring Integrity and Consistency across multiple systems.
A small team within a mid-sized company might adopt more straightforward, manually checked metrics focused on the immediate usability of data, emphasizing Validity and Relevance.
Additionally, as data environments evolve and new technologies emerge, new dimensions of data quality may become relevant, and existing dimensions may need to be reinterpreted or expanded. Continuous learning, adaptation, and innovation in data quality practices are essential for organizations to keep pace with these changes.</p>
<p>In conclusion, while the dimensions of data quality outlined in this chapter provide a comprehensive framework for understanding and improving data quality, their application must be adapted to fit the specific needs and constraints of each organization. By carefully selecting which dimensions to focus on and customizing metrics to their unique contexts, data teams can effectively enhance the quality of their data, driving more accurate insights, efficient operations, and strategic growth.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-quality--data-reliability"><a class="header" href="#data-quality--data-reliability">Data Quality &amp; Data Reliability</a></h1>
<p>As we conclude our exploration of data quality dimensions and their critical role within the broader context of data reliability engineering, it's essential to recognize that data quality is not just a set of standards to be met. Instead, it's a basic building block that supports the reliability, trustworthiness, and overall value of data in driving business decisions, insights, and strategies.</p>
<h3 id="the-role-of-data-quality-in-data-reliability"><a class="header" href="#the-role-of-data-quality-in-data-reliability">The Role of Data Quality in Data Reliability</a></h3>
<p>Data reliability depends on the consistent delivery of accurate, complete, and timely data. The dimensions of data quality, such as accuracy, completeness, consistency, timeliness, and others discussed in this chapter, serve as pillars that uphold the reliability of data. Ensuring high standards across these dimensions means that data can be trusted as a reliable asset for operational and analytical purposes.</p>
<h3 id="data-anomalies-and-their-impact-on-reliability"><a class="header" href="#data-anomalies-and-their-impact-on-reliability">Data Anomalies and Their Impact on Reliability</a></h3>
<p>Data anomalies, which may arise from inconsistencies, inaccuracies, or incomplete data, can significantly undermine data reliability. They can lead to faulty analyses, misguided business decisions, and diminished trust in data systems. Proactive measures to detect and rectify anomalies are crucial in maintaining the integrity and reliability of data.</p>
<h3 id="data-quality-in-data-integration-and-migration"><a class="header" href="#data-quality-in-data-integration-and-migration">Data Quality in Data Integration and Migration</a></h3>
<p>The integration and migration of data present critical moments where data quality must be rigorously managed to preserve data reliability. Ensuring that data remains valid, unique, and consistent across systems is super important, especially when consolidating data from disparate sources into a unified data lake, data warehouse, or data mart.</p>
<h3 id="the-influence-of-data-architecture-on-data-quality"><a class="header" href="#the-influence-of-data-architecture-on-data-quality">The Influence of Data Architecture on Data Quality</a></h3>
<p>The underlying data architecture plays a huge role in facilitating data quality. A well-designed architecture that supports robust data management practices, including effective data governance and metadata management, sets the foundation for high-quality, reliable data.</p>
<h3 id="role-of-metadata-in-data-quality-and-reliability"><a class="header" href="#role-of-metadata-in-data-quality-and-reliability">Role of Metadata in Data Quality and Reliability</a></h3>
<p>Metadata provides essential context that enhances the quality and reliability of data by offering insights into its origin, structure, and usage. Effective metadata management ensures that data is accurately described, classified, and easily discoverable, contributing to its overall quality and reliability.</p>
<h3 id="addressing-data-quality-at-the-source"><a class="header" href="#addressing-data-quality-at-the-source">Addressing Data Quality at the Source</a></h3>
<p>Proactive strategies that address data quality issues at the source are among the most effective. Implementing strict data entry checks, validation rules, and early anomaly detection can significantly reduce the downstream impact of data quality issues, enhancing data reliability.</p>
<h3 id="data-reliability-engineering--data-quality"><a class="header" href="#data-reliability-engineering--data-quality">Data Reliability Engineering &amp; Data Quality</a></h3>
<p>In this chapter, we mostly explored how data quality impacts the data reliability engineering, but the opposite is also true, the stability and dependability of technical systems and processes are critical for maintaining high data quality. If these technical aspects are not reliable, they can introduce errors and delays, directly affecting the accuracy, completeness, and timeliness of the data. This makes ensuring the smooth operation of data infrastructure essential for preserving the quality of data, highlighting the interconnectedness between technical reliability and data quality in supporting effective data management and utilization.</p>
<h3 id="final-thoughts"><a class="header" href="#final-thoughts">Final Thoughts</a></h3>
<p>In the diverse landscape of industries, company sizes, and data infrastructures, the relevance and applicability of specific data quality dimensions and metrics can vary widely. Each organization must tailor its approach to data quality, considering its unique context, requirements, and challenges. Not all dimensions may be equally relevant, and additional, industry-specific metrics may be necessary to fully capture the nuances of data quality within a particular domain.</p>
<p>Embracing a holistic view of data quality, one that integrates seamlessly with the principles of data reliability engineering, enables organizations to not only address data quality reactively but to embed quality and reliability into the very fabric of their data management practices. This proactive stance on data quality ensures that data remains a true, reliable asset that can support the organization's goals, drive innovation, and deliver lasting value in an increasingly data-driven world.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="confiabilidad-de-datos"><a class="header" href="#confiabilidad-de-datos">Confiabilidad de Datos</a></h1>
<blockquote>
<p>Tomándose por base los fundamentos y metodologías desarrolladas por los <strong>Site Reliability Engineers</strong>, conocidos como los “firefighters” del mundo de la ingeniería de sistemas, los cuales construyen sistemas automatizados para optimizar la disponibilidad de las aplicaciones (reducir downtime), definiremos <strong>Data Reliability</strong> cómo la capacidad del equipo de data en entregar alta disponibilidad de la data durante todo el ciclo de vida de la misma. En resumen, garantizar los periodos de tiempo que la data no presenta inacurácia, no es faltante ni errónea.</p>
</blockquote>
<p>...</p>
<h2 id="consecuencias-de-la-confiabilidad"><a class="header" href="#consecuencias-de-la-confiabilidad">Consecuencias de la confiabilidad</a></h2>
<p>Consecuencias (en administrar el downtime de la data):</p>
<ul>
<li>Los equipos de data reducen de manera muy importante el tiempo perdido en “apagar incendios”, escalaciones y troubleshooting de la data. Utilizan ese tiempo para enfocarse en la construcción de una buena infraestructura, y en agregar valor a la data.</li>
<li>Los equipos de data son más rápidos en actualizar y modificar la infraestructura de la data, ya que tienen claro la confiabilidad del sistema.</li>
<li>Los equipos de data ganan el respeto y la confianza de los stakeholders, ya que entregan datos confiables de manera consistente.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="data-architecture-1"><a class="header" href="#data-architecture-1">Data Architecture</a></h1>
<blockquote>
<p>Data Architecture is about how the data is managed, from <strong>collection</strong>, <strong>transformations</strong>, <strong>distribution</strong>, and <strong>consumption</strong>.</p>
</blockquote>
<p>It includes the models, policies, rules, and standards that govern which data is collected and how it is stored, arranged, integrated, and put to use in data systems and in organizations.</p>
<p>The data architecture aims to set standards to all data systems, and the interaction between them. It also dictates and materialises the organization understanding of its business in a Conceptual, Logical, and Physical level.</p>
<p>The Zachman Framework for enterprise architecture, understands the data architecture in five layers:</p>
<ol>
<li>Scope/Contextual: subjects and architectural standards important for the business.</li>
<li>Business/Conceptual: business entities, its attributes and associations.</li>
<li>System/Logical: how entities are related.</li>
<li>Technology/Physical: representation of a data design as implemented in a database management system.</li>
<li>Detailed Representations: databases.</li>
</ol>
<h2 id="conceptual-layer"><a class="header" href="#conceptual-layer">Conceptual Layer</a></h2>
<blockquote>
<p>Represents all <strong>Business Entities</strong>.</p>
</blockquote>
<p>The <strong>Conceptual Data Model</strong> (CDM) consists of the <strong>Business Entities</strong>, like <strong>User</strong>, <strong>Branch</strong>, <strong>Product</strong>.
The business entities (or business objects) carry <em>attributes</em> (name, identifiers, timestamps, etc.), and <em>associations</em> (relationships) with other business entities.
The complete set of business entities represents the business relationships.</p>
<p>From a data architecture perspective, these business entities are represented in a <strong>Conceptual Schema</strong>, which consists of a map of <strong>concepts</strong> (business entities) and their <strong>relationships</strong> in a database, normally in a <strong>Data Structure Diagram</strong> (DSD). It may also include <strong>Enterprise Data Modelling</strong> (EDM) outputs like  entity–relationship diagrams (ERDs), XML schemas (XSD), and an enterprise wide data dictionary.</p>
<p>The conceptual schema describes the semantics of an organization, and represents a series of assertions about its nature. It describes the objects of significance (business entities), of which the organization is ineterested in collecting information of, its characteristics (attributes), and the associations between each pair of objects of significance (relationships).
Please note that it's not the actual database design, and it is represented in different abrastraction layers.</p>
<p>Examples:</p>
<ul>
<li>Each ORDER must be from one and only one USER.</li>
<li>Each ORDER contains one or more PRODUCTS.</li>
<li>Each ORDER contains products from one or more BRANCHES.</li>
</ul>
<h2 id="logical-layer"><a class="header" href="#logical-layer">Logical Layer</a></h2>
<blockquote>
<p>Represents the logic and how the entities are related.</p>
</blockquote>
<p>The <strong>Logical Data Model</strong> (LDM), also known as Domain Model, represents the abstract structure of a domain of information, expressed independently of a particular database management product or storage technology (physical data model), but in terms of data structures such as relational tables and columns, object-oriented classes, or XML tags.</p>
<p>Once validated and approved, the logical data model can become the basis of a physical data model and form the design of a database.</p>
<h2 id="physical-layer"><a class="header" href="#physical-layer">Physical Layer</a></h2>
<blockquote>
<p>The representation of a data design as implemented, or intended to be implemented, in a database management system.</p>
</blockquote>
<p>The <strong>Physical Data Model</strong> (PDM) typically derives from a logical data model (LDM), though it may be reverse-engineered from a given database implementation. A complete physical data model will include all the database artifacts required to create relationships between tables or to achieve performance goals, such as indexes, constraint definitions, linking tables, partitioned tables or clusters.</p>
<h3 id="cdm-vs-ldm-vs-pdm"><a class="header" href="#cdm-vs-ldm-vs-pdm">CDM vs LDM vs PDM</a></h3>
<h4 id="data-constructs"><a class="header" href="#data-constructs">Data Constructs</a></h4>
<ul>
<li>CDM: uses general high-level data constructs from which Architectural Descriptions are created in non-technical terms.</li>
<li>LDM: includes entities (tables), attributes (columns/fields) and relationships (keys). Is independent of technology (platform, DBMS).</li>
<li>PDM:  includes tables, columns, keys, data types, validation rules, database triggers, stored procedures, domains, and access constraints, as primary keys and indices for fast data access.</li>
</ul>
<h4 id="naming-conventions"><a class="header" href="#naming-conventions">Naming Conventions</a></h4>
<ul>
<li>CDM: non-technical names, so that executives and managers at all levels can understand the data basis of Architectural Description.</li>
<li>LDM: uses business names for entities &amp; attributes.</li>
<li>PDM: uses more defined and less generic specific names for tables and columns, such as abbreviated column names, limited by the database management system (DBMS) and any company defined standards.</li>
</ul>
<h2 id="modern-data-architecture"><a class="header" href="#modern-data-architecture">Modern Data Architecture</a></h2>
<p>A modern approach to data architecture, extensivelly adapted by StartUps, reduces, restricts or understands the data architecture as the implementation of a Data Warehouse, a Data Lake + Data Warehouse, or a Data Lakehouse architecture, consisting of the data sources plus two or three tiers:</p>
<ul>
<li>Bottom/Data Tier: data warehouse server with functional gateway (ODBC, JDBC, etc.). May also include the data lake.</li>
<li>Middle/Application Tier: houses the business logic used to process user inputs (OLAP Servers, Snowflake, Apache Redshift, Databricks Data Lakehouse Platform, Apache Spark, etc.).</li>
<li>Top/Presentation Tier: front-end tools (Power BI, Tableau, etc.).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="data-lake"><a class="header" href="#data-lake">Data Lake</a></h1>
<blockquote>
<p>A Data Lake is a data repository for storing large amounts of Structured, Semi-Structured, and Unstructured data.</p>
</blockquote>
<p>It is a repository for storing all types of data in its native format without fixed limits on account size or file. Data Lake stores a high data quantity to increase native integration and analytic performance.
The Data Lake democratizes data and provides a cost-effective way of storing all organization data for later processing.</p>
<ul>
<li><a href="concepts/data_lake.html#data-lake-vs-data-warehouse">Data Lake vs Data Warehouse</a></li>
<li><a href="concepts/data_lake.html#goals">Goals</a></li>
<li><a href="concepts/data_lake.html#data-lake-architecture">Data Lake Architecture</a>
<ul>
<li><a href="concepts/data_lake.html#layers">Layers</a>
<ul>
<li><a href="concepts/data_lake.html#ingestion-layer-bronze">Ingestion Layer (Bronze)</a></li>
<li><a href="concepts/data_lake.html#distillation-layer-silver">Distillation Layer (Silver)</a></li>
<li><a href="concepts/data_lake.html#processing-layer-gold">Processing Layer (Gold)</a></li>
<li><a href="concepts/data_lake.html#insights-layer">Insights Layer</a></li>
<li><a href="concepts/data_lake.html#unified-operations-layer">Unified Operations Layer</a></li>
</ul>
</li>
<li><a href="concepts/data_lake.html#zones">Zones</a>
<ul>
<li><a href="concepts/data_lake.html#landing-zone">Landing Zone</a></li>
<li><a href="concepts/data_lake.html#raw-zone">Raw Zone</a></li>
<li><a href="concepts/data_lake.html#harmonized-zone">Harmonized Zone</a></li>
<li><a href="concepts/data_lake.html#distilled-zone">Distilled Zone</a></li>
<li><a href="concepts/data_lake.html#explorative-zone">Explorative Zone</a></li>
<li><a href="concepts/data_lake.html#delivery-zone">Delivery Zone</a></li>
<li><a href="concepts/data_lake.html#zones-comparisom">Zones Comparisom</a></li>
</ul>
</li>
<li><a href="concepts/data_lake.html#sandbox">Sandbox</a></li>
<li><a href="concepts/data_lake.html#maturity-stages">Maturity Stages</a>
<ul>
<li><a href="concepts/data_lake.html#handle-and-ingest-data-at-scale">Handle and Ingest data at scale</a></li>
<li><a href="concepts/data_lake.html#building-the-analytical-muscle">Building the analytical muscle</a></li>
<li><a href="concepts/data_lake.html#data-warehouse-and-data-lake-work-in-unison">Data Warehouse and Data Lake work in unison</a></li>
<li><a href="concepts/data_lake.html#enterprise-capability-in-the-lake">Enterprise capability in the lake</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="concepts/data_lake.html#key-components-of-data-lake-architecture">Key Components of Data Lake Architecture</a>
<ul>
<li><a href="concepts/data_lake.html#data-ingestion">Data Ingestion</a></li>
<li><a href="concepts/data_lake.html#data-storage">Data Storage</a></li>
<li><a href="concepts/data_lake.html#data-governance">Data Governance</a></li>
<li><a href="concepts/data_lake.html#security">Security</a></li>
<li><a href="concepts/data_lake.html#data-quality">Data Quality</a></li>
<li><a href="concepts/data_lake.html#data-discovery">Data Discovery</a></li>
<li><a href="concepts/data_lake.html#data-auditing">Data Auditing</a></li>
<li><a href="concepts/data_lake.html#data-lineage">Data Lineage</a></li>
<li><a href="concepts/data_lake.html#data-exploration">Data Exploration</a></li>
</ul>
</li>
<li><a href="concepts/data_lake.html#other-architecture-approaches">Other Architecture Approaches</a></li>
</ul>
<h2 id="data-lake-vs-data-warehouse"><a class="header" href="#data-lake-vs-data-warehouse">Data Lake vs Data Warehouse</a></h2>
<blockquote>
<p>A Data Warehouse is a repository that exclusively keeps pre-processed data from a Data Lake and many databases.</p>
</blockquote>
<p>Data Warehouses store data in a hierarchical format using files and folders. This is not the case with a Data Lake as it has flat architecture. In a Data Lake, every data element is identified by a unique identifier and a set of metadata information.</p>
<h2 id="goals"><a class="header" href="#goals">Goals</a></h2>
<blockquote>
<p>Building and maintaining a Data Lake have five main goals: unifying the data, full query access, performance and scalability, progression, and costs.</p>
</blockquote>
<p><strong>Unification</strong>: Data Lake is a perfect solution to accumulate all the data from distinct data sources (ERP, CRM, logs, data partners data, internal generated data) in one place. The Data Lake architecture makes it easier for companies to get a holistic view of data and generate insights from it.</p>
<p><strong>Full Query Access</strong>: storing data in Data Lakes allows full access to data that can be directly used by BI tools to pull data whenever needed. ELT process is a flexible, reliable, and fast way to load data into Data Lake and then use it with other tools.</p>
<p><strong>Performance and Scalability</strong>: Data Lake Architecture supports fast query processing. It enables users to perform ad hoc analytical queries independent of the production environment. Data Lake provides faster querying and makes it easier to scale up and down. Data Lake offer business agility.</p>
<p><strong>Progression</strong>: getting data in one place is a necessary step before progressing to other stages because loading data from one source makes it easier to work with BI tools. Data Lake helps you make data cleaner and error-free data that has less repetition.</p>
<p><strong>Costs</strong>: S3 repositories are a cost-efficient storage of large volumes of data.</p>
<h2 id="data-lake-architecture"><a class="header" href="#data-lake-architecture">Data Lake Architecture</a></h2>
<div style="position: relative; width: 100%; height: 0; padding-top: 50.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbUxswrb4&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbUxswrb4&#x2F;view?utm_content=DAFbUxswrb4&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Data Lake Architecture Framework.</p></a>
<blockquote>
<p>Data Lakes are often structured in zones or layers models. These models define in which processing degrees (raw, cleansed, aggregated) data are available in the data lake, and how they are governed (regarding access rights, data quality, and responsibilities).</p>
</blockquote>
<p>Zones are similar to the layers in data warehousing, but data may not move through all zones or even move back.</p>
<h3 id="layers"><a class="header" href="#layers">Layers</a></h3>
<div style="position: relative; width: 100%; height: 0; padding-top: 35.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbVHIphys&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbVHIphys&#x2F;view?utm_content=DAFbVHIphys&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Data Lake Layers.</p></a>
<p>The following data lake model approach is structured in layers:</p>
<ul>
<li>Ingestion Layer</li>
<li>Distilation Layer</li>
<li>Processing Layer</li>
<li>Insights Layer</li>
<li>Unified Operations Layer</li>
</ul>
<p>The <strong>Raw Data</strong> entering the Data Lake consists of the organizations internal data (Operational Systems), specially relational data from databases, also streaming and batch data from data partners.</p>
<p>In the other extreme, representing the data leaving the Data Lake, the <strong>Business Systems</strong>, consists of databases, the Data Warehouse, dashboards, reports, and external data connections.</p>
<p>The first three layers constitute the medallion architecture, which is a is a data design pattern used to logically organize data in a Data Lake (similar as in a Data Lakehouse), with the goal of incrementally and progressively improving the structure and quality of data as it flows through each layer of the architecture (from Bronze ⇒ Silver ⇒ Gold layer tables).
Medallion architectures are sometimes also referred to as "multi-hop" architectures.</p>
<p>You can see more details regarding this architecture approach in the <a href="https://www.researchgate.net/profile/Sidharth-S-Prakash/publication/343219651_Evolution_of_Data_Warehouses_to_Data_Lakes_for_Enterprise_Business_Intelligence/links/5f1d52ad92851cd5fa48958a/Evolution-of-Data-Warehouses-to-Data-Lakes-for-Enterprise-Business-Intelligence.pdf">article</a>{{Prakash, S. S. (2020). Evolution of Data Warehouses to Data Lakes for Enterprise Business Intelligence. Evolution, 8(4).}} "Evolution of Data Warehouses to Data Lakes for Enterprise Business Intelligence".</p>
<h4 id="ingestion-layer-bronze"><a class="header" href="#ingestion-layer-bronze">Ingestion Layer (Bronze)</a></h4>
<blockquote>
<p>The purpose of the Ingestion Layer of the Data Lake Architecture is to ingest raw data into the Data Lake. There is no data modification in this layer. This is where we land all the data from external source systems.</p>
</blockquote>
<p>The table structures in this layer correspond to the source system table structures "as-is," along with any additional metadata columns that capture the load date/time, process ID, etc.
The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.</p>
<p>The layer can ingest raw data in real-time or in batches, which is in turn organized into a logical folder structure.
The Ingestion Layer can pull data from different external sources, like social media platforms.</p>
<h4 id="distillation-layer-silver"><a class="header" href="#distillation-layer-silver">Distillation Layer (Silver)</a></h4>
<blockquote>
<p>The purpose of the Distillation Layer of the Data Lake Architecture is to convert the data stored in the Ingestion (Bronze) Layer in a Structured format for analytics.</p>
</blockquote>
<p>The data is matched, denormalized, merged, conformed, cleansed, and derive "just-enough" so that the Silver layer can provide an "Enterprise view" of all its key business entities, concepts and transactions (for example, master customers, stores, non-duplicated transactions and cross-reference tables).
The data in this layer becomes uniform in terms of format, encoding, and data type (parquet).</p>
<p>The Silver layer brings the data from different sources into an Enterprise view and enables self-service analytics for ad-hoc reporting, advanced analytics and ML.
It serves as a source for Departmental Analysts, Data Engineers and Data Scientists to further create projects and analysis to answer business problems via enterprise and departmental data projects in the Gold Layer.</p>
<p>In the lakehouse data engineering paradigm (of which we’re extending to the Data Lake), typically the ELT methodology is followed vs. ETL - which means only minimal or "just-enough" transformations and data cleansing rules are applied while loading the Silver layer.
Speed and agility to ingest and deliver the data in the data lake is prioritized, and a lot of project-specific complex transformations and business rules are applied while loading the data from the Silver to Gold layer.
From a data modeling perspective, the Silver Layer has more 3rd-Normal Form like data models. Data Vault-like, write-performant data models can be used in this layer.</p>
<h4 id="processing-layer-gold"><a class="header" href="#processing-layer-gold">Processing Layer (Gold)</a></h4>
<blockquote>
<p>This layer of the Data Lake Architecture executes user queries and advanced analytical tools on the Structured Data.</p>
</blockquote>
<p>The processes can be run in batch, in real-time, or interactively.
It is the layer that implements the business logic and analytical applications consume the data.
It is also known as the Trusted, Gold, or Production-Ready Layer.</p>
<p>It is typically organized in consumption-ready "project-specific" databases.
The Gold layer is for reporting and uses more de-normalized and read-optimized data models with fewer joins.
The final layer of data transformations and data quality rules are applied here.
Final presentation layer of projects such as Customer Analytics, Product Quality Analytics, Inventory Analytics, Customer Segmentation, Product Recommendations, Marking/Sales Analytics etc. fit in this layer.
We see a lot of Kimball style star schema-based data models or Inmon style Data marts fit.</p>
<p>Often, the Data Marts (and Data Warehouse data) from the traditional RDBMS technology stack are ingested into the Gold layer.</p>
<h4 id="insights-layer"><a class="header" href="#insights-layer">Insights Layer</a></h4>
<blockquote>
<p>This layer of the Data Lake Architecture acts as the query interface, or the output interface, of the Data Lake.</p>
</blockquote>
<p>It uses SQL and NoSQL queries to request or fetch data from the Data Lake.
The queries are normally executed by company users who need access to the data.
Once the data is fetched from the Data Lake, it is the same layer that displays it to the user for viewing.</p>
<p>Some examples include Amazon QuickSight, an AWS native BI tool and allows users to connect with software-as-a-service (SaaS) applications such as Salesforce or ServiceNow, third-party databases such as MySQL, Postgres, and SQL Server, as well as native AWS services including Amazon Athena, an interactive query service that allows them to analyze unstructured data in Amazon S3 data lakes using standard SQL queries.
While QuickSight doesn’t connect directly to the data lake, integration with Amazon Athena allows BI users to query data inside the lake without having to move data or build an ETL pipeline.</p>
<h4 id="unified-operations-layer"><a class="header" href="#unified-operations-layer">Unified Operations Layer</a></h4>
<blockquote>
<p>This layer governs system management and monitoring.</p>
</blockquote>
<p>It includes auditing and proficiency management, data management, workflow management. AWS data lake environments and monitoring tools and best practices are described in this <a href="https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/monitoring-optimizing-data-lake-environment.html">article</a><sup><a name="to-footnote-1"><a href="concepts/data_lake.html#footnote-1">1</a></a></sup> from AWS.</p>
<h3 id="zones"><a class="header" href="#zones">Zones</a></h3>
<div style="position: relative; width: 100%; height: 0; padding-top: 35.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbVKs6Or4&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbVKs6Or4&#x2F;view?utm_content=DAFbVKs6Or4&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Zone Reference Model for Enterprise-Grade Data Lake Management - Data Flow.</p></a>
<p>The following data lake approach is explored by the <a href="https://www.ipvs.uni-stuttgart.de/departments/as/publications/giebleca/20_zoneReferenceModel_EDOC_Preprint.pdf">University of Stuttgart and Bosch GmbH</a><sup><a name="to-footnote-2"><a href="concepts/data_lake.html#footnote-2">2</a></a></sup>, and known as Zone Reference Model for Enterprise-Grade Data Lake Management. It consists of:</p>
<ul>
<li>Landing Zone</li>
<li>Raw Zone</li>
<li>Harmonized Zone</li>
<li>Distilled Zone</li>
<li>Delivery Zone</li>
<li>Explorative Zone</li>
</ul>
<p><img src="concepts/../mdbook-plantuml-img/b510a51550cb2a65e78ebfbb460618d52bd15e92.svg" alt="" /></p>
<p style="text-align: center;">Zone Reference Model for Enterprise-Grade Data Lake Management - Meta-model for zones - Attributes.</p>
<p>Regarding how a zone interacts with other zones and the outside world, we have:</p>
<ul>
<li><strong>Zone</strong> <em>receives</em> data from another <strong>Zone</strong></li>
<li><strong>Zone</strong> <em>forwards</em> data to another <strong>Zone</strong></li>
<li><strong>Zone</strong> <em>imports</em> data from <strong>Data Source</strong></li>
<li><strong>Zone</strong> <em>exports</em> data to <strong>Data Sink</strong></li>
</ul>
<p>All zones contain a protected part.
This part is encrypted and secured, and stores data that need extensive protection (for example, PII, personal data). Data wander from the protected part of one zone to the protected part of the next zone.
They may only leave the protected part after being desensitized (for example, by anonymization).
Data in this part are subject to strict access controls and governance.
The protected part shares all other characteristics with the rest of the zone it is in.</p>
<h4 id="landing-zone"><a class="header" href="#landing-zone">Landing Zone</a></h4>
<blockquote>
<p>The Landing Zone is the first zone of the data lake. Data are ingested as batch or as data stream from the sources.</p>
</blockquote>
<p>The Landing Zone is beneficial when the requirements of the ingested data and those of the Raw Zone diverge.
For example, data might need to be ingested at a vast rate due to its volume and velocity.
If the technical implementation of the Raw Zone cannot provide this high ingestion rate, a Landing Zone can function as a mediator in between: data are ingested at a high rate into the Landing Zone, and then are forwarded to the Raw Zone as batches.
Examples of data coming to the Landing zone include data streamed by Kafka, Amazon Kinesis, Amazon SQS, RabbitMQ, Apache Spark, etc.</p>
<p>For the data characteristics, data ingested into the Landing Zone remains mostly raw.
Their granularity remains raw, just like in the source systems.
The schema of the data is not changed; they can simply be copied in their source system format.
However, their syntax might be changed.
Basic transformations are allowed upon ingestion into the Landing Zone, such as adjusting the character set of strings or transforming timestamps into a common format.
In addition, data may be masked or anonymized to comply with legal regulations.
Aside from these changes, the semantic of the data remains the same as in the source systems.</p>
<h4 id="raw-zone"><a class="header" href="#raw-zone">Raw Zone</a></h4>
<blockquote>
<p>All data in the data lake is available in mostly raw format in the Raw Zone. Only basic transformations (see Landing Zone) are applied on the data. If the Landing Zone is omitted, these transformations are performed in the Raw Zone.</p>
</blockquote>
<p>Differently from the Landing Zone, the Raw Zone stores data persistently.
In general, data should neither be manipulated nor deleted from the Raw Zone.
This zone persists (when possible) the original data type (json, csv, xml).</p>
<h4 id="harmonized-zone"><a class="header" href="#harmonized-zone">Harmonized Zone</a></h4>
<blockquote>
<p>The Harmonized Zone is the place where master data are accessible for analyses.</p>
</blockquote>
<p>A subset of the data stored in the Raw Zone is passed to the Harmonized Zone in a demand-based manner.
It is important to note that these data are not deleted from the Raw Zone.
Instead, the Harmonized Zone contains a copy of or a view on the data in the Raw Zone.
The Harmonized Zone is also the place where master data are accessible for analyses.
As these data are crucial for enterprises, master data management is of high importance in the data lake.
Thus, they should exclusively be accessed after being cleansed.</p>
<p>The data characteristics in this zone differ greatly from those in the Raw Zone.
Data schema and syntax change when compared to the source data.
Data from different source systems are integrated into a consolidated schema, regardless of their structure.
The data syntax is also consolidated in the Harmonized Zone: when data from multiple source systems are merged, data types have to be adapted.</p>
<p>The aim of the Harmonized Zone is to provide a harmonized and consolidated view on data.
To this end, the Harmonized Zone uses a standardized modeling approach (dimensional modeling or Data Vault) that all of the enterprise’s data are modeled in.
The files in this zone facilitates the ingestion (parquet).</p>
<h4 id="distilled-zone"><a class="header" href="#distilled-zone">Distilled Zone</a></h4>
<blockquote>
<p>Prepares data for processing and facilitates ingestion.</p>
</blockquote>
<p>In contrast to the Raw and Harmonized Zone, where the focus is to quickly make data available for use, the Distilled Zone focuses on increasing the efficiency of following analyses by preparing the data accordingly.
The granularity of the data may be changed (for example, data may be aggregated for the calculation of KPIs).
Complex processing is applied that change the data’s semantics but are too extensive for the Landing Zone, Raw Zone, and Harmonized Zone.
However, the schema might also change slightly, depending on the supported use case (for example, fields to enrich the data could be added).
The files in this zone facilitates the ingestion (parquet).</p>
<h4 id="explorative-zone"><a class="header" href="#explorative-zone">Explorative Zone</a></h4>
<blockquote>
<p>The Explorative Zone is the place where data scientists can play with and flexibly use the data.</p>
</blockquote>
<p>Data scientists can use and explore data in the data lake in any way they desire, except for sensitive data. These data are only usable according to strict rules. Granularity, schema, syntax, and semantic may be changed in any way necessary for analyses.</p>
<h4 id="delivery-zone"><a class="header" href="#delivery-zone">Delivery Zone</a></h4>
<blockquote>
<p>In the Delivery Zone, small subsets of data are tailored to specific usage and applications.</p>
</blockquote>
<p>This does not only include analytical use cases, such as reporting and OLAP, but also operational use cases.
This zone thus provides functionality similar to data marts and operational data stores in data warehousing.
Data from this zone may be forwarded to external data sinks.</p>
<p>The Delivery Zone especially supports users with little knowledge on data analytics.
Data have to be easily findable and importable into various analytics tools.
As for the modeling approach, data are available in whatever format supports the intended use case best, for example, dimensional modeling for OLAP, or flat tables for operational use.</p>
<h4 id="zones-comparisom"><a class="header" href="#zones-comparisom">Zones Comparisom</a></h4>
<table>
    <tr>
        <td></td>
        <td><strong>Landing</strong></td>
        <td><strong>Raw</strong></td>
        <td><strong>Harmonized</strong></td>
        <td><strong>Distilled</strong></td>
        <td><strong>Explorative</strong></td>
        <td><strong>Delivery</strong></td>
    </tr>
    <tr>
        <td><strong>Granularity</strong></td>
        <td>Raw</td>
        <td>Raw</td>
        <td>Raw</td>
        <td>Aggregated</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
    <tr>
        <td><strong>Schema</strong></td>
        <td>Any</td>
        <td>Any</td>
        <td>Consolidated</td>
        <td>Consolidated, Enriched</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
    <tr>
        <td><strong>Syntax</strong></td>
        <td>Basic transformations</td>
        <td>Basic transformations</td>
        <td>Consolidated</td>
        <td>Consolidated</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
    <tr>
        <td><strong>Semantics</strong></td>
        <td>Mostly unchanged, unless needed for compliance</td>
        <td>Mostly unchanged, unless needed for compliance</td>
        <td>Mostly unchanged, unless needed for compliance</td>
        <td>Complex processing</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
    <tr>
        <td><strong>Governed</strong></td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>False</td>
        <td>True</td>
    </tr>
    <tr>
        <td><strong>Historized</strong></td>
        <td>False</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>N/A</td>
        <td>N/A</td>
    </tr>
    <tr>
        <td><strong>Persistent</strong></td>
        <td>False</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>False</td>
        <td>True</td>
    </tr>
    <tr>
        <td><strong>Has Protected Part</strong></td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
    </tr>
    <tr>
        <td><strong>Use Case Dependent</strong></td>
        <td>False</td>
        <td>False</td>
        <td>False</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
    </tr>
    <tr>
        <td><strong>User Groups</strong></td>
        <td>Systems, Processes</td>
        <td>Data Scientists, Systems, Processes</td>
        <td>Data Scientists, Systems, Processes</td>
        <td>Data Scientists, Domain Experts, Systems, Processes</td>
        <td>Data Scientists</td>
        <td>Anyone</td>
    </tr>
    <tr>
        <td><strong>Modelling Approach</strong></td>
        <td>Any</td>
        <td>Any</td>
        <td>Standardized</td>
        <td>Standardized</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
</table>
<p style="text-align: center;">Zone Reference Model for Enterprise-Grade Data Lake Management - Zones comparison.</p>
<h3 id="sandbox"><a class="header" href="#sandbox">Sandbox</a></h3>
<blockquote>
<p>Also known as the Analytics Sandbox, it provides data scientists and advanced analysts with a place for data exploration.</p>
</blockquote>
<p>An Analytics Sandbox is a separate environment that is part of the overall data lake architecture, meaning that it is a centralized environment meant to be used by multiple users and is maintained with the support of IT. Some key characteristics of this layer:</p>
<ul>
<li>The environment is controlled by the analyst</li>
<li>Allows them to install and use the data tools of their choice</li>
<li>Allows them to manage the scheduling and processing of the data assets</li>
<li>Enables analysts to explore and experiment with internal and external data</li>
<li>Can hold and process large amounts of data efficiently from many different data sources; big data (unstructured), transactional data (structured), web data, social media data, documents, etc.</li>
</ul>
<p>There are many advantages to having an Analytics Sandbox as part of your data architecture.
The most important is that it decreases the amount of time that it takes a business to gain knowledge and insight from their data.
It does this by providing an on-demand/always ready environment that allows analysts to quickly dive into and process large amounts of data and prototype their solutions without kicking off a big BI project.
In other words, it enables agile BI by empowering your advanced users.</p>
<p>Another major benefit to the business and IT team is that by giving the business a place to prototype their data solutions it allows the business to figure what they want on their own without involving IT.
When they decide that a solution is adding business value, it becomes a good candidate for something that should be productionized and built into the Data Warehouse process at some point.
This saves both teams a lot of time and effort.</p>
<h3 id="maturity-stages"><a class="header" href="#maturity-stages">Maturity Stages</a></h3>
<p>The implementation of a Data Lake solution consists of some main maturity stages.</p>
<ol>
<li>Handle and ingest data at scale</li>
<li>Building the analytical muscle</li>
<li>Data Warehouse and Data Lake working in unison</li>
<li>Enterprise capability</li>
</ol>
<h4 id="handle-and-ingest-data-at-scale"><a class="header" href="#handle-and-ingest-data-at-scale">Handle and Ingest data at scale</a></h4>
<blockquote>
<p>This stage consists in improving the ability to transform and analyze data.</p>
</blockquote>
<h4 id="building-the-analytical-muscle"><a class="header" href="#building-the-analytical-muscle">Building the analytical muscle</a></h4>
<blockquote>
<p>This stage involves improving the ability to transform and analyze data.</p>
</blockquote>
<p>In this stage, the company start acquiring more data and building applications.
In this stage, capabilities of the Data Warehouse and the Data Lake are used together.</p>
<h4 id="data-warehouse-and-data-lake-work-in-unison"><a class="header" href="#data-warehouse-and-data-lake-work-in-unison">Data Warehouse and Data Lake work in unison</a></h4>
<blockquote>
<p>This step involves getting data and analytics into the hands of as many people as possible.</p>
</blockquote>
<p>In this stage, the Data Lake and the Data Warehouse start to work in a union.
Both playing their part in analytics.</p>
<h4 id="enterprise-capability-in-the-lake"><a class="header" href="#enterprise-capability-in-the-lake">Enterprise capability in the lake</a></h4>
<blockquote>
<p>In this maturity stage of the data lake, enterprise capabilities are added to the Data Lake.</p>
</blockquote>
<p>It includes the adoption of information governance, information lifecycle management capabilities, and Metadata management.</p>
<h2 id="key-components-of-data-lake-architecture"><a class="header" href="#key-components-of-data-lake-architecture">Key Components of Data Lake Architecture</a></h2>
<p><img src="concepts/../mdbook-plantuml-img/2b8772bdc69bce03cd960127332a78ef0a2193ea.svg" alt="" /></p>
<p style="text-align: center;">Key Components of Data Lake Architecture.</p>
<h4 id="data-ingestion"><a class="header" href="#data-ingestion">Data Ingestion</a></h4>
<blockquote>
<p>Data Ingestion allows connectors to get data from a different data sources and load into the Data Lake.</p>
</blockquote>
<p>Data Ingestion supports:</p>
<ul>
<li>All types of Structured, Semi-Structured, and Unstructured data.</li>
<li>Multiple ingestions like Batch, Real-Time, One-time load.</li>
<li>Many types of data sources like Databases, Webservers, Emails, and FTP.</li>
</ul>
<h4 id="data-storage"><a class="header" href="#data-storage">Data Storage</a></h4>
<blockquote>
<p>Data storage should be scalable, offers cost-effective storage and allow fast access to data exploration. It should support various data formats.</p>
</blockquote>
<h4 id="data-governance"><a class="header" href="#data-governance">Data Governance</a></h4>
<blockquote>
<p>Data governance is a process of managing availability, usability, security, and integrity of data used in an organization.</p>
</blockquote>
<h4 id="security"><a class="header" href="#security">Security</a></h4>
<blockquote>
<p>Security needs to be implemented in every layer of the Data Lake. It starts with Storage, Unearthing, and Consumption. The basic need is to stop access for unauthorized users. It should support different tools to access data with easy to navigate GUI and Dashboards.</p>
</blockquote>
<p>Authentication, Accounting, Authorization and Data Protection are some important features of Data Lake security.</p>
<h4 id="data-quality-2"><a class="header" href="#data-quality-2">Data Quality</a></h4>
<blockquote>
<p>Data quality is an essential component of Data Lake architecture. Data is used to exact business value. Extracting insights from poor quality data will lead to poor quality insights.</p>
</blockquote>
<h4 id="data-discovery"><a class="header" href="#data-discovery">Data Discovery</a></h4>
<blockquote>
<p>Data Discovery is another important stage before you can begin preparing data or analysis. In this stage, tagging technique is used to express the data understanding, by organizing and interpreting the data ingested in the Data Lake.</p>
</blockquote>
<h4 id="data-auditing"><a class="header" href="#data-auditing">Data Auditing</a></h4>
<blockquote>
<p>Data auditing helps to evaluate risk and compliance.</p>
</blockquote>
<p>The main Data auditing tasks are:</p>
<ul>
<li>Tracking changes to important dataset elements</li>
<li>Captures how/when/who changes to these elements.</li>
</ul>
<h4 id="data-lineage-1"><a class="header" href="#data-lineage-1">Data Lineage</a></h4>
<blockquote>
<p>This component deals with data’s origins. It mainly deals with where it movers over time and what happens to it. It eases errors corrections in a data analytics process from origin to destination.</p>
</blockquote>
<h4 id="data-exploration"><a class="header" href="#data-exploration">Data Exploration</a></h4>
<blockquote>
<p>It is the beginning stage of data analysis. It helps to identify right dataset is vital before starting Data Exploration.</p>
</blockquote>
<p>All given components need to work together to play an important part in Data Lake building easily evolve and explore the environment.</p>
<h2 id="other-architecture-approaches"><a class="header" href="#other-architecture-approaches">Other Architecture Approaches</a></h2>
<p><strong>Data Lake Lambda Architecture for Smart Grids Big Data Analytics</strong>: relies on Lambda architecture that is capable of performing parallel batch and real-time operations on distributed data. See the <a href="https://ieeexplore.ieee.org/abstract/document/8417407">article</a>{{A. A. Munshi and Y. A. -R. I. Mohamed, "Data Lake Lambda Architecture for Smart Grids Big Data Analytics," in IEEE Access, vol. 6, pp. 40463-40471, 2018, doi: 10.1109/ACCESS.2018.2858256.}}. Also see a brief explantion of the Lambda Architecture in this <a href="https://www.researchgate.net/profile/Ajit-Singh-46/publication/331890045_Architecture_of_Data_Lake/links/6061ef85458515e8347d6ecc/Architecture-of-Data-Lake.pdf">article</a>{{Ajit Singh, "Architecture of Data Lake", International Journal of Scientific Research in Computer Science, Engineering and Information Technology (IJSRCSEIT), ISSN : 2456-3307, Volume 5 Issue 2, pp. 411-414, March-April 2019. Available at doi: https://doi.org/10.32628/CSEIT1952121}}.<p><hr/></p>
<p><a name="footnote-1"><a href="concepts/data_lake.html#to-footnote-1">1</a></a>: The article discuss data lake optimizations using AWS products, like CloudWatch, Macie, CloudTrail, and S3 Intelligent-Tiering.</p>
<p><a name="footnote-2"><a href="concepts/data_lake.html#to-footnote-2">2</a></a>: C. Giebler, C. Gröger, E. Hoos, H. Schwarz and B. Mitschang, "A Zone Reference Model for Enterprise-Grade Data Lake Management," 2020 IEEE 24th International Enterprise Distributed Object Computing Conference (EDOC), Eindhoven, Netherlands, 2020, pp. 57-66, doi: 10.1109/EDOC49727.2020.00017.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-warehouse"><a class="header" href="#data-warehouse">Data Warehouse</a></h1>
<blockquote>
<p>A <strong>Data Warehouse</strong> (DWH), also known as Enterprise Data Warehouse (EDW) is a central repository of information that can be analyzed to make more informed decisions.</p>
</blockquote>
<p>Data flows into a data warehouse from Data Lake, transactional systems, relational databases, and other sources, typically on a regular cadence. Business analysts, data engineers, data scientists, and decision makers access the data through Business Intelligence (BI) tools, SQL clients, and other analytics applications.</p>
<ul>
<li><a href="concepts/data_warehouse.html#goals">Goals</a></li>
<li><a href="concepts/data_warehouse.html#data-warehouse-architecture">Data Warehouse Architecture</a>
<ul>
<li><a href="concepts/data_warehouse.html#data-sources">Data Sources</a></li>
<li><a href="concepts/data_warehouse.html#warehouse">Warehouse</a>
<ul>
<li><a href="concepts/data_warehouse.html#metadata">Metadata</a></li>
<li><a href="concepts/data_warehouse.html#summarized-data">Summarized data</a></li>
</ul>
</li>
<li><a href="concepts/data_warehouse.html#end-user-access-tools">End-User access Tools</a></li>
<li><a href="concepts/data_warehouse.html#two-tier-vs-three-tier-architecture">Two-Tier vs Three-Tier Architecture</a></li>
</ul>
</li>
<li><a href="concepts/data_warehouse.html#data-modeling-methodologies">Data Modeling Methodologies</a></li>
<li><a href="concepts/data_warehouse.html#maturity-stages">Maturity Stages</a></li>
<li><a href="concepts/data_warehouse.html#key-components-of-a-data-warehouse">Key Components of a Data Warehouse</a></li>
</ul>
<h2 id="goals-1"><a class="header" href="#goals-1">Goals</a></h2>
<blockquote>
<p>When implementing a data warehouse, the main goals are to achieve: consistency, enable data-driven decision-making and improvement, and to maintain data Single Source of Truth.</p>
</blockquote>
<p><strong>Consistency</strong>: to maintain a uniform format to all collected data, making it easier for corporate decision-makers to analyze and share data insights with their colleagues. Standardizing data from different sources also reduces the risk of error in interpretation and improves overall accuracy.</p>
<p><strong>Decision-making</strong>: successful business leaders develop data-driven strategies and rarely make decisions without consulting the facts. Data warehousing improves the speed and efficiency of accessing different data sets and makes it easier for corporate decision-makers to derive insights that will guide the business and marketing strategies that set them apart from their competitors.</p>
<p><strong>Improving</strong>: allow business leaders to quickly access the organization historical activities and evaluate initiatives that have been successful — or unsuccessful — in the past. This allows executives to see where they can adjust their strategy to decrease costs, maximize efficiency and increase business results.</p>
<p><strong>Single Source of Truth</strong>: the whole organization would benefit on having a single source of truth, specially when there are multiple data sources to a common business dimension.</p>
<h2 id="data-warehouse-architecture"><a class="header" href="#data-warehouse-architecture">Data Warehouse Architecture</a></h2>
<p><img src="concepts/../mdbook-plantuml-img/85627189967d2e5f9857b7b50dfac44b17456eb7.svg" alt="" /></p>
<p style="text-align: center;">Data Warehouse Solution.</p>
<p>There are several data warehouses architecture approaches available. Data warehouses would have in common some key components:</p>
<h3 id="data-sources"><a class="header" href="#data-sources">Data Sources</a></h3>
<blockquote>
<p>In most architectures approaches, it’s the Source Layer, or Data Source Layer, and consists on all the data sources the Warehouse Layer will consume.</p>
</blockquote>
<p><strong>Operational System</strong>: is a method used in data warehousing to refer to a system that is used to process the day-to-day transactions of an organization. Physically, it will normally refer to the databases the organization applications and micro-services create.</p>
<p><strong>Flat Files System</strong>: is a system of files in which transactional data is stored, and every file in the system must have a different name.</p>
<h3 id="warehouse"><a class="header" href="#warehouse">Warehouse</a></h3>
<blockquote>
<p>In most architectures approaches, it’s the Warehouse Layer, or Data Warehouse Layer, and consists on all the data stored in RDBMS database with available gateway access (ODBC, JDBC, etc.). It also contains the metadata, and some degree of data summarization, and business logic applied, which differentiate an <strong>DWH database</strong> from a <strong>Production database</strong>.</p>
</blockquote>
<h4 id="metadata"><a class="header" href="#metadata">Metadata</a></h4>
<blockquote>
<p>Metadata is the road-map to a data warehouse, it defines the warehouse objects, and acts as a directory. This directory helps the decision support system to locate the contents of a data warehouse.</p>
</blockquote>
<p>It normally contains:</p>
<ol>
<li>A description of the Data Warehouse structure, including the warehouse schema, dimensions, hierarchies, data mart locations, contents, etc.</li>
<li>Operational metadata, which usually describes the currency level of the stored data (for example, active, archived or purged), and warehouse monitoring information (for example, usage statistics, error reports, audit, etc).</li>
<li>System performance data, which includes indices, used to improve data access and retrieval performance.</li>
<li>Information about the mapping from operational databases, which provides source RDBMSs and their contents, cleaning and transformation rules, etc.</li>
<li>Summarization algorithms, predefined queries, and reports business data, which include business terms and definitions, ownership information, etc.</li>
</ol>
<p>Metadata management tool examples are Datahub, Open Metadata, and Amundsen.</p>
<h4 id="summarized-data"><a class="header" href="#summarized-data">Summarized data</a></h4>
<blockquote>
<p>The area of the data warehouse that maintains all the predefined lightly and highly summarized (aggregated) data. The main goal is to speed up query performance, and the summarized records are updated continuously as new information is loaded into the warehouse.</p>
</blockquote>
<h3 id="end-user-access-tools"><a class="header" href="#end-user-access-tools">End-User access Tools</a></h3>
<blockquote>
<p>The main purpose of a data warehouse is to provide information to the business for strategic decision-making. These end-users interact with the warehouse using end-client access tools.</p>
</blockquote>
<p>The examples of some of the end-user access tools can be:</p>
<ul>
<li>Reporting and Query Tools</li>
<li>Application Development Tools</li>
<li>Executive Information Systems Tools</li>
<li>Online Analytical Processing Tools</li>
<li>Data Mining Tools</li>
</ul>
<h3 id="two-tier-vs-three-tier-architecture"><a class="header" href="#two-tier-vs-three-tier-architecture">Two-Tier vs Three-Tier Architecture</a></h3>
<p>The data warehouse will normally be designed in a Two-Tier or in a Three-Tier architecture approach. The details of which one will be explored in the chapter <a href="concepts/./data_warehouse_tier_architecture.html">Data Warehouse Tier Architecture</a>.</p>
<p>In short, the tiers are:</p>
<ol>
<li>Bottom/Data Tier: data warehouse server with functional gateway (ODBC, JDBC, etc.).</li>
<li>Middle/Application Tier: houses the business logic used to process user inputs. Example: OLAP Servers, Snowflake, Apache Redshift, Databricks Data Lakehouse Platform, Apache Spark.</li>
<li>Top/Presentation Tier: front-end tools.</li>
</ol>
<h2 id="data-modeling-methodologies"><a class="header" href="#data-modeling-methodologies">Data Modeling Methodologies</a></h2>
<blockquote>
<p>Being one of the most important topics of data warehouse design and architecture, the data modeling methodology choosing process is arduous and polemic, and will impact the whole design and implementation of the data warehouse solution.</p>
</blockquote>
<p>Specially for startups, the first versions or iterations of a data solution (implemented before the organization even start discussing the implementation of a data warehouse solution) will be very similar to a <strong>Kimball</strong> (or Bottom-up) methodology approach, though not planned explicitly as such. Which means the data marts (or the data to be accessed by the first BI tools adopted in the organization) are first formed based on the business requirements.</p>
<p>There are lots of advantages and disadvantages of priming this approach over a top-down approach (or any of the hybrid or alternative methodologies).</p>
<p>See all the details of the different data modelling methodologies in the chapter <a href="concepts/./data_modelling.html">Data Modelling</a>. See also the implementation of a three-tier data warehouse architecture in the <a href="https://iopscience.iop.org/article/10.1088/1757-899X/306/1/012061/pdf">paper</a>{{Tangkawarow, I. R. H. T., Runtuwene, J. P. A., Sangkop, F. I., &amp; Ngantung, L. V. F. (2018, February). Three Tier-Level Architecture Data Warehouse Design of Civil Servant Data in Minahasa Regency. In IOP Conference Series: Materials Science and Engineering (Vol. 306, No. 1, p. 012061). IOP Publishing.}} "Three Tier-Level Architecture Data Warehouse Design of Civil Servant Data in Minahasa Regency".</p>
<h2 id="maturity-stages-1"><a class="header" href="#maturity-stages-1">Maturity Stages</a></h2>
<p>#TODO</p>
<h2 id="key-components-of-a-data-warehouse"><a class="header" href="#key-components-of-a-data-warehouse">Key Components of a Data Warehouse</a></h2>
<p><strong>Data Ingestion</strong>: allows connectors to get data from a different data sources and load into the Data Warehouse. The data will normally come from the Data Lake and External Sources connection (Fivetran), through multiple ETLs (Airflow, services, apps, ETL tools and platforms, etc.).</p>
<p><strong>Data Storage</strong>: the data is stored in the data warehouse database, a relational database (RDBMS), like Postgres.</p>
<p><strong>Data Governance</strong>: is a process of managing availability, usability, security, and integrity of data used in an organization.</p>
<p><strong>Security</strong>: it needs to be implemented in every layer of the Data Warehouse. It includes setting up the data warehouse read-only by default, and setting up custom User Groups. It also includes the access to the databases (VPCs, VPNs, Whitelisting, etc.), strong and active DevOps monitoring and the enforcing of best practices in all levels of the data warehouse environment (data ingestion, data marts consumption, ETLs design, etc.).</p>
<p><strong>Data Quality</strong>: it is an essential component of Data Warehouse architecture. Data is used to exact business value. Extracting insights from poor quality data will lead to poor quality insights.</p>
<p><strong>Data Discovery</strong>: it is another important stage before you can begin preparing data or analysis. All this rely on good metadata, and data modeling.</p>
<p><strong>Data Auditing</strong>: it helps to evaluate risk and compliance. Two major Data auditing tasks are tracking changes to the key dataset.</p>
<ul>
<li>Tracking changes to important dataset elements.</li>
<li>Captures how/when/who changes to these elements.</li>
</ul>
<p><strong>Data Lineage</strong>: it deals with data’s origins. It mainly deals with where it movers over time and what happens to it. It eases errors corrections in a data analytics process from origin to destination. Some data modeling techniques may facilitate lineage in comparison to others (Vault vs Kimball vs Inmon).</p>
<p><strong>Data Exploration</strong>: it is the beginning stage of data analysis. It helps to identify right dataset is vital before starting Data Exploration. All given components need to work together to play an important part in Data Warehouse building easily evolve and explore the environment.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tier-architecture"><a class="header" href="#tier-architecture">Tier Architecture</a></h1>
<h2 id="two-tier-architecture"><a class="header" href="#two-tier-architecture">Two-Tier Architecture</a></h2>
<p><img src="concepts/../mdbook-plantuml-img/34e8845308d5034a348498bba9809daa207f0ca8.svg" alt="" /></p>
<p style="text-align: center;">Two-Tier Data Warehouse Architecture.</p>
<p><strong>Data Source Layer</strong>: A data warehouse system uses a heterogeneous source of data. That data is stored initially to the organization's relational databases or legacy databases, or it may come from an information system outside the organization walls.</p>
<p><strong>Data Staging Layer</strong>: The data stored to the source should be extracted, cleansed to remove inconsistencies and fill gaps, and integrated to merge heterogeneous sources into one standard schema. The ETLs can combine heterogeneous schemata, extract, transform, cleanse, validate, filter, and load source data into a data warehouse. Note that this can be achieved in two ways:</p>
<ol>
<li>Having the Distillation Layer (Silver) in the Data Lake as the Data Staging Layer.</li>
<li>Creating a separated database within the Data Warehouse, or a separated database schema. Following the principle that all the data in the data warehouse should be cleaned and have high quality standards, a separated database should be preferred.</li>
</ol>
<p><strong>Data Warehouse Layer</strong>: Information is saved to one logically centralized individual repository: a data warehouse. The data warehouses can be directly accessed, but it can also be used as a source for creating data marts, which partially replicate data warehouse contents and are designed for specific enterprise departments. Metadata repositories store information on sources, access procedures, data staging, users, data mart schema, and so on.</p>
<p><strong>Analysis Layer</strong>: In this layer, integrated data is efficiently, and flexibly accessed to issue reports, dynamically analyze information, and simulate hypothetical business scenarios. It should feature aggregated information navigators, complex query optimizers, and customer-friendly GUIs.</p>
<h2 id="three-tier-architecture"><a class="header" href="#three-tier-architecture">Three-Tier Architecture</a></h2>
<p><img src="concepts/../mdbook-plantuml-img/c23cf858fede086a583ebceb59b95433ed0e7a9a.svg" alt="" /></p>
<p style="text-align: center;">Three-Tier Data Warehouse Architecture.</p>
<p>As the name of the architecture suggests, it consists of three tiers (levels):</p>
<ol>
<li>Bottom/Data Tier: data warehouse server with functional gateway (ODBC, JDBC, etc.).</li>
<li>Middle/Application Tier: houses the business logic used to process user inputs. Example: OLAP Servers, Snowflake, Apache Redshift, Databricks Data Lakehouse Platform, Apache Spark.</li>
<li>Top/Presentation Tier: front-end tools.</li>
</ol>
<p>The Bottom and Top tiers were already discussed in details in the previous section, so we only have left the implementation of the Middle Tier, very important to enable fast querying of the data warehouse. It is important to note the solutions to the middle tier are often referred as the Data Warehouse itself, but they’re only the Application tier/level in a complete data warehouse solution (It’s like saying Snowflake is the DWH, when it’s just one part of the complete DWH solution).</p>
<h3 id="reconciled-layer"><a class="header" href="#reconciled-layer">Reconciled Layer</a></h3>
<p>The Reconciled Layer sits between the source data and data warehouse. The main advantage of the reconciled layer is that it creates a standard reference data model for the whole company. At the same time, it separates the problems of source data extraction and integration from those of data warehouse population. In some cases, the reconciled layer is also directly used to accomplish better operational tasks, such as producing daily reports that cannot be satisfactorily prepared using the corporate applications or generating data flows to feed external processes periodically to benefit from cleaning and integration.</p>
<p>This architecture is especially useful for the extensive, enterprise-wide systems. A disadvantage of this structure is the extra file storage space used through the extra redundant reconciled layer. It also makes the analytical tools a little further away from being real-time.</p>
<p>Please note that a reconciled layer could be part of the Data Warehouse, or the Data Lake (see Harmonized Zone, in Data Lake concepts chapter).</p>
<h3 id="middleapplication-tier"><a class="header" href="#middleapplication-tier">Middle/Application Tier</a></h3>
<blockquote>
<p>Houses the business logic used to process user inputs. Example: OLAP Servers, Snowflake, Apache Redshift, Databricks Data Lakehouse Platform.</p>
</blockquote>
<p>See details in the chapter <a href="concepts/./data_warehouse_application_tier.html">Data Warehouse Middle/Application Tier</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-warehouse-middleapplication-tier"><a class="header" href="#data-warehouse-middleapplication-tier">Data Warehouse Middle/Application Tier</a></h1>
<blockquote>
<p>Houses the business logic used to process user inputs.</p>
</blockquote>
<p>One could argue that OLAP is <a href="https://www.kdnuggets.com/2022/10/olap-dead.html">dead</a>, at least in the traditional format (solutions like Mondrian), and cloud/modern solutions should be applied to accompany business fast demand for data.</p>
<p>Given this book aims to explore business scenarios mostly common to startups, in this sense, given costs restrictions, solutions like Apache Kylin seem to be a better approach (details below). When costs are less restricted, and/or data demands increase, other solutions like AWS Redshift, Snowflake, and Databricks Lakehouse are preferred/recommended.</p>
<h2 id="snowflake"><a class="header" href="#snowflake">Snowflake</a></h2>
<h2 id="aws-redshift"><a class="header" href="#aws-redshift">AWS Redshift</a></h2>
<p>#TODO</p>
<h2 id="snowflake-vs-aws-redshift"><a class="header" href="#snowflake-vs-aws-redshift">Snowflake vs AWS Redshift</a></h2>
<h2 id="databricks-lakehouse-platform"><a class="header" href="#databricks-lakehouse-platform">Databricks Lakehouse Platform</a></h2>
<blockquote>
<p>It combines the ACID transactions and data governance of data warehouses with the flexibility and cost-efficiency of data lakes to enable business intelligence (BI) and machine learning (ML) on all data.</p>
</blockquote>
<h3 id="data-lakehouse-vs-data-warehouse-vs-data-lake"><a class="header" href="#data-lakehouse-vs-data-warehouse-vs-data-lake">Data Lakehouse vs Data Warehouse vs Data Lake</a></h3>
<p>Data in the data warehouse is easy to use, but harder to store. The opposite is true for the data lake: it’s easy to ingest and store data, but a pain to consume and query.</p>
<p>The data lakehouse has a layer design, with a warehouse layer on top of a data lake. This architecture, which enables combining structured and unstructured data, makes it efficient for business intelligence and business analysis. Data lakehouses provide structured storage for some types of data and unstructured storage for others while keeping all data in one place.</p>
<h2 id="olap-servers"><a class="header" href="#olap-servers">OLAP Servers</a></h2>
<h3 id="relational-olap-servers-rolap"><a class="header" href="#relational-olap-servers-rolap">Relational OLAP Servers (ROLAP)</a></h3>
<blockquote>
<p>They use a relational or extended-relational DBMS to save and handle warehouse data, and OLAP middleware to provide missing pieces. They work primarily from the data that resides in a relational database, where the base data and dimension tables are stored as relational tables. This model permits the multidimensional analysis of data.</p>
</blockquote>
<p>This technique relies on manipulating the data stored in the relational database to give the presence of traditional OLAP's slicing and dicing functionality.</p>
<h4 id="advantages"><a class="header" href="#advantages">Advantages</a></h4>
<p><strong>Can handle large amounts of information</strong>: the data size limitation of ROLAP technology is depends on the data size of the underlying RDBMS. So, ROLAP itself does not restrict the data amount.</p>
<p>RDBMS already comes with a lot of features. So ROLAP technologies, (works on top of the RDBMS) can control these functionalities.</p>
<h4 id="disadvantages"><a class="header" href="#disadvantages">Disadvantages</a></h4>
<p><strong>Performance can be slow</strong>: each ROLAP report is a SQL query (or multiple SQL queries) in the relational database, the query time can be prolonged if the underlying data size is large.</p>
<p><strong>Limited by SQL functionalities</strong>: ROLAP technology relies on upon developing SQL statements to query the relational database, and SQL statements do not suit all needs.</p>
<h3 id="multidimensional-olap-servers-molap"><a class="header" href="#multidimensional-olap-servers-molap">Multidimensional OLAP Servers (MOLAP)</a></h3>
<blockquote>
<p>It is based on a native logical model that directly supports multidimensional data and operations. Data are stored physically into multidimensional arrays, and positional techniques are used to access them.</p>
</blockquote>
<p>One of the significant distinctions of MOLAP against a ROLAP is that data are summarized and are stored in an optimized format in a multidimensional cube, instead of in a relational database. In MOLAP model, data are structured into proprietary formats by client's reporting requirements with the calculations pre-generated on the cubes.</p>
<h4 id="advantages-1"><a class="header" href="#advantages-1">Advantages</a></h4>
<p><strong>Excellent Performance</strong>: a MOLAP cube is built for fast information retrieval, and is optimal for slicing and dicing operations.</p>
<p><strong>Can perform complex calculations</strong>: all evaluation have been pre-generated when the cube is created. Hence, complex calculations are not only possible, but they return quickly.</p>
<h4 id="disadvantages-1"><a class="header" href="#disadvantages-1">Disadvantages</a></h4>
<p><strong>Limited in the amount of information it can handle</strong>: Because all calculations are performed when the cube is built, it is not possible to contain a large amount of data in the cube itself.</p>
<h3 id="hybrid-olap-servers-holap"><a class="header" href="#hybrid-olap-servers-holap">Hybrid OLAP Servers (HOLAP)</a></h3>
<blockquote>
<p>It incorporates the best features of MOLAP and ROLAP into a single architecture. It saves more substantial quantities of detailed data in the relational tables while the aggregations are stored in the pre-calculated cubes. HOLAP also can drill through from the cube down to the relational tables for delineated data.</p>
</blockquote>
<h4 id="advantages-2"><a class="header" href="#advantages-2">Advantages</a></h4>
<ul>
<li>It provides benefits of both MOLAP and ROLAP.</li>
<li>It provides fast access at all levels of aggregation.</li>
<li>It balances the disk space requirement, as it only stores the aggregate information on the OLAP server and the detail record remains in the relational database. So no duplicate copy of the detail record is maintained.</li>
</ul>
<h4 id="disadvantages-2"><a class="header" href="#disadvantages-2">Disadvantages</a></h4>
<p>HOLAP architecture is very complicated because it supports both MOLAP and ROLAP servers.</p>
<h3 id="olap-servers-options"><a class="header" href="#olap-servers-options">OLAP Servers options</a></h3>
<h4 id="apache-kylin"><a class="header" href="#apache-kylin">Apache Kylin</a></h4>
<p>Only supports MOLAP and Offline data storage modes. It supports both SQL and MDX queries, have RESTful API capabilities (also ODBC, and JDBC), and can be integrated/connect with Tableau (also Redash, Superset, Zeppelin, Qlik, and Excel).</p>
<p>It supports Real Time processing, partitioning, usage based optimizations, load balancing and clustering. It supports LDAP, SAML, Kerboros authentication.</p>
<h4 id="mondrian-olap-server"><a class="header" href="#mondrian-olap-server">Mondrian OLAP Server</a></h4>
<p>Only supports ROLAP data storage modes. It supports MDX queries but not SQL, and have REST API capabilities. Does not natively connect with Tableau, but queries can be performed via Java APIs. It supports Real Time processing, and partitioning.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-modelling"><a class="header" href="#data-modelling">Data Modelling</a></h1>
<h2 id="kimballbottom-up"><a class="header" href="#kimballbottom-up">Kimball/Bottom-Up</a></h2>
<blockquote>
<p>The design of the Data Marts comes from the business requirements.</p>
</blockquote>
<p>The primary data sources are then evaluated, ETL tools are used to fetch data from several sources and load it into a staging area of the relational database server.
Once data is uploaded in the  data warehouse staging area, the next phase includes loading data into a dimensional data warehouse model that is denormalized by nature.
This model partitions data into the fact and dimension tables.
Kimball dimensional modelling allows users to construct several star schemas to fulfill various reporting needs.</p>
<div style="position: relative; width: 100%; height: 0; padding-top: 50.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbfV1mwLM&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbfV1mwLM&#x2F;view?utm_content=DAFbfV1mwLM&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Kimball Approach to Data Warehouse Lifecycle.</p></a>
<h3 id="advantages-3"><a class="header" href="#advantages-3">Advantages</a></h3>
<p><strong>Fast to construct (quick initial phase)</strong>: it is fast to construct as no normalization is involved, which means swift execution of the initial phase of the data warehousing design process.</p>
<p><strong>Simplified queries</strong>: in a star schema, most data operators can easily comprehend it because of its denormalized structure, which simplifies querying and analysis.</p>
<p><strong>Simplified business management</strong>: the data warehouse system footprint is trivial because it focuses on individual business areas and processes rather than the whole company. So, it takes less space in the database, simplifying system management.</p>
<p><strong>Fast data retrieval</strong>: as data is segregated into fact tables and dimensions.</p>
<p><strong>Smaller teams</strong>: a smaller team of designers and planners is sufficient for data warehouse management because data source systems are stable, and the data warehouse is process-oriented. Also, query optimization is straightforward, predictable, and controllable.</p>
<p><strong>Deeper insights</strong>: it allows business intelligence tools to deeper across several star schemas and generates reliable insights.</p>
<h3 id="disadvantages-3"><a class="header" href="#disadvantages-3">Disadvantages</a></h3>
<p><strong>No Single Source of Truth</strong>: Data isn’t entirely integrated before reporting, so the idea of a single source of truth is lost.</p>
<p><strong>Too prone to data irregularities</strong>: this is because in denormalization technique, redundant data is added to database tables.</p>
<p><strong>Too difficult and expensive to add new columns</strong>: performance issues may occur due to the addition of columns in the fact table, as these tables are quite in-depth. The addition of new columns can expand the fact table dimensions, affecting its performance.</p>
<p><strong>Can't respond (well) to business changes</strong>: it is too difficult to alter the models.</p>
<p><strong>Not BI-friendly</strong>: as it is business process-oriented, instead of focusing on the company as a whole, it cannot handle all the BI reporting requirements.</p>
<p><strong>Inconsistent dimensional view</strong>: this model is not strong as top-down approach as dimensional view of data marts is not consistent as it is in Inmon approach.</p>
<p>In brief, the Kimball approach have a <strong>low start-up cost</strong>, is <strong>faster to deliver</strong> the first phase of the data warehouse design, is faster to release to production (first version), but is suitable for <strong>Tactical</strong> business decision support requirements (versus Strategic), and <strong>addresses individual business requirements</strong> (vs Enterprise-wide). Another important topic that derives from this methodology approach is the <strong>Data Warehouse Bus Architecture</strong>.</p>
<h2 id="inmontop-down"><a class="header" href="#inmontop-down">Inmon/Top-Down</a></h2>
<blockquote>
<p>Subject-oriented, nonvolatile, integrated, time-variant collection of data in support of management’s decisions.</p>
</blockquote>
<p>On the other hand, Bill Inmon, the father of data warehousing, came up with the concept to develop a data warehouse which identifies the main subject areas and entities the enterprise works with, such as customers, product, vendor, etc. Inmon’s definition of a data warehouse is that it is a “subject-oriented, nonvolatile, integrated, time-variant collection of data in support of management’s decisions”.</p>
<p>The model then creates a <strong>thorough, logical model for every primary entity</strong> (Business Entities). For instance, a logical model is constructed for products with all the attributes associated with that entity. This logical model could include many entities, including all the details, aspects, relationships, dependencies, and affiliations.</p>
<p>The Inmon design approach uses the <strong>normalized form</strong> for building <strong>entity structure</strong>, avoiding data redundancy as much as possible. This results in clearly identifying business requirements and <strong>preventing any data update irregularities</strong>. Moreover, the advantage of this top-down approach in database design is that it is <strong>robust to business changes</strong> and contains a dimensional perspective of data across data mart.</p>
<p>Next, the physical model is constructed, which follows the normalized structure. This Inmon model creates a <strong>Single Source of Truth</strong> for the whole business to consume. Data loading becomes less complex due to the normalized structure of the model. However, using this arrangement for querying is challenging as it includes numerous tables and links.</p>
<p>This Inmon data warehouse methodology proposes constructing data marts separately for each division, such as finance, marketing sales, etc. All the data entering the data warehouse is integrated. The data warehouse acts as a single data source for various data marts to ensure integrity and consistency across the enterprise.</p>
<h3 id="advantages-4"><a class="header" href="#advantages-4">Advantages</a></h3>
<p><strong>Single Source of Truth</strong>: the data warehouse acts as a unified source of truth for the entire business, where all data is integrated.</p>
<p><strong>Very low data redundancy</strong>: there’s less possibility of data update irregularities, making the data warehouse ETL processes more straightforward and less susceptible to failure.</p>
<p><strong>Great flexibility</strong>: it’s easier to update the data warehouse in case there’s any change in the business requirements or source data.</p>
<p><strong>BI-friendly</strong>: It can handle diverse company-wide reporting requirements.</p>
<h3 id="disadvantages-4"><a class="header" href="#disadvantages-4">Disadvantages</a></h3>
<p><strong>Increasing complexity</strong>: it increases as multiple tables are added to the data model with time.</p>
<p><strong>Skilled Human Resources</strong>: resources skilled in data warehouse data modelling are required, which can be expensive and challenging to find.</p>
<p><strong>Slow setup</strong>: the preliminary setup and delivery are time-consuming.</p>
<p><strong>Expert management</strong>: this approach requires experts to manage a data warehouse effectively.</p>
<p>In brief, the Inmon have a <strong>high start-up cost</strong>, requires <strong>more time to be in production</strong> and meet business needs (very large projects with a very broad scope), and <strong>requires a bigger team os specialists</strong>, but is more <strong>suitable for for systems and business changes</strong>, better <strong>integrates with the whole organization</strong>, favors <strong>Strategic business decision support requirements</strong> (vs Tactical), and <strong>facilitates Business Intelligence development</strong>.</p>
<h2 id="hybrid"><a class="header" href="#hybrid">Hybrid</a></h2>
<blockquote>
<p>In a hybrid model, the data warehouse is built using the Inmon model, and on top of the integrated data warehouse, the business process oriented data marts are built using the star schema for reporting.</p>
</blockquote>
<p>The hybrid approach provides a <strong>Single Source of Truth</strong> for the data marts, creating a highly flexible solutions from a BI point of view.</p>
<p>Based on the <a href="https://www.researchgate.net/publication/261302233_The_Customer-Centric_Data_Warehouse_An_Architectural_Approach_to_Meet_the_Challenges_of_Customer_Orientation">Hub and Spoke Architecture</a>, the hybrid design methodology can also make use of <a href="https://en.wikipedia.org/wiki/Operational_data_store">Operational Data Stores</a> (ODS), integrating and cleaning data from multiple data sources. The information is then parsed into the actual Data Warehouse.</p>
<p>Hybrid methods will normally keep the data in the 3rd normal form, reducing redundancy. Although normal relational database is not efficient for BI reports. Data marts for specific reports can then be built on top of the data warehouse solution.</p>
<p>When the data is denormalized, all the data available is pulled (as advocated by Inmon) while using a denormalized design (as advocated by Kimball). One example is the <a href="https://resilientbiz.com/the-resilient-hybrid-methodology-data-warehouse/">Carry Forward</a> method.</p>
<p>Another hybrid methodology is the Data Vault, discussed below.</p>
<div style="position: relative; width: 100%; height: 0; padding-top: 50.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbfihtfys&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbfihtfys&#x2F;view?utm_content=DAFbfihtfys&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Example of a Hybrid Methodology approach.</p></a>
<h2 id="vault"><a class="header" href="#vault">Vault</a></h2>
<p>The <strong>Vault Data Modelling</strong> is a hybrid design, consisting of the best of breed practices from both <strong>3rd normal form</strong> and <strong>star-schema</strong>.</p>
<p>It is not a true 3rd normal form, and breaks some of the rules that 3NF dictates. It is a top-down architecture with bottom-up design, geared to be strictly a data warehouse. It is not geared to be end-user accessible, which when built, still requires the user of a data mart or star-schema based release are for business purposes.</p>
<p>Data Vault data modelling breaks data into a small number of standard components – the most common of which are <strong>Hubs</strong>, <strong>Links</strong> and <strong>Satellites</strong>.</p>
<p><strong>Hubs</strong> are entities of interest to the business. They contain just a distinct list of business keys and metadata about when each key was first loaded and from where.</p>
<p><strong>Links</strong> connect Hubs and may record a transaction, composition, or other type of relationship between hubs. They contain details of the hubs involved (as foreign keys) and metadata about when the link was first loaded and from where.</p>
<p><strong>Satellites</strong> connect to Hubs or Links. They are Point in Time: so we can ask and answer the question, “what did we know when?”. Satellites contain data about their parent Hub or Link and metadata about when the data was loaded, from where, and a business effectivity date.</p>
<p>The data model of the data warehouse is constructed using these components. These are:</p>
<p><strong>Standard</strong>: each component is always constructed the same way.</p>
<p><strong>Simple</strong>: easy to understand, and with a little practice, easy to apply them to model your system.</p>
<p><strong>Connected</strong>: hubs only connect to links and satellites, links only connect to hubs and satellites, and satellites only connect to hubs or links.</p>
<p>Data Vault has staging, vault and mart layers. Star schemas live in the mart layer, each star schema exposes a subset of the vault for a particular group of users.  Typically, hubs and their satellites form dimensions, links and their satellites form facts.</p>
<p>A Data Vault complements the Data Lake and is a solution for organizations that need to integrate and add structure to the data held in the Data Lake.</p>
<div style="position: relative; width: 100%; height: 0; padding-top: 50.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbf7fKp9s&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbf7fKp9s&#x2F;view?utm_content=DAFbf7fKp9s&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Example of Data Vault 2.0 Modelling Methodology approach.</p></a>
<h2 id="bus-architecture"><a class="header" href="#bus-architecture">Bus Architecture</a></h2>
<blockquote>
<p>A bus architecture is composed of a set of tightly integrated data marts that get their power from conformed dimensions and fact tables. A conformed dimension is defined and implemented one time, so that it means the same thing everywhere it's used.</p>
</blockquote>
<p>A dimension table is the "lookup" table of a dimensional model.
It contains textual data that decodes an identifier in associated fact tables.
A conformed dimension is defined and implemented one time and used throughout the multiple star schemas that make up the enterprise data mart.
Dimensions define the who, what, where, when, why, and how of a situation, and are laid out for the benefit of business users.</p>
<blockquote>
<p>To conform a dimension, every stakeholder must agree on a common definition for the dimension, so that the dimension means the same thing no matter where it’s used.</p>
</blockquote>
<p>#TODO: continue from https://www.itprotoday.com/sql-server/data-warehouse-bus-architecture</p>
<p>#TODO. Inflow, Upflow, Downflow, Outflow and Meta flow.</p>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="backlog"><a class="header" href="#backlog">Backlog</a></h1>
<h2 id="topics"><a class="header" href="#topics">Topics</a></h2>
<p><strong>Fault Tolerant Systems</strong>:</p>
<ul>
<li>General Reliability Development Hazard logs (FRACAS) [Redundancia]</li>
<li>High Availability [https://www.controlglobal.com/assets/14WPpdf/140324-ISA-ControlSystemsHighAvailability.pdf]</li>
<li>Technical documentation</li>
<li>Safety cases</li>
<li>Bulkhead</li>
<li>Change Control</li>
<li>Cold Standby</li>
<li>Defensive Design</li>
<li>Derating</li>
<li>Design Debt</li>
<li>Design Life</li>
<li>Design Thinking</li>
<li>Durability</li>
<li>Edge Case</li>
<li>Entropy</li>
<li>Error Tolerance</li>
<li>Fault Tolerance</li>
<li>Fail Well</li>
<li>Fail-Safe</li>
<li>Graceful Degradation</li>
<li>Mistake Proofing &amp; Poka Yoke Technique</li>
<li>No Fault Found</li>
<li>Resilience</li>
<li>Safety by Design</li>
<li>Self-Healing</li>
<li>Service Life</li>
<li>Systems Thinking</li>
<li>Testbed</li>
<li>Waer and Tear</li>
<li>Deconstructability</li>
<li>Refinement</li>
<li>Defense in Depth</li>
<li>FMEA Design and Process</li>
<li>Physics of Failure (PoF)</li>
<li>Built-in Self-test</li>
<li>Eliminating single point of failure (SPOF)</li>
</ul>
<p><strong>Analysis</strong>:</p>
<ul>
<li>Root Cause analysis</li>
<li>Fault tree analysis (FTA)</li>
<li>Failure mode and effects analysis (FMEA)</li>
<li>Failure mode, effects and criticality analysis (FMECA)</li>
<li>Reliability, Availability and Maintainability Study (RAMS)</li>
<li>Mission Readiness analysis</li>
<li>Functional System Failure analysis</li>
<li>Inherent Design Reliability analysis</li>
<li>Use/Load analysis and wear calculations</li>
<li>Fatigue and creep analysis</li>
<li>Component Stress analysis</li>
<li>Field failure monitoring</li>
<li>Field data analysis</li>
<li>Caution and warning analysis</li>
<li>Chaos Engineering</li>
<li>Reliability Risk Assessments</li>
<li>Hazard analysis</li>
<li>Manufactoring defect analysis</li>
<li>Residual Risk analysis (RCA)</li>
<li>Weibull</li>
<li>Accelerated Life Testing (ALT Analysis)</li>
<li>Material Strength analysis</li>
<li>Quality of Service</li>
<li>Quality Control</li>
<li>Defect Rate</li>
<li>Failure Rate</li>
<li>Mean Time Between Failures</li>
<li>Mean Time to Repair (MTTR)</li>
<li>Mean Corrective Maintenance Time (MCMT)</li>
<li>Mean Preventive Maintenance Time (MPMT)</li>
<li>Mean Maintenance Hours per Repair (MMH/Repair)</li>
<li>Maximum Corrective Maintenance Time (MaxCMT)</li>
</ul>
<p><strong>Data Quality</strong>:</p>
<ul>
<li>Data Quality Completeness</li>
<li>Data Quality Correctness</li>
<li>Data Quality Credibility</li>
<li>Data Quality Precision</li>
<li>Data Quality Relevance</li>
<li>Data Quality Timeliness</li>
<li>Data Quality Traceability</li>
<li>Data Integrity</li>
<li>Data Cleansing</li>
<li>Data Corruption</li>
<li>Data Degradation</li>
<li>Data Artifact</li>
<li>Data Rot</li>
<li>Information Quality Accurate</li>
<li>Information Quality Completeness</li>
<li>Information Quality Comprehensible</li>
<li>Information Quality Credibility</li>
<li>Information Quality Precision</li>
<li>Information Quality Relevance</li>
<li>Information Quality Timeliness</li>
<li>Information Quality Uniqueness</li>
<li>Conformance Quality</li>
<li>Credence Quality</li>
<li>Quality Assurance</li>
<li>Quality Control</li>
<li>Service Quality</li>
<li>Experience Quality</li>
<li>Code Smell</li>
<li>Referential Integrity</li>
<li>Reusability</li>
</ul>
<p><strong>Maintenance</strong>:</p>
<ul>
<li>Maintenance Requirement Allocation</li>
<li>Predictive and Preventive maintenance</li>
<li>Reliability Centered Maintenance (RCM)</li>
</ul>
<p><strong>Failures</strong>:</p>
<ul>
<li>Manufactoring-induced failures</li>
<li>Assembly-induced failures</li>
<li>Transport-induced failures</li>
<li>Storage-induced failures</li>
<li>Systmatic failures</li>
</ul>
<p><strong>Tests</strong>:</p>
<ul>
<li>System Diagnostics Design</li>
<li>Failure/Reliability testing</li>
</ul>
<p><strong>Human Factors</strong>:</p>
<ul>
<li>Human Factors</li>
<li>Human Interaction</li>
<li>Human Errors</li>
<li>Latent Human Error</li>
</ul>
<p><strong>DataOps</strong>:</p>
<p><strong>Business Process Management</strong>:</p>
<ul>
<li>BPM</li>
<li>BPI</li>
<li>BPE</li>
<li>BPA</li>
<li>BPR</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="assets/mermaid.min.js"></script>
        <script type="text/javascript" src="assets/mermaid-init.js"></script>

        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </body>
</html>
