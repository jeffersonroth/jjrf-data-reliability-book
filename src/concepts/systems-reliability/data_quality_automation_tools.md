# Data Quality Automation Tools

Tools like Great Expectations or Deequ allow data engineers to define and automate data quality checks within data pipelines. By continuously testing data for anomalies, inconsistencies, or deviations from defined quality rules, these tools help maintain high data quality standards.

This topic will be explored in depth in the chapter on [**Data Quality**](../data_quality.md), and there will be many use cases and examples throughout the book. Additionally, I recommend using some tools, platforms, and libraries that might help automate and test data quality, including:

* [**dbt (Data Build Tool)**](https://www.getdbt.com/)

> An open-source tool that enables data analysts and engineers to transform data in their warehouses more effectively by defining data models, testing data quality, and documenting data.

* [**Great Expectations**](https://greatexpectations.io/)

> An open-source tool that allows data teams to write tests for their data, ensuring it meets defined expectations for quality.

* [**AWS Deequ**](https://github.com/awslabs/deequ)

> An open-source library built on top of Apache Spark for defining 'unit tests' for data, which allows for large-scale data quality verification.

* [**Soda Core**](https://github.com/sodadata/soda-core)

> An open-source framework for scanning, validating, and monitoring data quality, ensuring datasets meet quality standards.

Other options, which I haven't personally tried but frequently appear in online rankings, including those from enterprise-level solutions:

* [**Talend Data Catalog**](https://www.talend.com/products/data-catalog/) & [**Data Fabric**](https://www.talend.com/products/data-fabric/): These tools offer comprehensive data quality management, including discovery, cleansing, enrichment, and monitoring to ensure data integrity.

* [**SAS Data Quality**](https://www.sas.com/en_gb/software/data-preparation-and-quality.html): A suite of tools by SAS that helps cleanse, monitor, and enhance the quality of data within an organization.

* [**SAP Master Data Governance**](https://www.sap.com/products/technology-platform/master-data-governance.html): A platform that provides centralized governance for master data, ensuring compliance, data quality, and consistency across business processes.

* [**Oracle Cloud Infrastructure Data Catalog**](https://www.oracle.com/big-data/data-catalog/): A metadata management service that helps organize, find, access, and govern data using a comprehensive data catalog.

* [**Ataccama ONE Platform**](https://www.ataccama.com/platform): A comprehensive data management platform offering data quality, governance, and stewardship capabilities to ensure data is accurate and usable.

* [**First Eigen**](https://firsteigen.com/): A data quality management tool that provides analytics and monitoring to maintain high data quality standards across systems.

* [**BigEye**](https://www.bigeye.com/): A monitoring platform designed for data engineers, providing automated data quality checks to ensure real-time data reliability.

* [**Data Ladder**](https://dataladder.com/): A data quality software that provides cleansing, matching, deduplication, and enrichment features to improve data quality.

* [**DQLabs Data Quality Platform**](https://www.dqlabs.ai/platform/): An AI-driven platform for managing data quality, offering features like profiling, cataloging, and anomaly detection.

* [**Precisely Trillium Quality**](https://www.precisely.com/product/precisely-trillium/trillium-quality): A data quality solution that offers profiling, cleansing, matching, and enrichment capabilities to ensure high-quality data.

* [**Syniti Master Data Management**](https://www.syniti.com/solutions/master-data-management/): A solution to maintain and synchronize high-quality master data across the organizational ecosystem.
