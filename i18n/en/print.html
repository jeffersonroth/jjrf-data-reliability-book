<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Data Reliability Engineering</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="assets/mdbook-admonish.css">
        <link rel="stylesheet" href="assets/catppuccin.css">
        <link rel="stylesheet" href="assets/catppuccin-highlight.css">

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="COVER.html">Cover</a></li><li class="chapter-item expanded affix "><a href="TITLE.html">Title</a></li><li class="chapter-item expanded affix "><a href="SUMMARY.html">Summary</a></li><li class="chapter-item expanded affix "><a href="DEDICATION.html">Dedication</a></li><li class="chapter-item expanded affix "><a href="FOREWORD.html">Foreword</a></li><li class="chapter-item expanded affix "><a href="PREFACE.html">Preface</a></li><li class="chapter-item expanded affix "><a href="AUTHOR.html">Author</a></li><li class="chapter-item expanded affix "><a href="OBJECTIVES.html">Objectives</a></li><li class="chapter-item expanded affix "><a href="STRUCTURE.html">Structure</a></li><li class="chapter-item expanded "><a href="CONCEPTS.html"><strong aria-hidden="true">1.</strong> I - Concepts</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/systems_intro.html"><strong aria-hidden="true">1.1.</strong> Introduction to Systems</a></li><li class="chapter-item expanded "><a href="concepts/systems_reliability.html"><strong aria-hidden="true">1.2.</strong> Systems Reliability</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/impediments.html"><strong aria-hidden="true">1.2.1.</strong> Impediments</a></li><li class="chapter-item expanded "><a href="concepts/attributes.html"><strong aria-hidden="true">1.2.2.</strong> Attributes</a></li><li class="chapter-item expanded "><a href="concepts/mechanisms.html"><strong aria-hidden="true">1.2.3.</strong> Mechanisms</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/fault_prevention_avoidance.html"><strong aria-hidden="true">1.2.3.1.</strong> Fault Prevention: Avoidance</a></li><li class="chapter-item expanded "><a href="concepts/fault_tolerance.html"><strong aria-hidden="true">1.2.3.2.</strong> Fault Tolerance</a></li><li class="chapter-item expanded "><a href="concepts/fault_prevention_elimination.html"><strong aria-hidden="true">1.2.3.3.</strong> Fault Prevention: Elimination</a></li><li class="chapter-item expanded "><a href="concepts/fault_prediction.html"><strong aria-hidden="true">1.2.3.4.</strong> Fault Prediction</a></li><li class="chapter-item expanded "><a href="concepts/reliability_tools.html"><strong aria-hidden="true">1.2.3.5.</strong> Reliability Tools</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="concepts/data_quality.html"><strong aria-hidden="true">1.3.</strong> Data Quality</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data_quality_intro.html"><strong aria-hidden="true">1.3.1.</strong> Introduction to Data Quality</a></li><li class="chapter-item expanded "><a href="concepts/master_data.html"><strong aria-hidden="true">1.3.2.</strong> Master Data</a></li><li class="chapter-item expanded "><a href="concepts/data_management_processes.html"><strong aria-hidden="true">1.3.3.</strong> Data Management Processes</a></li><li class="chapter-item expanded "><a href="concepts/data_quality_models.html"><strong aria-hidden="true">1.3.4.</strong> Data Quality Models</a></li></ol></li><li class="chapter-item expanded "><a href="concepts/data_relibility.html"><strong aria-hidden="true">1.4.</strong> Data Reliability</a></li><li class="chapter-item expanded "><a href="concepts/processes.html"><strong aria-hidden="true">1.5.</strong> Processes</a></li><li class="chapter-item expanded "><a href="concepts/operations.html"><strong aria-hidden="true">1.6.</strong> Operations</a></li><li class="chapter-item expanded "><a href="concepts/data_architecture.html"><strong aria-hidden="true">1.7.</strong> Data Architecture</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data_sources.html"><strong aria-hidden="true">1.7.1.</strong> Data Sources</a></li><li class="chapter-item expanded "><a href="concepts/data_tier.html"><strong aria-hidden="true">1.7.2.</strong> Data Tier</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concepts/data_lake.html"><strong aria-hidden="true">1.7.2.1.</strong> Data Lake</a></li><li class="chapter-item expanded "><a href="concepts/data_warehouse.html"><strong aria-hidden="true">1.7.2.2.</strong> Data Warehouse</a></li><li class="chapter-item expanded "><a href="concepts/data_lakehouse.html"><strong aria-hidden="true">1.7.2.3.</strong> Data Lakehouse</a></li><li class="chapter-item expanded "><a href="concepts/data_marts.html"><strong aria-hidden="true">1.7.2.4.</strong> Data Marts</a></li></ol></li><li class="chapter-item expanded "><a href="concepts/application_tier.html"><strong aria-hidden="true">1.7.3.</strong> Application Tier</a></li><li class="chapter-item expanded "><a href="concepts/presentation_tier.html"><strong aria-hidden="true">1.7.4.</strong> Presentation Tier</a></li><li class="chapter-item expanded "><a href="concepts/metadata_management_tools.html"><strong aria-hidden="true">1.7.5.</strong> Metadata Management Tools</a></li></ol></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.</strong> II - Use Cases</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.</strong> A - Aranduka Inc.</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.</strong> Data Architecture</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.1.</strong> Operational System and Internal Data Sources</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.2.</strong> Integrating Data Partners</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.3.</strong> Designing the Data Lake</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.3.1.</strong> Anonymized Data</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.3.2.</strong> Distilled Data</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.4.</strong> Designing the Data Warehouse</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.4.1.</strong> Core Data</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.1.5.</strong> Designing the Data Marts</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.2.</strong> BICC & BI</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.2.1.</strong> Building Reliable Pipelines</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.2.2.</strong> Data Quality Assurance & Monitoring</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.2.3.</strong> Continuous Service</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.3.</strong> Growth, Marketing & Attribution Models</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.1.4.</strong> Multidimensional Analysis: Geo vs Verticals</div></li></ol></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.</strong> III - Incorporating Data Reliability Engineering</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">3.1.</strong> Solutions Architects</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">3.1.1.</strong> Data Architect</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.2.</strong> Data Engineers</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.3.</strong> Backend Engineers</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.4.</strong> BI Engineers</div></li></ol></li><li class="chapter-item expanded "><a href="EPILOGUE.html">Epilogue</a></li><li class="chapter-item expanded affix "><a href="DICTIONARY.html">Dictionary</a></li><li class="chapter-item expanded affix "><a href="REFERENCES.html">References</a></li><li class="chapter-item expanded affix "><a href="NEXT.html">Next</a></li><li class="chapter-item expanded affix "><a href="BACK_COVER.html">Back Cover</a></li><li class="chapter-item expanded affix "><a href="backlog.html">Backlog</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Data Reliability Engineering</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div style="break-before: page; page-break-before: always;"></div><p>Data Reliability Engineering</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="summary"><a class="header" href="#summary">Summary</a></h1>
<p><a href="./COVER.html">Cover</a>
<a href="./TITLE.html">Title</a>
<a href="./SUMMARY.html">Summary</a>
<a href="./DEDICATION.html">Dedication</a>
<a href="./FOREWORD.html">Foreword</a>
<a href="./PREFACE.html">Preface</a>
<a href="./AUTHOR.html">Author</a>
<a href="./OBJECTIVES.html">Objectives</a>
<a href="./STRUCTURE.html">Structure</a></p>
<ul>
<li><a href="./CONCEPTS.html">I - Concepts</a>
<ul>
<li><a href="./concepts/systems_intro.html">Introduction to Systems</a></li>
<li><a href="./concepts/systems_reliability.html">Systems Reliability</a>
<ul>
<li><a href="./concepts/impediments.html">Impediments</a></li>
<li><a href="./concepts/attributes.html">Attributes</a></li>
<li><a href="./concepts/mechanisms.html">Mechanisms</a>
<ul>
<li><a href="./concepts/fault_prevention_avoidance.html">Fault Prevention: Avoidance</a></li>
<li><a href="./concepts/fault_tolerance.html">Fault Tolerance</a></li>
<li><a href="./concepts/fault_prevention_elimination.html">Fault Prevention: Elimination</a></li>
<li><a href="./concepts/fault_prediction.html">Fault Prediction</a></li>
<li><a href="./concepts/reliability_tools.html">Reliability Tools</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="./concepts/data_quality.html">Data Quality</a>
<ul>
<li><a href="./concepts/data_quality_intro.html">Introduction to Data Quality</a></li>
<li><a href="./concepts/master_data.html">Master Data</a></li>
<li><a href="./concepts/data_management_processes.html">Data Management Processes</a></li>
<li><a href="./concepts/data_quality_models.html">Data Quality Models</a></li>
</ul>
</li>
<li><a href="./concepts/data_relibility.html">Data Reliability</a></li>
<li><a href="./concepts/processes.html">Processes</a></li>
<li><a href="./concepts/operations.html">Operations</a></li>
<li><a href="./concepts/data_architecture.html">Data Architecture</a>
<ul>
<li><a href="./concepts/data_sources.html">Data Sources</a></li>
<li><a href="./concepts/data_tier.html">Data Tier</a>
<ul>
<li><a href="./concepts/data_lake.html">Data Lake</a></li>
<li><a href="./concepts/data_warehouse.html">Data Warehouse</a></li>
<li><a href="./concepts/data_lakehouse.html">Data Lakehouse</a></li>
<li><a href="./concepts/data_marts.html">Data Marts</a></li>
</ul>
</li>
<li><a href="./concepts/application_tier.html">Application Tier</a></li>
<li><a href="./concepts/presentation_tier.html">Presentation Tier</a></li>
<li><a href="./concepts/metadata_management_tools.html">Metadata Management Tools</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="">II - Use Cases</a>
<ul>
<li><a href="">A - Aranduka Inc.</a>
<ul>
<li><a href="">Data Architecture</a>
<ul>
<li><a href="">Operational System and Internal Data Sources</a></li>
<li><a href="">Integrating Data Partners</a></li>
<li><a href="">Designing the Data Lake</a>
<ul>
<li><a href="">Anonymized Data</a></li>
<li><a href="">Distilled Data</a></li>
</ul>
</li>
<li><a href="">Designing the Data Warehouse</a>
<ul>
<li><a href="">Core Data</a></li>
</ul>
</li>
<li><a href="">Designing the Data Marts</a></li>
</ul>
</li>
<li><a href="">BICC &amp; BI</a>
<ul>
<li><a href="">Building Reliable Pipelines</a></li>
<li><a href="">Data Quality Assurance &amp; Monitoring</a></li>
<li><a href="">Continuous Service</a></li>
</ul>
</li>
<li><a href="">Growth, Marketing &amp; Attribution Models</a></li>
<li><a href="">Multidimensional Analysis: Geo vs Verticals</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="">III - Incorporating Data Reliability Engineering</a>
<ul>
<li><a href="">Solutions Architects</a>
<ul>
<li><a href="">Data Architect</a></li>
</ul>
</li>
<li><a href="">Data Engineers</a></li>
<li><a href="">Backend Engineers</a></li>
<li><a href="">BI Engineers</a></li>
</ul>
</li>
</ul>
<p><a href="./EPILOGUE.html">Epilogue</a>
<a href="./DICTIONARY.html">Dictionary</a>
<a href="./REFERENCES.html">References</a>
<a href="./NEXT.html">Next</a>
<a href="./BACK_COVER.html">Back Cover</a></p>
<p><a href="./backlog.html">Backlog</a></p>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="concepts"><a class="header" href="#concepts">Concepts</a></h1>
<blockquote>
<p>The first part of this book exposes the different concepts around the data reliability engineering subject. It's intended to be heavilly technical, in contrast with the subsequent parts, intended to explore practical use cases.</p>
</blockquote>
<h2 id="introduction-to-systems-and-systems-reliability"><a class="header" href="#introduction-to-systems-and-systems-reliability"><a href="./concepts/systems_intro.html">Introduction to Systems</a> and <a href="./concepts/systems_reliability.html">Systems Reliability</a></a></h2>
<blockquote>
<p>These chapters explore what are systems, what is reliability, and how to understand systems reliability, specially its impediments, its attributes, and mechanisms to design and maintain reliable systems. All this for general systems, data systems, and data products.</p>
</blockquote>
<h2 id="data-quality"><a class="header" href="#data-quality"><a href="./concepts/data_quality.html">Data Quality</a></a></h2>
<blockquote>
<p>This chapter explores what is data, what is quality, and what is data quality, to finally explore what is data reliability.
The goal is to understand these concepts in all aspects of the data: life cycle, design, modelling, governance, management, access, security, uses, legal frameworks, best practices, maturity, standards, etc.</p>
</blockquote>
<h2 id="processes"><a class="header" href="#processes"><a href="./concepts/processes.html">Processes</a></a></h2>
<blockquote>
<p>This chapter explores, for a given system, the concept of data processes, data and information flow, workflows, orchestration, pipelines, ETL, and ELT.</p>
</blockquote>
<h2 id="operations"><a class="header" href="#operations"><a href="./concepts/operations.html">Operations</a></a></h2>
<blockquote>
<p>This chapter explores the concept of SRE, DataOps, DevOps, Agile methodologies, CI/CD, and other methodologies to assure reliable data operations.</p>
</blockquote>
<h2 id="data-architecture"><a class="header" href="#data-architecture"><a href="./concepts/data_architecture.html">Data Architecture</a></a></h2>
<blockquote>
<p>This chapter explores what is data architecture, including its sources, its storage (Data Lake, Data Warehouses, Data Marts), its application (OLAP servers, processing engines), and its presentation (dashboards, reports).</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="confiabilidad-de-sistemas"><a class="header" href="#confiabilidad-de-sistemas">Confiabilidad de Sistemas</a></h1>
<blockquote>
<p>La confiabilidad de un sistema es la propiedad del sistema que permite calificar, justificadamente, como fiable al servicio que proporciona.</p>
</blockquote>
<p>El objetivo de este capítulo es introducir los conceptos de confiabilidad y seguridad explorados por Alan Burns y Andy Wellings en su libro &quot;Sistemas de Tiempo Real y Lenguajes de Programación&quot;, conceptos desarrollados por diferentes industrias principalmente entre los años 60 y 90, y los conceptos de Site Reliability Engineering (SRE) desarrollados a partir de los 2000, además de complementarlo con conceptos de confiabilidad trabajados en otras ingenierías (mecánica, industrial, etc.), bien cómo contextualizarlo con conceptos trabajados actualmente en la industria de software y sistemas informáticos.</p>
<h2 id="impedimentos"><a class="header" href="#impedimentos"><a href="concepts/./impediments.html">Impedimentos</a></a></h2>
<blockquote>
<p>Los impedimentos impiden el perfecto funcionamento de un sistema, o son consecuencia de este. En ese subcapítulo se trabajará la detección de los diferentes tipos de impedimentos, los cuales, los <strong>Fallos</strong>, <strong>Errores</strong> y <strong>Defectos</strong>.</p>
</blockquote>
<h2 id="atributos"><a class="header" href="#atributos"><a href="concepts/./attributes.html">Atributos</a></a></h2>
<blockquote>
<p>El modo y las medidas mediante las cuales se puede <strong>estimar la calidad de un servicio confiable</strong>.</p>
</blockquote>
<h2 id="mecanismos"><a class="header" href="#mecanismos"><a href="concepts/./mechanisms.html">Mecanismos</a></a></h2>
<blockquote>
<p>Los mecanismos los cuales se trabaja la confiabilidad de sistemas, sea por interiorización y adopción de buenas prácticas, sea por la aplicación de metodologías, arquitecturas o herramientas específicas. Este subcapítulo busca formar un <strong>marco de trabajo</strong> el cual los ingeníeros puedan adoptar la confiabilidad de sistemas desde el mismo diseño.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="impedimentos-1"><a class="header" href="#impedimentos-1">Impedimentos</a></h1>
<h2 id="fallos-errores-y-defectos"><a class="header" href="#fallos-errores-y-defectos">Fallos, Errores y Defectos</a></h2>
<blockquote>
<p>Los <strong>fallos</strong> son el resultado de problemas internos no esperados que el sistema manifiesta eventualmente en su comportamiento externo. Estos problemas se llaman <strong>errores</strong>, y sus causas mecánicas o algorítmicas se denominan <strong>defectos</strong>. Cuando el comportamiento de un sistema se desvía del especificado para él, se dice que es un fallo.</p>
</blockquote>
<p>Los sistemas están compuestos de <strong>componentes</strong>, cada uno de los cuales se puede considerar como un sistema en sí mismo. Así, un fallo en un sistema puede inducir un defecto en otro, el cual puede acabar en un error y en un fallo potencial de este sistema. Esto puede continuar y producir un efecto en cualquier sistema relacionado, y así sucesivamente.</p>
<p>Un componente defectuoso de un sistema es un componente que producirá un error bajo un conjunto concreto de circunstancias durante la vida del sistema. Visto en términos de transición de estados, <em>un sistema puede ser considerado como un número de estados externos e internos</em>. </p>
<p>Un estado externo no especificado en el comportamiento del sistema se considerará un fallo del sistema. El sistema en sí mismo consta de un número de componentes (cada uno con sus propios estados), contribuyendo todos ellos al comportamiento externo del sistema. La combinación de los estados de estos componentes se denomina estado interno del sistema. <em>Un estado interno no especificado se considera un error, y el componente que produjo la transición de estados ilegal se dice que es defectuoso</em>.</p>
<p>Definiré tres tipos de fallos:</p>
<ul>
<li><strong>Fallos transitorios</strong>: comienza en un instante de tiempo concreto, se mantiene en el sistema durante algún periodo de tiempo, y luego desaparece.</li>
<li><strong>Fallos permanentes</strong>: comienzan en un instante determinado y permanecen en el sistema hasta que son reparados.</li>
<li><strong>Fallos intermitentes</strong>: son fallos transitorios que ocurren de vez en cuando.</li>
</ul>
<h2 id="modos-de-fallos"><a class="header" href="#modos-de-fallos">Modos de Fallos</a></h2>
<blockquote>
<p>Un sistema puede fallar de muchas maneras. Un diseñador puede diseñar el sistema suponiendo un número finito de modos de fallo, sin embargo el sistema puede fallar de manera diferente a lo esperado. </p>
</blockquote>
<p>Podemos clasificar los modos de fallos de los servicios que proporciona un sistema, los cuales:</p>
<ul>
<li><strong>Fallos de valor</strong>: el valor asociado con el servicio es erróneo.</li>
<li><strong>Fallo de tiempo</strong>: el servicio se completa a destiempo.</li>
<li><strong>Fallo arbitrario</strong>: combinación de fallos de valor y tiempo.</li>
</ul>
<p>Los modos de fallo de valor se denominan <strong>domínio de valor</strong>, y son clasificados en <strong>error de límites</strong>, y <strong>valor erróneo</strong>, dónde el valor se encuentra fuera del rango estipulado.</p>
<p>Los fallos en el dominio del tiempo pueden hacer que el servicio sea entregado:</p>
<ul>
<li><strong>Demasiado pronto</strong> (adelantado): el servicio se entrega antes de lo requerido.</li>
<li><strong>Demasiado tarde</strong> (retrasado o error de prestaciones): el servicio se entrega después de lo requerido.</li>
<li><strong>Infinitamente tarde</strong> (fallo de omisión): el servicio nunca es entregado.</li>
<li><strong>No esperado</strong> (fallo de encargo o improvisación): el servicio es entregado sin ser esperado.</li>
</ul>
<p>En general, podemos suponer los modos que un sistema puede fallar:</p>
<ul>
<li><strong>Fallo descontrolado</strong>: un sistema que produce errores arbitrales, tanto en el dominio del valor como en el del tiempo (incluyendo errores de improvisación).</li>
<li><strong>Fallo de retraso</strong>: un sistema que produce servicios correctos en el dominio del valor, pero que sufre errores de retraso en el tiempo.</li>
<li><strong>Fallo de silencio</strong>: un sistema que produce servicios correctos tanto en el dominio del valor como en el del tiempo, hasta que falla. El único fallo posible es el de omisión, y cuando ocurre, todos los servicios siguientes también sufrirán fallos de omisión.</li>
<li><strong>Fallo de parada</strong>: un sistema que tiene todas las propiedades de un fallo silencioso, pero que permite que otros sistemas puedan detectar que ha entrado en el estado de fallo de silencio.</li>
<li><strong>Fallo controlado</strong>: un sistema que falla de una forma especificada y controlada.</li>
<li><strong>Sin fallos</strong>: un sistema que siempre produce los servicios correctos.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="atributos-1"><a class="header" href="#atributos-1">Atributos</a></h1>
<h2 id="confiabilidad"><a class="header" href="#confiabilidad">Confiabilidad</a></h2>
<blockquote>
<p>Es la probabilidad <em>R(t)</em> de que el sistema <strong>siga funcionando al final del proceso</strong>. El tiempo <em>t</em> se mide en horas continuas de trabajo entre diagnósticos. La tasa constante de fallos λ se mide en <em>fallos/h</em>. La vida útil de un componente del sistema es la región constante (escala logarítmica) de la curva entre vida del componente (Component Age) y su tasa de fallos (Failure Rate). La región de la gráfica antes del equilibrio es el Burn In Phase, y la región en donde la tasa de fallos empieza a crecer es el End of Life Phase. Así tenemos <em>R(t)=exp(-λt)</em>.</p>
</blockquote>
<h2 id="disponibilidad"><a class="header" href="#disponibilidad">Disponibilidad</a></h2>
<blockquote>
<p>Es la medida de la <strong>frecuencia de los periodos de servicio incorrecto</strong>.</p>
</blockquote>
<h2 id="fiabilidad"><a class="header" href="#fiabilidad">Fiabilidad</a></h2>
<blockquote>
<p>Continuidad de entrega del servicio. </p>
</blockquote>
<p>Es una medida (probabilidad) del <strong>éxito con el que el sistema se ajusta a la especificación definitiva de su comportamiento</strong>.</p>
<h2 id="seguridad"><a class="header" href="#seguridad">Seguridad</a></h2>
<blockquote>
<p>Es la ausencia de condiciones que pueden causar daños y propagación de <strong>daños catastróficos</strong> en producción.</p>
</blockquote>
<p>Sin embargo, como esa definición puede clasificar cómo inseguros virtualmente cualquier proceso, consideraremos a menudo el término <strong>percance</strong>.</p>
<blockquote>
<p>Un percance es un <strong>evento no planeado</strong> o secuencia de eventos que pueden producir daños catastróficos.</p>
</blockquote>
<p>Por mayores que sean su similitud con la definición de <strong>fiabilidad</strong>, se debe considerar la diferencia en su énfasis. La fiabilidad es la medida de éxito con la cual el sistema se ajusta a la especificación de su comportamiento, normalmente en términos de <strong>probabilidad</strong>. La seguridad, sin embargo, es la <strong>improbabilidad de que se den las condiciones que conducen a un percance, independientemente si se realiza la función prevista</strong>.</p>
<h2 id="integridad"><a class="header" href="#integridad">Integridad</a></h2>
<blockquote>
<p>Es la ausencia de condiciones que pueden llevar a alteraciones inapropiadas de los datos en producción. Es la <strong>improbabilidad de que se den las condiciones que alteran en producción datos inapropiados, independientemente si se realiza la función prevista</strong>.</p>
</blockquote>
<h2 id="mantenimiento"><a class="header" href="#mantenimiento">Mantenimiento</a></h2>
<blockquote>
<p>Capacidad de superar reparaciones y evolucionar.</p>
</blockquote>
<h2 id="escalabilidad"><a class="header" href="#escalabilidad">Escalabilidad</a></h2>
<blockquote>
<p>Capacidad de adecuación al negocio.</p>
</blockquote>
<h2 id="deficiencias"><a class="header" href="#deficiencias">Deficiencias</a></h2>
<blockquote>
<p>Circunstancias que causan o son producto de la no <strong>confiabilidad</strong>.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mecanismos-1"><a class="header" href="#mecanismos-1">Mecanismos</a></h1>
<h2 id="prevención-de-fallos--evitación"><a class="header" href="#prevención-de-fallos--evitación"><a href="concepts/./fault_prevention_avoidance.html">Prevención de Fallos : Evitación</a></a></h2>
<h2 id="tolerancia-de-fallos"><a class="header" href="#tolerancia-de-fallos"><a href="concepts/./fault_tolerance.html">Tolerancia de Fallos</a></a></h2>
<h2 id="prevención-de-fallos-eliminación"><a class="header" href="#prevención-de-fallos-eliminación"><a href="concepts/./fault_prevention_elimination.html">Prevención de Fallos: Eliminación</a></a></h2>
<h2 id="predicción-de-fallos"><a class="header" href="#predicción-de-fallos"><a href="concepts/./fault_prediction.html">Predicción de Fallos</a></a></h2>
<h2 id="herramientas-de-confiabilidad"><a class="header" href="#herramientas-de-confiabilidad"><a href="concepts/./reliability_tools.html">Herramientas de Confiabilidad</a></a></h2>
<div style="break-before: page; page-break-before: always;"></div><h1 id="prevención-de-fallos--evitación-1"><a class="header" href="#prevención-de-fallos--evitación-1">Prevención de Fallos : Evitación</a></h1>
<p>Existen dos fases en la prevención de fallos: <strong>evitación y eliminación</strong>.</p>
<blockquote>
<p>Con la evitación se intenta limitar la introducción de datos y objetos potencialmente defectuosos durante la ejecución del proceso.</p>
</blockquote>
<p>Como:</p>
<ul>
<li>La utilización de fuentes de información validadas y limpias (cuando sea posible).</li>
<li>La introducción de procesos de limpieza y validación de los datos (data cruda).</li>
<li>La introducción de validación de disponibilidad de tablas y columnas, y la introducción de operadores de rama para su manejo.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tolerancia-de-fallos-1"><a class="header" href="#tolerancia-de-fallos-1">Tolerancia de Fallos</a></h1>
<blockquote>
<p>Debido a las limitaciones en las prevenciones de fallos, una vez que la data y los procesos cambian con frecuencia, es necesario recurrir a la tolerancia de fallos.</p>
</blockquote>
<p>Existen diferentes niveles de tolerancia a fallos:</p>
<ul>
<li><strong>Tolerancia total</strong>: no hay manejo de condiciones adversas o no deseadas, el proceso no se adapta a las validaciones y variables de entorno u otras informaciones externas para la ejecución de los tasks.</li>
<li><strong>Degradación controlada</strong> (o caída suave): notificaciones serán disparadas en la presencia de fallos, y siendo el suficiente importantes para interrumpir el flujo de tareas (thresholds, inexistencia o indisponibilidad de la data), los operadores de rama seleccionarán los tasks subsiguientes.</li>
<li><strong>Fallo seguro</strong>: los fallos detectados son suficientes para detectar que el proceso no debe ocurrir, un operador de cortocircuito cancela la ejecución de los subsiguientes tasks, los responsables son notificados, y caso no exista un proceso automático para lidiar con el problema, el equipo de data puede tomar acciones como volver a ejecutar los procesos que generan los inputs necesarios, o escalar el caso.</li>
</ul>
<p>El diseño de los procesos tolerantes a fallos supone:</p>
<ul>
<li>Los algoritmos de las tareas se han diseñado correctamente.</li>
<li>Se conocen todos los posibles modos de fallos de los componentes.</li>
<li>Se han tenido en cuenta todas las posibles interacciones entre el proceso y su entorno.</li>
</ul>
<h2 id="redundancia"><a class="header" href="#redundancia">Redundancia</a></h2>
<blockquote>
<p>Todas las técnicas utilizadas para conseguir tolerancia a fallos se basan en añadir elementos externos al sistema para que detecte y se recupere de fallos. Estos elementos son redundantes en el sentido de que no son necesarios para el normal funcionamiento del sistema, a esto llamamos <strong>redundancia protectora</strong>. El objetivo de la tolerancia es minimizar la redundancia, maximizando la fiabilidad, siempre bajo las restricciones de complejidad y tamaño del sistema. <strong>Se debe tener cuidado al diseñar los sistemas tolerantes a fallos, ya que los componentes incrementan la complejidad y mantenimiento de todo el sistema, lo que puede en sí, conducir a sistemas menos fiables</strong>.</p>
</blockquote>
<p>Clasificamos la redundancia en los sistemas en estáticas y dinámicas. La <strong>redundancia estática</strong>, o enmascaramiento, consiste en que los componentes redundantes son utilizados para ocultar los efectos de los fallos. La <strong>redundancia dinámica</strong> es la redundancia aportada dentro de un componente que hace que el mismo indique, implícita o explícitamente, que la salida es errónea; la recuperación debe ser proporcionada por otro componente. Esta técnica de tolerancia a fallos tiene cuatro fases:</p>
<ol>
<li><strong>Detección de errores</strong>: no se utilizará ningún esquema de tolerancia a fallos hasta que se haya detectado un error.</li>
<li><strong>Confinamiento y valoración de daños</strong>: cuando se detecte un error, debe estimarse la extensión del sistema que ha sido corrompida (diagnóstico de error) y su escopo.</li>
<li><strong>Recuperación del error</strong>: este es uno de los aspectos más importantes de la tolerancia a fallos. Las técnicas de recuperación de errores deberían dirigir al sistema corrupto a un estado a partir del cual pueda continuar con su normal funcionamiento (quizás con una degradación funcional).</li>
<li><strong>Tratamiento del fallo y continuación del servicio</strong>: un error es un síntoma de un fallo; aunque el daño pudiera haber sido reparado, el fallo continúa existiendo, y por lo tanto el error puede volver a darse a menos que se realice algún tipo de mantenimiento.</li>
</ol>
<h3 id="1-detección-de-errores"><a class="header" href="#1-detección-de-errores">1. Detección de errores</a></h3>
<blockquote>
<p>La efectividad de un sistema tolerante a fallos depende de la <strong>efectividad de detección de errores</strong>.</p>
</blockquote>
<p>La detección de errores se clasifican en:</p>
<ul>
<li><strong>Detecciones en el entorno</strong>. Los errores se detectan en el entorno en el cual se ejecutan el programa. Son manejados por las excepciones (exceptions).</li>
<li><strong>Detección en la aplicación</strong>. Los errores se detectan en la misma aplicación.
<ul>
<li><strong>Comprobaciones inversas</strong>. Aplicadas en componentes de relación isomórfica (uno a uno) entre la entrada y la salida. En este método, se toma la salida y se calcula la entrada, lo cual es comparado con el valor de entrada original. Para casos de números reales, es necesario adoptar técnicas de comparación inexactas.</li>
<li><strong>Comprobación de racionalidad</strong>. Se basan en el conocimiento del diseño y de la construcción del sistema. Comprueban que el estado de los datos o el valor de un objeto es razonable basándose en su supuesto uso.</li>
</ul>
</li>
</ul>
<h3 id="2-confinamiento-y-valoración-de-los-daños"><a class="header" href="#2-confinamiento-y-valoración-de-los-daños">2. Confinamiento y valoración de los daños</a></h3>
<blockquote>
<p>Siempre existirá una magnitud de tiempo, entre la ocurrencia de un defecto y la detección del error, siendo por lo tanto importante la valoración de cualquier daño que se haya podido producir en este intervalo de tiempo.</p>
</blockquote>
<p>Aunque el tipo de error detectado podrá dar ideas sobre el daño a la rutina de tratamiento del error, podrían haber sido diseminadas informaciones erróneas por el sistema y su entorno. Así, la valoración de los daños estará directamente relacionada con las precauciones tomadas por el diseñador de este sistema para el confinamiento del daño (<strong>construcción de cortafuegos</strong>). El confinamiento del daño se refiere a la estructuración del sistema de modo que se minimicen los daños causados por un componente defectuoso.</p>
<p>Existen dos técnicas principales para estructurar los sistemas de modo que se facilite el confinamiento de daños: <strong>descomposición modular</strong> y <strong>acciones atómicas</strong>. Por descomposición modular entiéndase que los sistemas deben ser descompuestos en componentes, cada uno de los cuales se representa por uno o más módulos. La interacción de los componentes se produce a través de interfaces bien definidas, y los detalles internos de los módulos están ocultos y no son accesibles directamente desde el exterior. Esto hace más difícil que un error en un componente pase indiscriminadamente a otro.</p>
<p>La descomposición modular proporciona al sistema una estructura estática, ya las acciones atómicas proporcionan al mismo una estructura dinámica. Se dice que una acción es atómica si no existen interacciones entre la actividad y el sistema durante el transcurso de la acción. Estas acciones se utilizan para mover el sistema de un estado consistente a otro, y para restringir el flujo de información entre los componentes.</p>
<h3 id="3-recuperación-de-errores"><a class="header" href="#3-recuperación-de-errores">3. Recuperación de errores</a></h3>
<blockquote>
<p>Una vez detectada la situación de error, y que sus posibles daños hayan sido valorados, se inician los procedimientos de recuperación de errores. Esta fase es probablemente la más importante dentro de las técnicas de tolerancia a fallos, la cual debe transformar un estado erróneo del sistema en otro desde el cual el sistema pueda continuar con su funcionamiento normal, quizás con una cierta degradación en el servicio.</p>
</blockquote>
<p>Aquí citaré dos estrategias para la recuperación de errores: recuperación <strong>hacia adelante</strong>, y <strong>hacia atrás</strong>. La recuperación de errores hacia adelante intenta continuar desde el estado erróneo realizando correcciones selectivas en el estado del sistema, que incluye proteger cualquier aspecto del entorno controlado que pudiera ser puesto en riesgo o dañado por el fallo.</p>
<p>La recuperación hacia atrás se basa en restaurar el sistema a un estado seguro previo a aquél en el que se produjo el error, para luego ejecutar una sección alternativa de la tarea. Ésta tendrá la misma funcionalidad que la sección que produjo el defecto, pero utilizando un algoritmo distinto. Se espera que esta alternativa no produzca el mismo defecto que la versión anterior, así que dependerá del conocimiento del diseñador sobre los posibles modos de fallo de este componente.</p>
<p>El diseñador debe tener claro los niveles de degradación de un servicio, teniendo en cuenta los servicios y procesos que dependen de éste. La recuperación de errores hace parte de los procesos de acción correctiva y acciones preventivas (CAPA - Corrective Action and Preventive Action Process), el cual se trabajará en dos momentos: en ese mismo capitulo de tolerancia a fallos cuando se trabajen las acciones correctivas, y en el próximo capitulo, prevención de fallos, cuando se aborde el tema de acciones preventivas.</p>
<h3 id="4-tratamiento-de-los-fallos-y-servicio-continuado"><a class="header" href="#4-tratamiento-de-los-fallos-y-servicio-continuado">4. Tratamiento de los fallos y servicio continuado</a></h3>
<blockquote>
<p>Un error es una manifestación de un defecto, y aunque la fase de recuperación del error puede haber llevado el sistema a un estado libre de error, el error se puede volver a producir. Por lo tanto, la fase final de la tolerancia de fallos es erradicar el fallo del sistema, de forma que se pueda continuar con el servicio normal.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="prevención-de-fallos-eliminación-1"><a class="header" href="#prevención-de-fallos-eliminación-1">Prevención de Fallos: Eliminación</a></h1>
<p>La segunda fase de prevención de fallos es la eliminación de fallos. Consiste normalmente en procedimientos para encontrar y eliminar las causas de los errores. Aunque se pueden utilizar técnicas como los revisores de código (IDEs, linter) y el debugging en local, ni siempre se llevan a cabo las revisiones por pares y pruebas exhaustivas con las distintas combinaciones de estados de entrada y entorno. </p>
<p>Las pruebas en QA no pueden verificar que los valores de salida sean compatibles con el negocio y sus aplicaciones, así que se concentran normalmente en modos de fallo de tiempo (timeouts) y <strong>defectos</strong>. Desafortunadamente, la prueba del sistema no puede ser exhaustiva y eliminar todos los potenciales fallos, principalmente por:</p>
<ul>
<li>Las pruebas se utilizan para demostrar la presencia de fallos, no su ausencia.</li>
<li>La dificultad de realizar pruebas en producción. La manera de probar fallas en producción son del tipo <strong>combate real</strong>, o sea, la consecuencia de los errores pueden afectar directamente el negocio, haciendo que pueda tomar malas decisiones (ejemplo: un mal cálculo de un KPI, puede además de llevar a acciones erróneas, disminuir la confianza del negocio en los procesos de data). Existen alternativas de diseño de procesos para detección de fallos en producción, las cuales discutiré más adelante.</li>
<li>Los errores que han sido introducidos en la etapa de requisitos del sistema puede que no se manifiesten hasta que el sistema esté operativo. Ejemplo: Un DAG para el procesamiento y limpieza de la data de delivery de las campañas de marketing online que se diseñó para ejecutar antes del proceso de atribución de media, a las 4am, no llevó en consideración que la data no estará disponible hasta las 5am.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="predicción-de-fallos-1"><a class="header" href="#predicción-de-fallos-1">Predicción de Fallos</a></h1>
<p>La predicción acurada y rápida de los fallos posibilita a los que mantenemos los procesos asegurar mayor disponibilidad de los servicios. Desafortunadamente, la predicción de fallos es muchísimo menos sencilla que su detección.</p>
<p>Para poder predecir un fallo, éste debe ser identificado y clasificado. Los fallos también deben ser predecibles (o capaces de predicción), lo que significa que existen alteraciones de estados de los sistemas (y componentes) que llevan al fallo, o el fallo ocurre regularmente siguiendo algún patrón. Ambos casos pueden ser traducidos a problemas de predicción de series temporales, y la data de los sensores y logs puede ser trabajada para entrenar los modelos de predicción.</p>
<p>La data colectada muy difícilmente estará lista para ser utilizada por los modelos de predicción, así que uno o más tareas de preprocesamiento deben llevarse a cabo:</p>
<ul>
<li><strong>Sincronización de la data</strong>: las métricas colectadas por diversos agentes (sensores) deben alinearse en la dimensión de tiempo.</li>
<li><strong>Limpieza de la data</strong>: remoción de data innecesaria, y generación de data faltante (ej: interpolación).</li>
<li><strong>Normalización de la data</strong>: los valores de las métricas son normalizados para que las magnitudes sean comparables.</li>
<li><strong>Selección de features</strong>: las métricas relevantes son identificadas para su utilización en los modelos.</li>
</ul>
<p>Una vez preprocesada la data, la misma será utilizada en dos pipelines: pipeline de entrenamiento, y pipeline de inferencia. El pipeline de entrenamiento usa la data en bulk, para entrenar el modelo a ser disponibilizado al pipeline de inferencia. Los resultados de la inferencia indicará la existencia o no de tipos específicos de fallas, sobre la muestra de métricas monitoreadas.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="herramientas-de-confiabilidad-1"><a class="header" href="#herramientas-de-confiabilidad-1">Herramientas de Confiabilidad</a></h1>
<h2 id="diagramas-de-blocos-de-confiabilidad"><a class="header" href="#diagramas-de-blocos-de-confiabilidad">Diagramas de blocos de confiabilidad</a></h2>
<blockquote>
<p>Los diagramas de blocos de confiabilidad (reliability block diagram, o RBD) es un método para diagramar e identificar como la confiabilidad de componentes (o subsistemas) <em>R(t)</em>, contribuyen para el éxito o fracaso de una redundancia. Es decir, es un método que puede ser utilizado para diseñar y optimizar componentes y seleccionar redundancias, visando bajar los failure rates.</p>
</blockquote>
<p>Un RBD es representado en una serie de blocos conectados (en série, en paralelo, o su combinación), indicando componentes redundantes, indicando el tipo de redundancia y su respectivo failure rate.</p>
<p>Al analizarse el diagrama, se indican componentes que fallaron y los que no fallaron. Si es posible encontrar una ruta o camino entre el inicio y el fin de proceso con componentes que no fallaron, se puede suponer que el proceso se puede completar.</p>
<p>Cada RBD debe incluir afirmaciones o sentencias listando todas las relaciones entre los componentes, es decir que condiciones llevarón a tomar un componente u otro en la ejecución del proceso.</p>
<p>Links:</p>
<ul>
<li><a href="https://moodle.univ-angers.fr/pluginfile.php/2071725/mod_resource/content/1/Reliability%20Engineering%20-%20ISMP%20-%20Chap%203%20-%20RBD.pdf">Université Angers</a></li>
<li><a href="https://en.wikipedia.org/wiki/Reliability_block_diagram">Wikipedia</a></li>
<li><a href="https://hpreliability.com/understanding-reliability-block-diagrams/">HPReliability</a></li>
<li><a href="https://www.sydneywater.com.au/web/groups/publicwebcontent/documents/document/zgrf/mdq2/~edisp/dd_046415.pdf">Sydney Water</a></li>
</ul>
<h2 id="failure-reporting-analysis-and-corrective-action-system-fracas"><a class="header" href="#failure-reporting-analysis-and-corrective-action-system-fracas">Failure Reporting, Analysis, and Corrective Action System (FRACAS)</a></h2>
<blockquote>
<p>FRACAS es un sistema o proceso definido para el reporte, clasificación y análisis de fallos, bien como la planeación de acciones correctivas de dichos fallos. Es parte del proceso guardar el historial de los análisis y acciones tomadas.</p>
</blockquote>
<p>Llevar a cabo dicho proceso supone automatizar el análisis de los logs de los procesos de data (logs), commits, pull requests y tickets.</p>
<p>La implementación de proceso es cíclica y se da por (FRACAS Kaizen Loop adaptado):</p>
<ul>
<li><strong>Failure Mode Analysis</strong>: analysis de los modos de fallos.</li>
<li><strong>Failure Codes Creation</strong>: creación de códigos de fallos, o la metodología para clasificarlos.</li>
<li><strong>Work Order History Analysis</strong>: análisis del historial de tickets enviados al equipo de data.</li>
<li><strong>Root Cause Analysis</strong>: análisis de las causas raíces.</li>
<li><strong>Strategy Adjustment</strong>: ajuste de estratégia.</li>
</ul>
<p>Links:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Failure_reporting,_analysis,_and_corrective_action_system">Wikipedia</a></li>
<li><a href="https://reliabilityweb.com/articles/entry/whats_the_fracas">Reliability Web</a></li>
<li><a href="https://ieeexplore.ieee.org/abstract/document/1285523">IEEE Best Practices</a></li>
</ul>
<h2 id="spare-parts-stocking-strategy"><a class="header" href="#spare-parts-stocking-strategy">Spare Parts Stocking Strategy</a></h2>
<blockquote>
<p>Con suerte siempre existirán disponibles fuentes de datos limpias, con complejas transformaciones y limpiezas, que ahorran tiempo y procesamiento, y que pueden ser usadas en multiples etapas de multiples procesos, sin embargo las mismas pueden temporalmente estar no disponibles o fallar. Una vez identificados tales fuentes, y constatado que son críticas a un sistema o proceso, es prudente tener tareas mínimas de limpieza y transformaciones que trabajen sobre los datos crudos o fuentes de la fuente, que quizas no resultará en datos finales con los mismos niveles de detalles, pero que serán lo suficiente buenos.</p>
</blockquote>
<p>Tales tareas no son diseñadas para hacer parte del flujo normal de los procesos, pero son &quot;piezas de recambio&quot;, disponibles para cuando los tiempos de mantenimiento son demasiado largos. El empleo de dichas tareas deben ser por el mínimo de tiempo posible, mientras el equipo tiene tiempo de resolver los fallos en la tarea original, o diseñar su reemplazo.</p>
<p>Links:</p>
<ul>
<li><a href="https://reliabilityweb.com/articles/entry/how_to_develop_a_spare_parts_stocking_strategy">ReliabilityWeb</a></li>
</ul>
<h2 id="availability-controls"><a class="header" href="#availability-controls">Availability Controls</a></h2>
<blockquote>
<p>Fallos de disponibilidad pueden ocurrir por un sin numero de razones (desde hardware hasta bugs), y algunos sistemas o procesos tienen suficiente relevancia para que controles de disponibilidad (availability controls) sean implementados, para asegurar que determinados servicios o data sigan disponibles cuando ocurra dichos fallos.</p>
</blockquote>
<p>Los controles de disponibilidad van desde el uso de backups periódicos de la data, snaps, timetravel, procesos redundantes, sistemas de respaldo en servidores locales o cloud, etc.</p>
<p>Links:</p>
<ul>
<li><a href="https://www.whitehatsec.com/glossary/content/availability-controls">WhiteHatSec</a></li>
<li><a href="https://www.lawinsider.com/clause/availability-control">Law Insider</a></li>
<li><a href="https://www.controlglobal.com/assets/14WPpdf/140324-ISA-ControlSystemsHighAvailability.pdf">Control Global</a></li>
</ul>
<h2 id="acciones-correctivas"><a class="header" href="#acciones-correctivas">Acciones Correctivas</a></h2>
<blockquote>
<p>Parte del CAPA (Corrective Action and Preventive Action Process), las acciones correctivas (CAP - Corrective Action Process) consisten en la detección de fallos, la determinación de sus causas raíces, las acciones de corrección, y la toma de medidas de prevención para que el mismo fallo vuelva a ocurrir por los mismos motivos. La definición completa se encuentra en la ISO 9001.</p>
</blockquote>
<p>Diversas herramientas y técnicas son utilizadas en diversas industrias para su aplicación, dentre ellas, PDCA (Plan, Do, Check, Act), DMAIC (Define. Measure, Analyse, Improve, Control), 8D, etc. De manera general cualquier herramienta, técnica o metodologia, es sumarizada en la ISO 9001, en siete &quot;pasos&quot;:</p>
<ol>
<li>Definir el problema. Consiste en definir que el problema sea real, identificar Quien, Qué, Cuando, Dónde y Por qué. En el mundo de la ingeniería de datos, ese paso debe ser, en lo posíble, automático, y el fallo debe ser detectado desde sensores.</li>
<li>Definir el escopo. Consiste en mensurar el problema a se resolver, conociendo su frecuencia, a que procesos o tareas, y stakeholders afecta. Para los procesos de data, muchos de los detalles de escopo ya deberían ser información conocida, desde el diseño de los procesos y tareas, ya la frecuencia puede ser levantada desde los procesos de observability y FRACAS.</li>
<li>Acciones de confinamiento o contención. Son medidas puntuales y adoptadas por el mínimo de tiempo posible, mientras se trabaja en la solución definitiva del fallo. De antemano, tales medidas ya deberían estar diseñadas, para cada tarea o sub-tarea. La selección de medidas debería estar automatizada, de no serlo, se deben implementar de inmediato.</li>
<li>Identificación de causas raíz. Diagnosis clara, precisa y completa del fallo. Su documentación hace parte del FRACAS.</li>
<li>Planeación de acciones correctivas. Consiste en la planeación de acciones de corrección basadas especificamente en la causa raíz.</li>
<li>Implementación de acciones correctivas. Consiste en la implementación final de las acciones correctivas en el proceso, que deben automaticamente estar disponibles cuando fallos similares se presenten.</li>
<li>Acompañamento de los resultados (Follow up). Documentación, comunicación, FRACAS completos.</li>
</ol>
<h2 id="antifragilidad"><a class="header" href="#antifragilidad">Antifragilidad</a></h2>
<blockquote>
<p>Inspirado en el libro <em>Antifragile: Things That Gain from Disorder</em> de Nissim Nicholas Taleb, la antifragilidad difere de los conceptos de resiliencia o robustez, dónde los sistemas buscan mantener su nivel de confiabilidad, sino que desde su diseño, los sistemas aumentar su confiabilidad con respecto a los inputs del sistema.</p>
</blockquote>
<p>La antifragilidad propone un cambio de diseño de los sistemas (en el escopo de este libro, procesos), los cuales comumente son diseñados para ser frágiles, en el sentido de que si el mismo opera fuera de sus requerimientos, lo mismo fallará. La antifragilidad propone lo contrário, diseñar sistemas que se vuelven mejores cuando expuestos a cargas fuera de los requerimientos. En ese sentido, los sistemas no son diseñados para responder solamente a lo esperado o anticipado, sino que interactúan con su entorno en tiempo real y se adaptan a ello.</p>
<ul>
<li>Self-healing</li>
<li>Real time sensoring, monitoring</li>
<li>Live FRACAS</li>
<li>System Health Management</li>
<li>Automatic Repair</li>
</ul>
<p>Links:</p>
<ul>
<li><a href="https://ntrs.nasa.gov/api/citations/20140010075/downloads/20140010075.pdf">NASA</a></li>
<li><a href="https://refuses.github.io/preprints/antifragile.pdf">Refuses</a></li>
</ul>
<h2 id="bulkhead-pattern"><a class="header" href="#bulkhead-pattern">Bulkhead Pattern</a></h2>
<blockquote>
<p>En el mundo náutico encontramos los mamparos, placas de madera que encontramos en los barcos, que buscan que el barco no naufrague cuando se tiene comprometida una porción del casco. Para los sistemas el Bulkhead Pattern adapta exactamente esa idea, de que un fallo en una porción del sistema no comprometa el sistema en su totalidad.</p>
</blockquote>
<p>Este design pattern es aplicado comumente en el desarrollo de software, consiste en no sobrecargar un servicio con más llamadas de las que puede soportar en un determinado tiempo, un ejemplo de eso es Hystrix, de Netflix.</p>
<p>Links:</p>
<ul>
<li><a href="https://github.com/Netflix/Hystrix/wiki/How-it-Works">Netflix</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="calidad-de-datos"><a class="header" href="#calidad-de-datos">Calidad de Datos</a></h1>
<h2 id="fundamentos-de-la-calidad-de-datos"><a class="header" href="#fundamentos-de-la-calidad-de-datos">Fundamentos de la Calidad de Datos</a></h2>
<h3 id="ciclo-de-vida-de-los-datos"><a class="header" href="#ciclo-de-vida-de-los-datos">Ciclo de vida de los datos</a></h3>
<h3 id="dama"><a class="header" href="#dama">DAMA</a></h3>
<h3 id="posmad"><a class="header" href="#posmad">POSMAD</a></h3>
<h3 id="cobit"><a class="header" href="#cobit">COBIT</a></h3>
<h3 id="gobierno-versus-gestión-de-los-datos"><a class="header" href="#gobierno-versus-gestión-de-los-datos">Gobierno versus Gestión de los datos</a></h3>
<h2 id="datos-maestros"><a class="header" href="#datos-maestros">Datos Maestros</a></h2>
<h3 id="arquitectura-mdm"><a class="header" href="#arquitectura-mdm">Arquitectura MDM</a></h3>
<h3 id="modelo-de-madurez"><a class="header" href="#modelo-de-madurez">Modelo de madurez</a></h3>
<h3 id="estándares"><a class="header" href="#estándares">Estándares</a></h3>
<h4 id="iso-8000"><a class="header" href="#iso-8000">ISO 8000</a></h4>
<h4 id="isoiec-22745"><a class="header" href="#isoiec-22745">ISO/IEC 22745</a></h4>
<h2 id="calidad-de-los-procesos-de-datos"><a class="header" href="#calidad-de-los-procesos-de-datos">Calidad de los procesos de datos</a></h2>
<h3 id="dama-dmbok"><a class="header" href="#dama-dmbok">DAMA DMBOK</a></h3>
<h3 id="modelo-de-aiken"><a class="header" href="#modelo-de-aiken">Modelo de Aiken</a></h3>
<h3 id="data-management-maturity-model-dmm"><a class="header" href="#data-management-maturity-model-dmm">Data Management Maturity Model (DMM)</a></h3>
<h3 id="modelo-ibm"><a class="header" href="#modelo-ibm">Modelo IBM</a></h3>
<h3 id="modelo-de-gartner"><a class="header" href="#modelo-de-gartner">Modelo de Gartner</a></h3>
<h3 id="tqdm"><a class="header" href="#tqdm">TQDM</a></h3>
<h3 id="dcam"><a class="header" href="#dcam">DCAM</a></h3>
<h3 id="modelo-mamd"><a class="header" href="#modelo-mamd">Modelo MAMD</a></h3>
<h2 id="modelos-de-calidad-de-datos"><a class="header" href="#modelos-de-calidad-de-datos">Modelos de calidad de datos</a></h2>
<h3 id="modelo-de-calidad-de-datos"><a class="header" href="#modelo-de-calidad-de-datos">Modelo de calidad de datos</a></h3>
<h3 id="medidas-de-calidad-de-datos"><a class="header" href="#medidas-de-calidad-de-datos">Medidas de calidad de datos</a></h3>
<h3 id="proceso-de-evaluación"><a class="header" href="#proceso-de-evaluación">Proceso de evaluación</a></h3>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="confiabilidad-de-datos"><a class="header" href="#confiabilidad-de-datos">Confiabilidad de Datos</a></h1>
<blockquote>
<p>Tomándose por base los fundamentos y metodologías desarrolladas por los <strong>Site Reliability Engineers</strong>, conocidos como los “firefighters” del mundo de la ingeniería de sistemas, los cuales construyen sistemas automatizados para optimizar la disponibilidad de las aplicaciones (reducir downtime), definiremos <strong>Data Reliability</strong> cómo la capacidad del equipo de data en entregar alta disponibilidad de la data durante todo el ciclo de vida de la misma. En resumen, garantizar los periodos de tiempo que la data no presenta inacurácia, no es faltante ni errónea.</p>
</blockquote>
<p>...</p>
<h2 id="consecuencias-de-la-confiabilidad"><a class="header" href="#consecuencias-de-la-confiabilidad">Consecuencias de la confiabilidad</a></h2>
<p>Consecuencias (en administrar el downtime de la data):</p>
<ul>
<li>Los equipos de data reducen de manera muy importante el tiempo perdido en “apagar incendios”, escalaciones y troubleshooting de la data. Utilizan ese tiempo para enfocarse en la construcción de una buena infraestructura, y en agregar valor a la data.</li>
<li>Los equipos de data son más rápidos en actualizar y modificar la infraestructura de la data, ya que tienen claro la confiabilidad del sistema.</li>
<li>Los equipos de data ganan el respeto y la confianza de los stakeholders, ya que entregan datos confiables de manera consistente.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="data-architecture-1"><a class="header" href="#data-architecture-1">Data Architecture</a></h1>
<blockquote>
<p>Data Architecture is about how the data is managed, from <strong>collection</strong>, <strong>transformations</strong>, <strong>distribution</strong>, and <strong>consumption</strong>.</p>
</blockquote>
<p>It includes the models, policies, rules, and standards that govern which data is collected and how it is stored, arranged, integrated, and put to use in data systems and in organizations.</p>
<p>The data architecture aims to set standards to all data systems, and the interaction between them. It also dictates and materialises the organization understanding of its business in a Conceptual, Logical, and Physical level.</p>
<p>The Zachman Framework for enterprise architecture, understands the data architecture in five layers:</p>
<ol>
<li>Scope/Contextual: subjects and architectural standards important for the business.</li>
<li>Business/Conceptual: business entities, its attributes and associations.</li>
<li>System/Logical: how entities are related.</li>
<li>Technology/Physical: representation of a data design as implemented in a database management system.</li>
<li>Detailed Representations: databases.</li>
</ol>
<h2 id="conceptual-layer"><a class="header" href="#conceptual-layer">Conceptual Layer</a></h2>
<blockquote>
<p>Represents all <strong>Business Entities</strong>.</p>
</blockquote>
<p>The <strong>Conceptual Data Model</strong> (CDM) consists of the <strong>Business Entities</strong>, like <strong>User</strong>, <strong>Branch</strong>, <strong>Product</strong>.
The business entities (or business objects) carry <em>attributes</em> (name, identifiers, timestamps, etc.), and <em>associations</em> (relationships) with other business entities.
The complete set of business entities represents the business relationships.</p>
<p>From a data architecture perspective, these business entities are represented in a <strong>Conceptual Schema</strong>, which consists of a map of <strong>concepts</strong> (business entities) and their <strong>relationships</strong> in a database, normally in a <strong>Data Structure Diagram</strong> (DSD). It may also include <strong>Enterprise Data Modelling</strong> (EDM) outputs like  entity–relationship diagrams (ERDs), XML schemas (XSD), and an enterprise wide data dictionary.</p>
<p>The conceptual schema describes the semantics of an organization, and represents a series of assertions about its nature. It describes the objects of significance (business entities), of which the organization is ineterested in collecting information of, its characteristics (attributes), and the associations between each pair of objects of significance (relationships).
Please note that it's not the actual database design, and it is represented in different abrastraction layers.</p>
<p>Examples:</p>
<ul>
<li>Each ORDER must be from one and only one USER.</li>
<li>Each ORDER contains one or more PRODUCTS.</li>
<li>Each ORDER contains products from one or more BRANCHES.</li>
</ul>
<h2 id="logical-layer"><a class="header" href="#logical-layer">Logical Layer</a></h2>
<blockquote>
<p>Represents the logic and how the entities are related.</p>
</blockquote>
<p>The <strong>Logical Data Model</strong> (LDM), also known as Domain Model, represents the abstract structure of a domain of information, expressed independently of a particular database management product or storage technology (physical data model), but in terms of data structures such as relational tables and columns, object-oriented classes, or XML tags.</p>
<p>Once validated and approved, the logical data model can become the basis of a physical data model and form the design of a database.</p>
<h2 id="physical-layer"><a class="header" href="#physical-layer">Physical Layer</a></h2>
<blockquote>
<p>The representation of a data design as implemented, or intended to be implemented, in a database management system.</p>
</blockquote>
<p>The <strong>Physical Data Model</strong> (PDM) typically derives from a logical data model (LDM), though it may be reverse-engineered from a given database implementation. A complete physical data model will include all the database artifacts required to create relationships between tables or to achieve performance goals, such as indexes, constraint definitions, linking tables, partitioned tables or clusters.</p>
<h3 id="cdm-vs-ldm-vs-pdm"><a class="header" href="#cdm-vs-ldm-vs-pdm">CDM vs LDM vs PDM</a></h3>
<h4 id="data-constructs"><a class="header" href="#data-constructs">Data Constructs</a></h4>
<ul>
<li>CDM: uses general high-level data constructs from which Architectural Descriptions are created in non-technical terms.</li>
<li>LDM: includes entities (tables), attributes (columns/fields) and relationships (keys). Is independent of technology (platform, DBMS).</li>
<li>PDM:  includes tables, columns, keys, data types, validation rules, database triggers, stored procedures, domains, and access constraints, as primary keys and indices for fast data access.</li>
</ul>
<h4 id="naming-conventions"><a class="header" href="#naming-conventions">Naming Conventions</a></h4>
<ul>
<li>CDM: non-technical names, so that executives and managers at all levels can understand the data basis of Architectural Description.</li>
<li>LDM: uses business names for entities &amp; attributes.</li>
<li>PDM: uses more defined and less generic specific names for tables and columns, such as abbreviated column names, limited by the database management system (DBMS) and any company defined standards.</li>
</ul>
<h2 id="modern-data-architecture"><a class="header" href="#modern-data-architecture">Modern Data Architecture</a></h2>
<p>A modern approach to data architecture, extensivelly adapted by StartUps, reduces, restricts or understands the data architecture as the implementation of a Data Warehouse, a Data Lake + Data Warehouse, or a Data Lakehouse architecture, consisting of the data sources plus two or three tiers:</p>
<ul>
<li>Bottom/Data Tier: data warehouse server with functional gateway (ODBC, JDBC, etc.). May also include the data lake.</li>
<li>Middle/Application Tier: houses the business logic used to process user inputs (OLAP Servers, Snowflake, Apache Redshift, Databricks Data Lakehouse Platform, Apache Spark, etc.).</li>
<li>Top/Presentation Tier: front-end tools (Power BI, Tableau, etc.).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="data-lake"><a class="header" href="#data-lake">Data Lake</a></h1>
<blockquote>
<p>A Data Lake is a data repository for storing large amounts of Structured, Semi-Structured, and Unstructured data. </p>
</blockquote>
<p>It is a repository for storing all types of data in its native format without fixed limits on account size or file. Data Lake stores a high data quantity to increase native integration and analytic performance.
The Data Lake democratizes data and provides a cost-effective way of storing all organization data for later processing.</p>
<ul>
<li>auto-gen TOC;
{:toc}</li>
</ul>
<h2 id="data-lake-vs-data-warehouse"><a class="header" href="#data-lake-vs-data-warehouse">Data Lake vs Data Warehouse</a></h2>
<blockquote>
<p>A Data Warehouse is a repository that exclusively keeps pre-processed data from a Data Lake or many databases.</p>
</blockquote>
<p>ETL operations are used to arrange data in multi-dimensional structures so that Analytics workflows using Data Warehouses can be accelerated.
Business Intelligence specialists and Data Analysts can generate reports and develop dashboards using the data housed in a Data Warehouse.</p>
<p>Data Warehouses store data in a hierarchical format using files and folders. This is not the case with a Data Lake as Data Lake Architecture is a flat architecture. In a Data Lake, every data element is identified by a unique identifier and a set of metadata information.</p>
<h2 id="goals"><a class="header" href="#goals">Goals</a></h2>
<blockquote>
<p>Building and maintaining a Data Lake have five main goals: unifying the data, full query access, performance and scalability, progression, and costs.</p>
</blockquote>
<p><strong>Unification</strong>: Data Lake is a perfect solution to accumulate all the data from distinct data sources (ERP, CRM, logs, data partners data, internal generated data) in one place. The Data Lake Architecture makes it easier for companies to get a holistic view of data and generate insights from it.</p>
<p><strong>Full Query Access</strong>: storing data in Data Lakes allows full access to data that can be directly used by BI tools to pull data whenever needed. ELT process is a flexible, reliable, and fast way to load data into Data Lake and then use it with other tools.</p>
<p><strong>Performance and Scalability</strong>: Data Lake Architecture supports fast query processing. It enables users to perform ad hoc analytical queries independent of the production environment. Data Lake provides faster querying and makes it easier to scale up and down. Data Lake offer business agility.</p>
<p><strong>Progression</strong>: getting data in one place is a necessary step before progressing to other stages because loading data from one source makes it easier to work with BI tools. Data Lake helps you make data cleaner and error-free data that has less repetition.</p>
<p><strong>Costs</strong>: S3 repositories are a cost-efficient storage of large volumes of data.</p>
<h2 id="data-lake-architecture"><a class="header" href="#data-lake-architecture">Data Lake Architecture</a></h2>
<div style="position: relative; width: 100%; height: 0; padding-top: 50.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbUxswrb4&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbUxswrb4&#x2F;view?utm_content=DAFbUxswrb4&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Data Lake Architecture Framework.</p></a>
<blockquote>
<p>Data Lakes are often structured in zones or layers models. These models define in which processing degrees (raw, cleansed, aggregated) data are available in the data lake, and how they are governed (regarding access rights, data quality, and responsibilities).</p>
</blockquote>
<p>Zones are similar to the layers in data warehousing, but data may not move through all zones or even move back.</p>
<h3 id="layers"><a class="header" href="#layers">Layers</a></h3>
<div style="position: relative; width: 100%; height: 0; padding-top: 35.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbVHIphys&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbVHIphys&#x2F;view?utm_content=DAFbVHIphys&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Data Lake Layers.</p></a>
<p>The following data lake model approach is structured in layers:</p>
<ul>
<li>Ingestion Layer</li>
<li>Distilation Layer</li>
<li>Processing Layer</li>
<li>Insights Layer</li>
<li>Unified Operations Layer</li>
</ul>
<p>The <strong>Raw Data</strong> entering the Data Lake consists of the organizations internal data (Operational Systems), specially relational data from databases, also streaming and batch data from data partners.</p>
<p>In the other extreme, representing the data leaving the Data Lake, the <strong>Business Systems</strong>, consists of databases, the Data Warehouse, dashboards, reports, and external data connections.</p>
<p>The first three layers constitute the medallion architecture, which is a is a data design pattern used to logically organize data in a Data Lake (similar as in a Data Lakehouse), with the goal of incrementally and progressively improving the structure and quality of data as it flows through each layer of the architecture (from Bronze ⇒ Silver ⇒ Gold layer tables).
Medallion architectures are sometimes also referred to as &quot;multi-hop&quot; architectures.</p>
<h4 id="ingestion-layer-bronze"><a class="header" href="#ingestion-layer-bronze">Ingestion Layer (Bronze)</a></h4>
<blockquote>
<p>The purpose of the Ingestion Layer of the Data Lake Architecture is to ingest raw data into the Data Lake. There is no data modification in this layer. This is where we land all the data from external source systems.</p>
</blockquote>
<p>The table structures in this layer correspond to the source system table structures &quot;as-is,&quot; along with any additional metadata columns that capture the load date/time, process ID, etc.
The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.</p>
<p>The layer can ingest raw data in real-time or in batches, which is in turn organized into a logical folder structure.
The Ingestion Layer can pull data from different external sources, like social media platforms.</p>
<h4 id="distillation-layer-silver"><a class="header" href="#distillation-layer-silver">Distillation Layer (Silver)</a></h4>
<blockquote>
<p>The purpose of the Distillation Layer of the Data Lake Architecture is to convert the data stored in the Ingestion (Bronze) Layer in a Structured format for analytics. </p>
</blockquote>
<p>The data is matched, denormalized, merged, conformed, cleansed, and derive &quot;just-enough&quot; so that the Silver layer can provide an &quot;Enterprise view&quot; of all its key business entities, concepts and transactions (for example, master customers, stores, non-duplicated transactions and cross-reference tables).
The data in this layer becomes uniform in terms of format, encoding, and data type (parquet).</p>
<p>The Silver layer brings the data from different sources into an Enterprise view and enables self-service analytics for ad-hoc reporting, advanced analytics and ML. 
It serves as a source for Departmental Analysts, Data Engineers and Data Scientists to further create projects and analysis to answer business problems via enterprise and departmental data projects in the Gold Layer.</p>
<p>In the lakehouse data engineering paradigm (of which we’re extending to the Data Lake), typically the ELT methodology is followed vs. ETL - which means only minimal or &quot;just-enough&quot; transformations and data cleansing rules are applied while loading the Silver layer.
Speed and agility to ingest and deliver the data in the data lake is prioritized, and a lot of project-specific complex transformations and business rules are applied while loading the data from the Silver to Gold layer.
From a data modeling perspective, the Silver Layer has more 3rd-Normal Form like data models. Data Vault-like, write-performant data models can be used in this layer.</p>
<h4 id="processing-layer-gold"><a class="header" href="#processing-layer-gold">Processing Layer (Gold)</a></h4>
<blockquote>
<p>This layer of the Data Lake Architecture executes user queries and advanced analytical tools on the Structured Data. </p>
</blockquote>
<p>The processes can be run in batch, in real-time, or interactively. 
It is the layer that implements the business logic and analytical applications consume the data.
It is also known as the Trusted, Gold, or Production-Ready Layer. </p>
<p>It is typically organized in consumption-ready &quot;project-specific&quot; databases.
The Gold layer is for reporting and uses more de-normalized and read-optimized data models with fewer joins.
The final layer of data transformations and data quality rules are applied here.
Final presentation layer of projects such as Customer Analytics, Product Quality Analytics, Inventory Analytics, Customer Segmentation, Product Recommendations, Marking/Sales Analytics etc. fit in this layer.
We see a lot of Kimball style star schema-based data models or Inmon style Data marts fit.</p>
<p>Often, the Data Marts (and Data Warehouse data) from the traditional RDBMS technology stack are ingested into the Gold layer.</p>
<h4 id="insights-layer"><a class="header" href="#insights-layer">Insights Layer</a></h4>
<blockquote>
<p>This layer of the Data Lake Architecture acts as the query interface, or the output interface, of the Data Lake. </p>
</blockquote>
<p>It uses SQL and NoSQL queries to request or fetch data from the Data Lake.
The queries are normally executed by company users who need access to the data.
Once the data is fetched from the Data Lake, it is the same layer that displays it to the user for viewing. </p>
<p>Some examples include Amazon QuickSight, an AWS native BI tool and allows users to connect with software-as-a-service (SaaS) applications such as Salesforce or ServiceNow, third-party databases such as MySQL, Postgres, and SQL Server, as well as native AWS services including Amazon Athena, an interactive query service that allows them to analyze unstructured data in Amazon S3 data lakes using standard SQL queries.
While QuickSight doesn’t connect directly to the data lake, integration with Amazon Athena allows BI users to query data inside the lake without having to move data or build an ETL pipeline.</p>
<h4 id="unified-operations-layer"><a class="header" href="#unified-operations-layer">Unified Operations Layer</a></h4>
<blockquote>
<p>This layer governs system management and monitoring.</p>
</blockquote>
<p>It includes auditing and proficiency management, data management, workflow management. AWS data lake environments and monitoring tools and best practices are described here.</p>
<h3 id="zones"><a class="header" href="#zones">Zones</a></h3>
<div style="position: relative; width: 100%; height: 0; padding-top: 35.0000%;
 padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden;
 border-radius: 8px; will-change: transform;">
  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"
    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbVKs6Or4&#x2F;view?embed" allowfullscreen="allowfullscreen" allow="fullscreen">
  </iframe>
</div>
<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAFbVKs6Or4&#x2F;view?utm_content=DAFbVKs6Or4&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener"><p style="text-align: center;">Zone Reference Model for Enterprise-Grade Data Lake Management - Data Flow.</p></a>
<p>The following data lake approach is explored by the <a href="https://www.ipvs.uni-stuttgart.de/departments/as/publications/giebleca/20_zoneReferenceModel_EDOC_Preprint.pdf">University of Stuttgart and Bosch GmbH</a>, and known as Zone Reference Model for Enterprise-Grade Data Lake Management. It consists of:</p>
<ul>
<li>Landing Zone</li>
<li>Raw Zone</li>
<li>Harmonized Zone</li>
<li>Distilled Zone</li>
<li>Delivery Zone</li>
<li>Explorative Zone</li>
</ul>
<pre><code class="language-plantuml">@startmindmap
+ Zone
++_ Properties
+++_ Governed
+++_ Historized
+++_ Persistent
+++_ Protected
+++_ Use Case Independent
--_ Data Characteristics
---_ Granularity
---_ Schema
---_ Syntax
---_ Semantics
--_ User Groups
--_ Name
--_ Modelling Approach
@endmindmap
</code></pre>
<p style="text-align: center;">Zone Reference Model for Enterprise-Grade Data Lake Management - Meta-model for zones - Attributes.</p>
<p>The following describes how a zone interacts with other zones and the outside world.</p>
<pre><code class="language-plantuml">@startuml
:Zone;
@enduml
</code></pre>
<p style="text-align: center;">Zone Reference Model for Enterprise-Grade Data Lake Management - Meta-model for zones - Interactions.</p>
<p>All zones contain a protected part.
This part is encrypted and secured, and stores data that need extensive protection (for example, PII, personal data). Data wander from the protected part of one zone to the protected part of the next zone.
They may only leave the protected part after being desensitized (for example, by anonymization).
Data in this part are subject to strict access controls and governance.
The protected part shares all other characteristics with the rest of the zone it is in.</p>
<h4 id="landing-zone"><a class="header" href="#landing-zone">Landing Zone</a></h4>
<blockquote>
<p>The Landing Zone is the first zone of the data lake. Data are ingested as batch or as data stream from the sources.</p>
</blockquote>
<p>The Landing Zone is beneficial when the requirements of the ingested data and those of the Raw Zone diverge.
For example, data might need to be ingested at a vast rate due to its volume and velocity.
If the technical implementation of the Raw Zone cannot provide this high ingestion rate, a Landing Zone can function as a mediator in between: data are ingested at a high rate into the Landing Zone, and then are forwarded to the Raw Zone as batches.
Examples of data coming to the Landing zone include data streamed by Kafka, Amazon Kinesis, Amazon SQS, RabbitMQ, Apache Spark, etc.</p>
<p>For the data characteristics, data ingested into the Landing Zone remains mostly raw.
Their granularity remains raw, just like in the source systems.
The schema of the data is not changed; they can simply be copied in their source system format.
However, their syntax might be changed.
Basic transformations are allowed upon ingestion into the Landing Zone, such as adjusting the character set of strings or transforming timestamps into a common format.
In addition, data may be masked or anonymized to comply with legal regulations.
Aside from these changes, the semantic of the data remains the same as in the source systems.</p>
<h4 id="raw-zone"><a class="header" href="#raw-zone">Raw Zone</a></h4>
<blockquote>
<p>All data in the data lake is available in mostly raw format in the Raw Zone. Only basic transformations (see Landing Zone) are applied on the data. If the Landing Zone is omitted, these transformations are performed in the Raw Zone.</p>
</blockquote>
<p>Differently from the Landing Zone, the Raw Zone stores data persistently.
In general, data should neither be manipulated nor deleted from the Raw Zone.
This zone persists (when possible) the original data type (json, csv, xml).</p>
<h4 id="harmonized-zone"><a class="header" href="#harmonized-zone">Harmonized Zone</a></h4>
<blockquote>
<p>The Harmonized Zone is the place where master data are accessible for analyses.</p>
</blockquote>
<p>A subset of the data stored in the Raw Zone is passed to the Harmonized Zone in a demand-based manner.
It is important to note that these data are not deleted from the Raw Zone.
Instead, the Harmonized Zone contains a copy of or a view on the data in the Raw Zone.
The Harmonized Zone is also the place where master data are accessible for analyses.
As these data are crucial for enterprises, master data management is of high importance in the data lake.
Thus, they should exclusively be accessed after being cleansed.</p>
<p>The data characteristics in this zone differ greatly from those in the Raw Zone.
Data schema and syntax change when compared to the source data.
Data from different source systems are integrated into a consolidated schema, regardless of their structure.
The data syntax is also consolidated in the Harmonized Zone: when data from multiple source systems are merged, data types have to be adapted.</p>
<p>The aim of the Harmonized Zone is to provide a harmonized and consolidated view on data.
To this end, the Harmonized Zone uses a standardized modeling approach (dimensional modeling or Data Vault) that all of the enterprise’s data are modeled in.
The files in this zone facilitates the ingestion (parquet).</p>
<h4 id="distilled-zone"><a class="header" href="#distilled-zone">Distilled Zone</a></h4>
<blockquote>
<p>Prepares data for processing and facilitates ingestion.</p>
</blockquote>
<p>In contrast to the Raw and Harmonized Zone, where the focus is to quickly make data available for use, the Distilled Zone focuses on increasing the efficiency of following analyses by preparing the data accordingly.
The granularity of the data may be changed (for example, data may be aggregated for the calculation of KPIs).
Complex processing is applied that change the data’s semantics but are too extensive for the Landing Zone, Raw Zone, and Harmonized Zone.
However, the schema might also change slightly, depending on the supported use case (for example, fields to enrich the data could be added).
The files in this zone facilitates the ingestion (parquet).</p>
<h4 id="explorative-zone"><a class="header" href="#explorative-zone">Explorative Zone</a></h4>
<blockquote>
<p>The Explorative Zone is the place where data scientists can play with and flexibly use the data.</p>
</blockquote>
<p>Data scientists can use and explore data in the data lake in any way they desire, except for sensitive data. These data are only usable according to strict rules. Granularity, schema, syntax, and semantic may be changed in any way necessary for analyses.</p>
<h4 id="delivery-zone"><a class="header" href="#delivery-zone">Delivery Zone</a></h4>
<blockquote>
<p>In the Delivery Zone, small subsets of data are tailored to specific usage and applications.</p>
</blockquote>
<p>This does not only include analytical use cases, such as reporting and OLAP, but also operational use cases.
This zone thus provides functionality similar to data marts and operational data stores in data warehousing.
Data from this zone may be forwarded to external data sinks.</p>
<p>The Delivery Zone especially supports users with little knowledge on data analytics.
Data have to be easily findable and importable into various analytics tools.
As for the modeling approach, data are available in whatever format supports the intended use case best, for example, dimensional modeling for OLAP, or flat tables for operational use.</p>
<h4 id="zones-comparisom"><a class="header" href="#zones-comparisom">Zones Comparisom</a></h4>
<table>
    <tr>
        <td></td>
        <td><strong>Landing</strong></td>
        <td><strong>Raw</strong></td>
        <td><strong>Harmonized</strong></td>
        <td><strong>Distilled</strong></td>
        <td><strong>Explorative</strong></td>
        <td><strong>Delivery</strong></td>
    </tr>
    <tr>
        <td><strong>Granularity</strong></td>
        <td>Raw</td>
        <td>Raw</td>
        <td>Raw</td>
        <td>Aggregated</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
    <tr>
        <td><strong>Schema</strong></td>
        <td>Any</td>
        <td>Any</td>
        <td>Consolidated</td>
        <td>Consolidated, Enriched</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
    <tr>
        <td><strong>Syntax</strong></td>
        <td>Basic transformations</td>
        <td>Basic transformations</td>
        <td>Consolidated</td>
        <td>Consolidated</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
    <tr>
        <td><strong>Semantics</strong></td>
        <td>Mostly unchanged, unless needed for compliance</td>
        <td>Mostly unchanged, unless needed for compliance</td>
        <td>Mostly unchanged, unless needed for compliance</td>
        <td>Complex processing</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
    <tr>
        <td><strong>Governed</strong></td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>False</td>
        <td>True</td>
    </tr>
    <tr>
        <td><strong>Historized</strong></td>
        <td>False</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>N/A</td>
        <td>N/A</td>
    </tr>
    <tr>
        <td><strong>Persistent</strong></td>
        <td>False</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>False</td>
        <td>True</td>
    </tr>
    <tr>
        <td><strong>Has Protected Part</strong></td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
    </tr>
    <tr>
        <td><strong>Use Case Dependent</strong></td>
        <td>False</td>
        <td>False</td>
        <td>False</td>
        <td>True</td>
        <td>True</td>
        <td>True</td>
    </tr>
    <tr>
        <td><strong>User Groups</strong></td>
        <td>Systems, Processes</td>
        <td>Data Scientists, Systems, Processes</td>
        <td>Data Scientists, Systems, Processes</td>
        <td>Data Scientists, Domain Experts, Systems, Processes</td>
        <td>Data Scientists</td>
        <td>Anyone</td>
    </tr>
    <tr>
        <td><strong>Modelling Approach</strong></td>
        <td>Any</td>
        <td>Any</td>
        <td>Standardized</td>
        <td>Standardized</td>
        <td>Any</td>
        <td>Any</td>
    </tr>
</table>
<p style="text-align: center;">Zone Reference Model for Enterprise-Grade Data Lake Management - Zones comparison.</p>
<h3 id="sandbox"><a class="header" href="#sandbox">Sandbox</a></h3>
<blockquote>
<p>Also known as the Analytics Sandbox, it provides data scientists and advanced analysts with a place for data exploration.</p>
</blockquote>
<p>An Analytics Sandbox is a separate environment that is part of the overall data lake architecture, meaning that it is a centralized environment meant to be used by multiple users and is maintained with the support of IT. Some key characteristics of this layer:</p>
<ul>
<li>The environment is controlled by the analyst</li>
<li>Allows them to install and use the data tools of their choice</li>
<li>Allows them to manage the scheduling and processing of the data assets</li>
<li>Enables analysts to explore and experiment with internal and external data</li>
<li>Can hold and process large amounts of data efficiently from many different data sources; big data (unstructured), transactional data (structured), web data, social media data, documents, etc.</li>
</ul>
<p>There are many advantages to having an Analytics Sandbox as part of your data architecture.
The most important is that it decreases the amount of time that it takes a business to gain knowledge and insight from their data.
It does this by providing an on-demand/always ready environment that allows analysts to quickly dive into and process large amounts of data and prototype their solutions without kicking off a big BI project.
In other words, it enables agile BI by empowering your advanced users.</p>
<p>Another major benefit to the business and IT team is that by giving the business a place to prototype their data solutions it allows the business to figure what they want on their own without involving IT.
When they decide that a solution is adding business value, it becomes a good candidate for something that should be productionized and built into the Data Warehouse process at some point.
This saves both teams a lot of time and effort.</p>
<h3 id="maturity-stages"><a class="header" href="#maturity-stages">Maturity Stages</a></h3>
<p>The implementation of a Data Lake solution consists of some main maturity stages.</p>
<ol>
<li>Handle and ingest data at scale</li>
<li>Building the analytical muscle</li>
<li>Data Warehouse and Data Lake working in unison</li>
<li>Enterprise capability</li>
</ol>
<h4 id="handle-and-ingest-data-at-scale"><a class="header" href="#handle-and-ingest-data-at-scale">Handle and Ingest data at scale</a></h4>
<blockquote>
<p>This stage consists in improving the ability to transform and analyze data.</p>
</blockquote>
<h4 id="building-the-analytical-muscle"><a class="header" href="#building-the-analytical-muscle">Building the analytical muscle</a></h4>
<blockquote>
<p>This stage involves improving the ability to transform and analyze data.</p>
</blockquote>
<p>In this stage, the company start acquiring more data and building applications.
In this stage, capabilities of the Data Warehouse and the Data Lake are used together.</p>
<h4 id="data-warehouse-and-data-lake-work-in-unison"><a class="header" href="#data-warehouse-and-data-lake-work-in-unison">Data Warehouse and Data Lake work in unison</a></h4>
<blockquote>
<p>This step involves getting data and analytics into the hands of as many people as possible.</p>
</blockquote>
<p>In this stage, the Data Lake and the Data Warehouse start to work in a union.
Both playing their part in analytics.</p>
<h4 id="enterprise-capability-in-the-lake"><a class="header" href="#enterprise-capability-in-the-lake">Enterprise capability in the lake</a></h4>
<blockquote>
<p>In this maturity stage of the data lake, enterprise capabilities are added to the Data Lake.</p>
</blockquote>
<p>It includes the adoption of information governance, information lifecycle management capabilities, and Metadata management.</p>
<h2 id="key-components-of-data-lake-architecture"><a class="header" href="#key-components-of-data-lake-architecture">Key Components of Data Lake Architecture</a></h2>
<p>#TODO: draw and include here diagram of data lake architecture including data sources, data processing layer, and data targets.</p>
<h4 id="data-ingestion"><a class="header" href="#data-ingestion">Data Ingestion</a></h4>
<blockquote>
<p>Data Ingestion allows connectors to get data from a different data sources and load into the Data Lake.</p>
</blockquote>
<p>Data Ingestion supports:</p>
<ul>
<li>All types of Structured, Semi-Structured, and Unstructured data.</li>
<li>Multiple ingestions like Batch, Real-Time, One-time load.</li>
<li>Many types of data sources like Databases, Webservers, Emails, and FTP.</li>
</ul>
<h4 id="data-storage"><a class="header" href="#data-storage">Data Storage</a></h4>
<blockquote>
<p>Data storage should be scalable, offers cost-effective storage and allow fast access to data exploration. It should support various data formats.</p>
</blockquote>
<h4 id="data-governance"><a class="header" href="#data-governance">Data Governance</a></h4>
<blockquote>
<p>Data governance is a process of managing availability, usability, security, and integrity of data used in an organization.</p>
</blockquote>
<h4 id="security"><a class="header" href="#security">Security</a></h4>
<blockquote>
<p>Security needs to be implemented in every layer of the Data Lake. It starts with Storage, Unearthing, and Consumption. The basic need is to stop access for unauthorized users. It should support different tools to access data with easy to navigate GUI and Dashboards.</p>
</blockquote>
<p>Authentication, Accounting, Authorization and Data Protection are some important features of Data Lake security.</p>
<h4 id="data-quality-1"><a class="header" href="#data-quality-1">Data Quality</a></h4>
<blockquote>
<p>Data quality is an essential component of Data Lake architecture. Data is used to exact business value. Extracting insights from poor quality data will lead to poor quality insights.</p>
</blockquote>
<h4 id="data-discovery"><a class="header" href="#data-discovery">Data Discovery</a></h4>
<blockquote>
<p>Data Discovery is another important stage before you can begin preparing data or analysis. In this stage, tagging technique is used to express the data understanding, by organizing and interpreting the data ingested in the Data Lake.</p>
</blockquote>
<h4 id="data-auditing"><a class="header" href="#data-auditing">Data Auditing</a></h4>
<blockquote>
<p>Data auditing helps to evaluate risk and compliance.</p>
</blockquote>
<p>The main Data auditing tasks are:</p>
<ul>
<li>Tracking changes to important dataset elements</li>
<li>Captures how/when/who changes to these elements.</li>
</ul>
<h4 id="data-lineage"><a class="header" href="#data-lineage">Data Lineage</a></h4>
<blockquote>
<p>This component deals with data’s origins. It mainly deals with where it movers over time and what happens to it. It eases errors corrections in a data analytics process from origin to destination.</p>
</blockquote>
<h4 id="data-exploration"><a class="header" href="#data-exploration">Data Exploration</a></h4>
<blockquote>
<p>It is the beginning stage of data analysis. It helps to identify right dataset is vital before starting Data Exploration.</p>
</blockquote>
<p>All given components need to work together to play an important part in Data Lake building easily evolve and explore the environment.</p>
<div style="break-before: page; page-break-before: always;"></div><p>A Data Warehouse (DWH), also known as Enterprise Data Warehouse (EDW) is a central repository of information that can be analyzed to make more informed decisions. </p>
<p>Data flows into a data warehouse from Data Lake, transactional systems, relational databases, and other sources, typically on a regular cadence. Business analysts, data engineers, data scientists, and decision makers access the data through Business Intelligence (BI) tools, SQL clients, and other analytics applications.</p>
<p>Goals
Data Warehouse Architecture
Data Sources
Warehouse
Metadata
Summarized data
End-User access Tools
Two-Tier Architecture
Three-Tier Architecture
Reconciled Layer
Middle Tier
AWS Redshift &amp; Snowflake
Databricks Lakehouse Platform
Data Lakehouse vs Data Warehouse vs Data Lake
OLAP Servers
Relational OLAP Servers (ROLAP)
Multidimensional OLAP Servers (MOLAP)
Hybrid OLAP Servers (HOLAP)
OLAP Servers options
Data Modeling Methodologies
Kimball (Bottom-Up)
Advantages
Disadvantages
Inmon (Top-Down)
Advantages
Disadvantages
Hybrid
Vault
Bus Architecture
Maturity Stages
Key Components of a Data Warehouse
References
Goals
When implementing a data warehouse, the main goals are to achieve:</p>
<p>Consistency: maintain a uniform format to all collected data, making it easier for corporate decision-makers to analyze and share data insights with their colleagues. Standardizing data from different sources also reduces the risk of error in interpretation and improves overall accuracy.</p>
<p>Decision-making: successful business leaders develop data-driven strategies and rarely make decisions without consulting the facts. Data warehousing improves the speed and efficiency of accessing different data sets and makes it easier for corporate decision-makers to derive insights that will guide the business and marketing strategies that set them apart from their competitors.</p>
<p>Improving: allow business leaders to quickly access the organization historical activities and evaluate initiatives that have been successful — or unsuccessful — in the past. This allows executives to see where they can adjust their strategy to decrease costs, maximize efficiency and increase business results.</p>
<p>Single Source of Truth: the whole company would benefit on having a single source of truth, specially when there are multiple data sources to a common business dimension.</p>
<p>Data Warehouse Architecture</p>
<p>There are several data warehouses architecture approaches available. Data warehouses would have in common some key components:</p>
<p>Data Sources
In most architectures approaches, it’s the Source Layer, or Data Source Layer, and consists on all the data sources the Warehouse Layer will consume.</p>
<p>Operational System: is a method used in data warehousing to refer to a system that is used to process the day-to-day transactions of an organization.</p>
<p>In our case, that’s the data coming from our internal services (user service, commerce service, etc.). It’ll be available in the Data Lake, in parquet format.</p>
<p>Flat Files System: is a system of files in which transactional data is stored, and every file in the system must have a different name.</p>
<p>In the current DWH implementation, that’s the data coming from external providers as Braze and Segment. I’ll be also available in the Data Lake, in parquet format.</p>
<p>Warehouse
In most architectures approaches, it’s the Warehouse Layer, or Data Warehouse Layer, and consists on all the data stored in RDBMS database with available gateway access (ODBC, JDBC, etc.). It also contains the metadata, and some degree of data summarization, and business logic applied, which differentiate an DWH database from a Production database.</p>
<p>Metadata
Metadata is the road-map to a data warehouse, it defines the warehouse objects, and acts as a directory. This directory helps the decision support system to locate the contents of a data warehouse.</p>
<p>In the current DWH implementation, there’s no metadata management tool yet in use. The only metadata we’re applying is when the data was last updated in the Data Lake (dl_updated_at), when the data first entered the Data Warehouse (dwh_created_at), and when it was last updated (dwh_updated_at).</p>
<p>It should soon be extended to:</p>
<p>A description of the Data Warehouse structure, including the warehouse schema, dimensions, hierarchies, data mart locations, contents, etc.</p>
<p>Operational metadata, which usually describes the currency level of the stored data (for example, active, archived or purged), and warehouse monitoring information (for example, usage statistics, error reports, audit, etc).</p>
<p>System performance data, which includes indices, used to improve data access and retrieval performance.</p>
<p>Information about the mapping from operational databases, which provides source RDBMSs and their contents, cleaning and transformation rules, etc.</p>
<p>Summarization algorithms, predefined queries, and reports business data, which include business terms and definitions, ownership information, etc.</p>
<p>Metadata management tool example.</p>
<p>Summarized data
The area of the data warehouse that maintains all the predefined lightly and highly summarized (aggregated) data. The main goal is to speed up query performance, and the summarized records are updated continuously as new information is loaded into the warehouse.</p>
<p>In the current DWH implementation, the data is being summarized directly in the Data Marts, using dbt (bottom-down approach). That means we do not have summarized data stored and available for BI teams to work with, nor have a Single Source of Truth applied at data warehouse layer.</p>
<p>End-User access Tools
The principal purpose of a data warehouse is to provide information to the business for strategic decision-making. These end-users interact with the warehouse using end-client access tools.</p>
<p>The examples of some of the end-user access tools can be:</p>
<p>Reporting and Query Tools</p>
<p>Application Development Tools</p>
<p>Executive Information Systems Tools</p>
<p>Online Analytical Processing Tools</p>
<p>Data Mining Tools</p>
<p>In our case, the only tool currently consuming the Data Marts, is Tableau.</p>
<p>Two-Tier Architecture
The current architecture of our data warehouse is close to a Two-Tier architecture. To reach a proper two-tier architecture, we still have to adopt a Data Staging solution, and a Metadata Management solution. The last great step to fully implement it, is to define how the data can be accessed by external tools.</p>
<p>A data staging solution in this context means to have data cleansed and available with a standard schema at the data warehouse level, before the data enters the data marts pipelines.</p>
<p>Once we fully migrate to this architecture (as a middle step to reach a Three-Tier architecture), our architecture will be as follows:</p>
<p>Data Source Layer: A data warehouse system uses a heterogeneous source of data. That data is stored initially to corporate relational databases or legacy databases, or it may come from an information system outside the corporate walls.</p>
<p>Data Staging Layer: The data stored to the source should be extracted, cleansed to remove inconsistencies and fill gaps, and integrated to merge heterogeneous sources into one standard schema. The ETLs can combine heterogeneous schemata, extract, transform, cleanse, validate, filter, and load source data into a data warehouse. Note that this can be achieved in two ways:</p>
<p>Having the Distillation Layer (Silver) in the Data Lake as the Data Staging Layer.</p>
<p>Creating a separated database within the Data Warehouse, or a separated database schema. Following the principle that all the data in the data warehouse should be cleaned and have high quality standards, a separated database should be preferred.</p>
<p>Data Warehouse Layer: Information is saved to one logically centralized individual repository: a data warehouse. The data warehouses can be directly accessed, but it can also be used as a source for creating data marts, which partially replicate data warehouse contents and are designed for specific enterprise departments. Metadata repositories store information on sources, access procedures, data staging, users, data mart schema, and so on.</p>
<p>Analysis Layer: In this layer, integrated data is efficiently, and flexibly accessed to issue reports, dynamically analyze information, and simulate hypothetical business scenarios. It should feature aggregated information navigators, complex query optimizers, and customer-friendly GUIs.</p>
<p>Three-Tier Architecture
Reconciled Layer
Once we fully implement a Two-Tier Architecture, the next step is to adopt a Three-Tier Architecture. For it it’s necessary to implement a Reconciled Layer, and a Middle Tier.</p>
<p>Once we fully implement the Reconciled Layer, our architecture will be as follows:</p>
<p>The Reconciled Layer sits between the source data and data warehouse. The main advantage of the reconciled layer is that it creates a standard reference data model for the whole company. At the same time, it separates the problems of source data extraction and integration from those of data warehouse population. In some cases, the reconciled layer is also directly used to accomplish better operational tasks, such as producing daily reports that cannot be satisfactorily prepared using the corporate applications or generating data flows to feed external processes periodically to benefit from cleaning and integration.</p>
<p>This architecture is especially useful for the extensive, enterprise-wide systems. A disadvantage of this structure is the extra file storage space used through the extra redundant reconciled layer. It also makes the analytical tools a little further away from being real-time.</p>
<p>Middle Tier
As the name of the architecture suggests, it consists of three tiers (levels):</p>
<p>Bottom Tier (Data): data warehouse server with functional gateway (ODBC, JDBC, etc.).</p>
<p>Middle Tier (Application): houses the business logic used to process user inputs. Example: OLAP Servers, Snowflake, Apache Redshift, Databricks Data Lakehouse Platform.</p>
<p>Top Tier (Presentation): front-end tools.</p>
<p>The Bottom and Top tiers were already discussed in details in the previous steps, so we only have left the implementation of the Middle Tier, very important to enable fast querying of the data warehouse. It is important to note the solutions to the middle tier are often referred as the Data Warehouse itself, but they’re only the Application tier/level in a complete data warehouse solution (It’s like saying Redshift is the DWH, when it’s just one part of the complete DWH solution).</p>
<p>Once we fully implement the Middle Tier, our architecture will be as follows:</p>
<p>One could argue that OLAP is dead, at least in the traditional format (solutions like Mondrian), and cloud/modern solutions should be applied to accompany business fast demand for data. In this sense, given costs restrictions, solutions like Apache Kylin seem to be a better approach (details below). When costs are less restricted, and/or data demands increase, other solutions like AWS Redshift, Snowflake, and Databricks Lakehouse are preferred/recommended.</p>
<p>AWS Redshift &amp; Snowflake
AWS Redshift and Snowflake comparison available in this miro dashboard (to be migrated to Confluence soon).</p>
<p>Databricks Lakehouse Platform
It combines the ACID transactions and data governance of data warehouses with the flexibility and cost-efficiency of data lakes to enable business intelligence (BI) and machine learning (ML) on all data.</p>
<p>Data Lakehouse vs Data Warehouse vs Data Lake
Data in the data warehouse is easy to use, but harder to store. The opposite is true for the data lake: it’s easy to ingest and store data, but a pain to consume and query.</p>
<p>The data lakehouse has a layer design, with a warehouse layer on top of a data lake. This architecture, which enables combining structured and unstructured data, makes it efficient for business intelligence and business analysis. Data lakehouses provide structured storage for some types of data and unstructured storage for others while keeping all data in one place.</p>
<p>OLAP Servers
Relational OLAP Servers (ROLAP)
They use a relational or extended-relational DBMS to save and handle warehouse data, and OLAP middleware to provide missing pieces. They work primarily from the data that resides in a relational database, where the base data and dimension tables are stored as relational tables. This model permits the multidimensional analysis of data.</p>
<p>This technique relies on manipulating the data stored in the relational database to give the presence of traditional OLAP's slicing and dicing functionality.</p>
<p>Advantages
Can handle large amounts of information: the data size limitation of ROLAP technology is depends on the data size of the underlying RDBMS. So, ROLAP itself does not restrict the data amount.</p>
<p>RDBMS already comes with a lot of features. So ROLAP technologies, (works on top of the RDBMS) can control these functionalities.</p>
<p>Disadvantages
Performance can be slow: each ROLAP report is a SQL query (or multiple SQL queries) in the relational database, the query time can be prolonged if the underlying data size is large.</p>
<p>Limited by SQL functionalities: ROLAP technology relies on upon developing SQL statements to query the relational database, and SQL statements do not suit all needs.</p>
<p>Multidimensional OLAP Servers (MOLAP)
It is based on a native logical model that directly supports multidimensional data and operations. Data are stored physically into multidimensional arrays, and positional techniques are used to access them.</p>
<p>One of the significant distinctions of MOLAP against a ROLAP is that data are summarized and are stored in an optimized format in a multidimensional cube, instead of in a relational database. In MOLAP model, data are structured into proprietary formats by client's reporting requirements with the calculations pre-generated on the cubes.</p>
<p>Advantages
Excellent Performance: a MOLAP cube is built for fast information retrieval, and is optimal for slicing and dicing operations.</p>
<p>Can perform complex calculations: all evaluation have been pre-generated when the cube is created. Hence, complex calculations are not only possible, but they return quickly.</p>
<p>Disadvantages
Limited in the amount of information it can handle: Because all calculations are performed when the cube is built, it is not possible to contain a large amount of data in the cube itself.</p>
<p>Hybrid OLAP Servers (HOLAP)
It incorporates the best features of MOLAP and ROLAP into a single architecture. It saves more substantial quantities of detailed data in the relational tables while the aggregations are stored in the pre-calculated cubes. HOLAP also can drill through from the cube down to the relational tables for delineated data.</p>
<p>Advantages
It provides benefits of both MOLAP and ROLAP.</p>
<p>It provides fast access at all levels of aggregation.</p>
<p>It balances the disk space requirement, as it only stores the aggregate information on the OLAP server and the detail record remains in the relational database. So no duplicate copy of the detail record is maintained.</p>
<p>Disadvantages
HOLAP architecture is very complicated because it supports both MOLAP and ROLAP servers.</p>
<p>OLAP Servers options
Apache Kylin
Only supports MOLAP and Offline data storage modes. It supports both SQL and MDX queries, have RESTful API capabilities (also ODBC, and JDBC), and can be integrated/connect with Tableau (also Redash, Superset, Zeppelin, Qlik, and Excel).</p>
<p>It supports Real Time processing, partitioning, usage based optimizations, load balancing and clustering. It supports LDAP, SAML, Kerboros authentication.</p>
<p>Mondrian OLAP Server
Only supports ROLAP data storage modes. It supports MDX queries but not SQL, and have REST API capabilities. Does not natively connect with Tableau, but queries can be performed via Java APIs. It supports Real Time processing, and partitioning.</p>
<p>Data Modeling Methodologies
Being one of the most important topics of data warehouse design and architecture, the data modeling methodology choosing process is arduous and polemic, and will impact the whole design and implementation of the data warehouse solution.</p>
<p>Though not planned, or discussed, the current architecture of the data warehouse, is of type Kimball (bottom-up), which means the data marts are first formed based on the business requirements. There are lots of advantages and disadvantages of priming this approach over a top-down approach (or any of the hybrid or alternative methodologies).</p>
<p>Kimball (Bottom-Up)
The design of the Data Marts comes from the business requirements. The primary data sources are then evaluated, and an Extract, Transform and Load (ETL) tool is used to fetch data from several sources and load it into a staging area of the relational database server. Once data is uploaded in the  data warehouse staging area, the next phase includes loading data into a dimensional data warehouse model that is denormalized by nature. This model partitions data into the fact and dimension tables. Kimball dimensional modeling allows users to construct several star schemas to fulfill various reporting needs.</p>
<p>Kimball Approach to Data Warehouse Lifecycle.
Advantages
Fast to construct (quick initial phase): it is fast to construct as no normalization is involved, which means swift execution of the initial phase of the data warehousing design process.</p>
<p>Simplified queries: in a star schema, most data operators can easily comprehend it because of its denormalized structure, which simplifies querying and analysis.</p>
<p>Simplified business management: the data warehouse system footprint is trivial because it focuses on individual business areas and processes rather than the whole company. So, it takes less space in the database, simplifying system management.</p>
<p>Fast data retrieval: as data is segregated into fact tables and dimensions.</p>
<p>Smaller teams: a smaller team of designers and planners is sufficient for data warehouse management because data source systems are stable, and the data warehouse is process-oriented. Also, query optimization is straightforward, predictable, and controllable.</p>
<p>Deeper insights: it allows business intelligence tools to deeper across several star schemas and generates reliable insights.</p>
<p>Disadvantages
No Single Source of Truth: Data isn’t entirely integrated before reporting, so the idea of a single source of truth is lost.</p>
<p>Too prone to data irregularities: this is because in denormalization technique, redundant data is added to database tables.</p>
<p>Too difficult and expensive to add new columns: performance issues may occur due to the addition of columns in the fact table, as these tables are quite in-depth. The addition of new columns can expand the fact table dimensions, affecting its performance.</p>
<p>Can’t respond (well) to business changes: it is too difficult to alter the models.</p>
<p>Not BI-friendly: as it is business process-oriented, instead of focusing on the company as a whole, it cannot handle all the BI reporting requirements.</p>
<p>Inconsistent dimensional view: this model is not strong as top-down approach as dimensional view of data marts is not consistent as it is in Inmon approach.</p>
<p>In brief, the Kimball approach have a low start-up cost, is faster to deliver the first phase of the data warehouse design, is faster to release to production (first version), but is suitable for Tactical business decision support requirements (versus Strategic), and addresses individual business requirements (vs Enterprise-wide). Another important topic that derives from this methodology approach is the Data Warehouse Bus Architecture (described in next topics).</p>
<p>Inmon (Top-Down)
On the other hand, Bill Inmon, the father of data warehousing, came up with the concept to develop a data warehouse which identifies the main subject areas and entities the enterprise works with, such as customers, product, vendor, etc. Inmon’s definition of a data warehouse is that it is a “subject-oriented, nonvolatile, integrated, time-variant collection of data in support of management’s decisions”.</p>
<p>The model then creates a thorough, logical model for every primary entity. For instance, a logical model is constructed for products with all the attributes associated with that entity. This logical model could include many entities, including all the details, aspects, relationships, dependencies, and affiliations.</p>
<p>The Inmon design approach uses the normalized form for building entity structure, avoiding data redundancy as much as possible. This results in clearly identifying business requirements and preventing any data update irregularities. Moreover, the advantage of this top-down approach in database design is that it is robust to business changes and contains a dimensional perspective of data across data mart.</p>
<p>Next, the physical model is constructed, which follows the normalized structure. This Inmon model creates a Single Source of Truth for the whole business to consume. Data loading becomes less complex due to the normalized structure of the model. However, using this arrangement for querying is challenging as it includes numerous tables and links.</p>
<p>This Inmon data warehouse methodology proposes constructing data marts separately for each division, such as finance, marketing sales, etc. All the data entering the data warehouse is integrated. The data warehouse acts as a single data source for various data marts to ensure integrity and consistency across the enterprise.</p>
<p>Advantages
Single Source of Truth: the data warehouse acts as a unified source of truth for the entire business, where all data is integrated.</p>
<p>Very low data redundancy: there’s less possibility of data update irregularities, making the data warehouse ETL processes more straightforward and less susceptible to failure.</p>
<p>Great flexibility: it’s easier to update the data warehouse in case there’s any change in the business requirements or source data.</p>
<p>BI-friendly: It can handle diverse company-wide reporting requirements.</p>
<p>Disadvantages
Increasing complexity: it increases as multiple tables are added to the data model with time.</p>
<p>Skilled Human Resources: resources skilled in data warehouse data modeling are required, which can be expensive and challenging to find.</p>
<p>Slow setup: the preliminary setup and delivery are time-consuming.</p>
<p>Expert management: this approach requires experts to manage a data warehouse effectively.</p>
<p>In brief, the Inmon have a high start-up cost, requires more time to be in production and meet business needs (very large projects with a very broad scope), and requires a bigger team os specialists, but is more suitable for for systems and business changes, better integrates with the whole company, favors Strategic business decision support requirements (vs Tactical), and facilitates Business Intelligence development.</p>
<p>Hybrid
In a hybrid model, the data warehouse is built using the Inmon model, and on top of the integrated data warehouse, the business process oriented data marts are built using the star schema for reporting.</p>
<p>The hybrid approach provides a Single Source of Truth for the data marts, creating a highly flexible solutions from a BI point of view.</p>
<p>Based on the Hub and Spoke Architecture, the hybrid design methodology can also make use of Operational Data Stores (ODS),  integrating and cleaning data from multiple data sources. The information is then parsed into the actual Data Warehouse.</p>
<p>Hybrid methods will normally keep the data in the 3rd normal form, reducing redundancy. Although normal relational database is not efficient for BI reports. Data marts for specific reports can then be built on top of the data warehouse solution.</p>
<p>When the data is denormalized, all the data available is pulled (as advocated by Inmon) while using a denormalized design (as advocated by Kimball). One example is the Carry Forward method.</p>
<p>Another hybrid methodology is the Data Vault, discussed below.</p>
<p>Example of a Hybrid Methodology approach.
Vault
The Vault Data Modeling is a hybrid design, consisting of the best of breed practices from both 3rd normal form and star-schema.</p>
<p>It is not a true 3rd normal form, and breaks some of the rules that 3NF dictates. It is a top-down architecture with bottom-up design, geared to be strictly a data warehouse. It is not geared to be end-user accessible, which when built, still requires the user of a data mart or star-schema based release are for business purposes.</p>
<p>Data Vault data modeling breaks data into a small number of standard components – the most common of which are Hubs, Links and Satellites. </p>
<p>Hubs are entities of interest to the business. They contain just a distinct list of business keys and metadata about when each key was first loaded and from where.</p>
<p>Links connect Hubs and may record a transaction, composition, or other type of relationship between hubs. They contain details of the hubs involved (as foreign keys) and metadata about when the link was first loaded and from where.</p>
<p>Satellites connect to Hubs or Links. They are Point in Time: so we can ask and answer the question, “what did we know when?”. Satellites contain data about their parent Hub or Link and metadata about when the data was loaded, from where, and a business effectivity date.</p>
<p>The data model of the data warehouse is constructed using these components. These are:</p>
<p>Standard: each component is always constructed the same way.</p>
<p>Simple: easy to understand, and with a little practice, easy to apply them to model your system.</p>
<p>Connected: hubs only connect to links and satellites, links only connect to hubs and satellites, and satellites only connect to hubs or links.</p>
<p>Data Vault has staging, vault and mart layers. Star schemas live in the mart layer, each star schema exposes a subset of the vault for a particular group of users.  Typically, hubs and their satellites form dimensions, links and their satellites form facts.</p>
<p>A Data Vault complements the Data Lake and is a solution for organizations that need to integrate and add structure to the data held in the Data Lake.</p>
<p>Data Vault Modeling Methodology.
This is how our Data Warehouse Architecture would look like once the Data Vault Modeling Methodology is implemented:</p>
<p>Bus Architecture
… Inflow, Upflow, Downflow, Outflow and Meta flow.</p>
<p>Maturity Stages
…</p>
<p>Key Components of a Data Warehouse
Data Ingestion: allows connectors to get data from a different data sources and load into the Data Warehouse. The data will normally come from the Data Lake and External Sources connection (Fivetran), through multiple ETLs (Airflow, services, apps, ETL tools and platforms, etc.).</p>
<p>Data Storage: the data is stored in the data warehouse database, a relational database (RDBMS), like Postgres.</p>
<p>Data Governance: is a process of managing availability, usability, security, and integrity of data used in an organization.</p>
<p>Security: it needs to be implemented in every layer of the Data Warehouse. It includes setting up the data warehouse read-only by default, and setting up custom User Groups. It also includes the access to the databases (VPCs, VPNs, Whitelisting, etc.), strong and active DevOps monitoring and the enforcing of best practices in all levels of the data warehouse environment (data ingestion, data marts consumption, ETLs design, etc.).</p>
<p>Data Quality: it is an essential component of Data Warehouse architecture. Data is used to exact business value. Extracting insights from poor quality data will lead to poor quality insights.</p>
<p>Data Discovery: it is another important stage before you can begin preparing data or analysis. All this rely on good metadata, and data modeling.</p>
<p>Data Auditing: it helps to evaluate risk and compliance. Two major Data auditing tasks are tracking changes to the key dataset.</p>
<p>Tracking changes to important dataset elements.</p>
<p>Captures how/when/who changes to these elements.</p>
<p>Data Lineage: it deals with data’s origins. It mainly deals with where it movers over time and what happens to it. It eases errors corrections in a data analytics process from origin to destination. Some data modeling techniques may facilitate lineage in comparison to others (Vault vs Kimball vs Inmon).</p>
<p>Data Exploration: it is the beginning stage of data analysis. It helps to identify right dataset is vital before starting Data Exploration. All given components need to work together to play an important part in Data Warehouse building easily evolve and explore the environment.</p>
<p>References
Data Warehouse - AWS</p>
<p>Data Warehouse Architecture - javatpoint
Data Warehouse Architecture with Introduction, What is Data Warehouse, History of Data Warehouse, Data Warehouse Components, Operational Database Vs Data Warehouse etc.</p>
<p>www.javatpoint.com</p>
<p>Data Warehouse Architecture, Components &amp; Diagram Concepts
This data warehouse architecture tutorial covers all the basic to advance stuff like definitions, characteristics, architectures, components, data marts, and more.</p>
<p>Guru99</p>
<p>Data Warehouse Architecture - GeeksforGeeks
A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.</p>
<p>GeeksforGeeks</p>
<p>Data Warehouse Architecture: Traditional vs. Cloud
Data warehouse architecture is changing. Learn about traditional EDW vs. cloud-based architectures with lower upfront cost, improved scalability and performance.</p>
<p>Panoply</p>
<p>Kimball vs. Inmon in Data Warehouse Architecture</p>
<p>We will discuss about the Kimball vs. Inmon in data warehouse architecture and design approach. We also answer the question of how to choose Kimball or Inmon's architecture to build data warehouse.</p>
<p>zentut
Data Warehouse Concepts: Kimball vs. Inmon Approach</p>
<p>Inmon vs Kimball: Which data warehouse concept should you use to design a data warehouse. Find out in this blog.</p>
<p>Astera</p>
<p>Data vault modeling</p>
<p>Data vault modeling is a database modeling method that is designed to provide long-term historical storage of data coming in from multiple operational systems. It is also a method of looking at historical data that deals with issues such as auditing, tracing of data, loading speed and resilience to change as well as emphasizing the need to trace where all the data in the database came from. This means that every row in a data vault must be accompanied by record source and load date attributes, enabling an auditor to trace values back to the source. It was developed by Daniel (Dan) Linstedt in 2000.</p>
<p>Wikipedia</p>
<p>Data Warehousing concepts: Kimball vs. Inmon vs. Hybrid vs. Vault
Knowledge sharing with intelligence beyond rational...</p>
<p>Drazda</p>
<p>Data warehouse  12 reconciled data layers</p>
<p>Reconciled Data Layers during the ETL process</p>
<p>SlideShare</p>
<p>Full screen view
https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=c80f8aaea5bf58846b0125b460401fed8230c2d2</p>
<p>Data Warehousing - Metadata Concepts
Data Warehousing Metadata Concepts - Metadata is simply defined as data about data. The data that is used to represent other data is known as metadata. For example, the index of a book serves as a metadata for the contents in the book. In other words, we can say that metadata is the summarized data that leads us to detailed data. In te</p>
<p>Tutorialspoint</p>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="backlog"><a class="header" href="#backlog">Backlog</a></h1>
<h2 id="topics"><a class="header" href="#topics">Topics</a></h2>
<p><strong>Fault Tolerant Systems</strong>:</p>
<ul>
<li>General Reliability Development Hazard logs (FRACAS) [Redundancia]</li>
<li>High Availability [https://www.controlglobal.com/assets/14WPpdf/140324-ISA-ControlSystemsHighAvailability.pdf]</li>
<li>Technical documentation</li>
<li>Safety cases</li>
<li>Bulkhead</li>
<li>Change Control</li>
<li>Cold Standby</li>
<li>Defensive Design</li>
<li>Derating</li>
<li>Design Debt</li>
<li>Design Life</li>
<li>Design Thinking</li>
<li>Durability</li>
<li>Edge Case</li>
<li>Entropy</li>
<li>Error Tolerance</li>
<li>Fault Tolerance</li>
<li>Fail Well</li>
<li>Fail-Safe</li>
<li>Graceful Degradation</li>
<li>Mistake Proofing &amp; Poka Yoke Technique</li>
<li>No Fault Found</li>
<li>Resilience</li>
<li>Safety by Design</li>
<li>Self-Healing</li>
<li>Service Life</li>
<li>Systems Thinking</li>
<li>Testbed</li>
<li>Waer and Tear</li>
<li>Deconstructability</li>
<li>Refinement</li>
<li>Defense in Depth</li>
<li>FMEA Design and Process</li>
<li>Physics of Failure (PoF)</li>
<li>Built-in Self-test</li>
<li>Eliminating single point of failure (SPOF)</li>
</ul>
<p><strong>Analysis</strong>:</p>
<ul>
<li>Root Cause analysis</li>
<li>Fault tree analysis (FTA)</li>
<li>Failure mode and effects analysis (FMEA)</li>
<li>Failure mode, effects and criticality analysis (FMECA)</li>
<li>Reliability, Availability and Maintainability Study (RAMS)</li>
<li>Mission Readiness analysis</li>
<li>Functional System Failure analysis</li>
<li>Inherent Design Reliability analysis</li>
<li>Use/Load analysis and wear calculations</li>
<li>Fatigue and creep analysis</li>
<li>Component Stress analysis</li>
<li>Field failure monitoring</li>
<li>Field data analysis</li>
<li>Caution and warning analysis</li>
<li>Chaos Engineering</li>
<li>Reliability Risk Assessments</li>
<li>Hazard analysis</li>
<li>Manufactoring defect analysis</li>
<li>Residual Risk analysis (RCA)</li>
<li>Weibull</li>
<li>Accelerated Life Testing (ALT Analysis)</li>
<li>Material Strength analysis</li>
<li>Quality of Service</li>
<li>Quality Control</li>
<li>Defect Rate</li>
<li>Failure Rate</li>
<li>Mean Time Between Failures</li>
<li>Mean Time to Repair (MTTR)</li>
<li>Mean Corrective Maintenance Time (MCMT)</li>
<li>Mean Preventive Maintenance Time (MPMT)</li>
<li>Mean Maintenance Hours per Repair (MMH/Repair)</li>
<li>Maximum Corrective Maintenance Time (MaxCMT)</li>
</ul>
<p><strong>Data Quality</strong>:</p>
<ul>
<li>Data Quality Completeness</li>
<li>Data Quality Correctness</li>
<li>Data Quality Credibility</li>
<li>Data Quality Precision</li>
<li>Data Quality Relevance</li>
<li>Data Quality Timeliness</li>
<li>Data Quality Traceability</li>
<li>Data Integrity</li>
<li>Data Cleansing</li>
<li>Data Corruption</li>
<li>Data Degradation</li>
<li>Data Artifact</li>
<li>Data Rot</li>
<li>Information Quality Accurate</li>
<li>Information Quality Completeness</li>
<li>Information Quality Comprehensible</li>
<li>Information Quality Credibility</li>
<li>Information Quality Precision</li>
<li>Information Quality Relevance</li>
<li>Information Quality Timeliness</li>
<li>Information Quality Uniqueness</li>
<li>Conformance Quality</li>
<li>Credence Quality</li>
<li>Quality Assurance</li>
<li>Quality Control</li>
<li>Service Quality</li>
<li>Experience Quality</li>
<li>Code Smell</li>
<li>Referential Integrity</li>
<li>Reusability</li>
</ul>
<p><strong>Maintenance</strong>:</p>
<ul>
<li>Maintenance Requirement Allocation</li>
<li>Predictive and Preventive maintenance</li>
<li>Reliability Centered Maintenance (RCM)</li>
</ul>
<p><strong>Failures</strong>:</p>
<ul>
<li>Manufactoring-induced failures</li>
<li>Assembly-induced failures</li>
<li>Transport-induced failures</li>
<li>Storage-induced failures</li>
<li>Systmatic failures</li>
</ul>
<p><strong>Tests</strong>:</p>
<ul>
<li>System Diagnostics Design</li>
<li>Failure/Reliability testing</li>
</ul>
<p><strong>Human Factors</strong>:</p>
<ul>
<li>Human Factors</li>
<li>Human Interaction</li>
<li>Human Errors</li>
<li>Latent Human Error</li>
</ul>
<p><strong>DataOps</strong>:</p>
<p><strong>Business Process Management</strong>:</p>
<ul>
<li>BPM</li>
<li>BPI</li>
<li>BPE</li>
<li>BPA</li>
<li>BPR</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="assets/mermaid.min.js"></script>
        <script src="assets/mermaid-init.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
